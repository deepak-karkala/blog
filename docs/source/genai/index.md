# Gen AI

<!--
```{toctree}
:hidden:

use_cases/index
rag/index
rag_implementation
agents/index
inference_optimization/index
deploying_llms/index
evaluation/index
langchain
```
-->


```{toctree}
:hidden:

ch1_planning
ch2_arch
ch3_prompting
ch4_data_models
ch5_fine_tuning
ch6_evals
ch7_serving
ch8_inference_opt
ch9_industry
```


##

### 




**LLM Serving**
- [RunPod Blog: How Much VRAM Does Your LLM Need? A Guide to GPU Memory Requirements: Discover how to determine the right VRAM for your Large Language Model (LLM). Learn about GPU memory requirements, model parameters, and tools to optimize your AI deployments.](https://blog.runpod.io/understanding-vram-and-how-much-your-llm-needs/)

**Inference Optimisations**
- [RunPod Blog: Introduction to vLLM and PagedAttention](https://blog.runpod.io/introduction-to-vllm-and-how-to-run-vllm-on-runpod-serverless/)
- 