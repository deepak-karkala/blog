# Summarization

### TL;DR

- Evaluating text summarization is difficult because there is no one correct solution, and summarization quality often depends on the summary’s context and purpose.

- Metrics like ROUGE, METEOR, and BLEU focus on N-gram overlap but fail to capture the semantic meaning and context.

- LLM-based evaluation approaches like BERTScore and G-eval aim to address these shortcomings by evaluating semantic similarity and coherence, providing a more accurate assessment.

- Despite these advancements and the widespread use of LLM-generated summaries, ensuring robust and comprehensive evaluation remains an open problem and active area of research.

<div class="container py-4 py-md-5 px-4 px-md-3 text-body-secondary">
    <div class="row" >
      <div class="col-lg-4 mb-4">
        <img src="../../_static/genai/evaluation/summarization_eval_metrics.png"></img>
      </div>
    </div>
</div>

Source: [Neptune: LLM Evaluation For Text Summarization](https://neptune.ai/blog/llm-evaluation-text-summarization)



### Intro
- Two main approaches:

	- <b>Extractive summarization</b> creates a summary by selecting and extracting key sentences, phrases, and ideas directly from the original text. Accordingly, the summary is a subset of the original text, and no text is generated by the ML model. Extractive summarization relies on statistical and linguistic features—either explicitly or implicitly—such as word frequency, sentence position, and significance scores to identify important sentences or phrases.

	- <b>Abstractive summarization</b> produces new text that conveys the most critical information from the original. It aims to identify the key information and generate a concise version. Abstractive summarization is typically performed with sequence-to-sequence models, a category to which LLMs with encoder-decoder architecture belong.


### Factors to assess the performance of text summarization models:
- Coherence - the collective quality of all sentences in the summary. The summary should be well-structured and well-organized and should build a coherent body of information about a topic.
- Consistency - factual alignment between the summary and source document. The summary should contain only statements that are entailed by the source document.
- Fluency - the quality of individual sentences of the summary. The summary should have no formatting problems and grammatical errors that make the summary difficult to read.
- Relevance - selection of the most important content from the source document. The summary should include only important information from the source document.

> Most modern language models, fluency and Coherence are less of a concern, leaving us with factual consistency and relevance, which we can frame as binary classification and reuse the classification metrics.


### Traditional, non-LLM Eval Metrics

```python
from openai import OpenAI
import os
import re
import pandas as pd

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "<your OpenAI API key if not set as env var>"))

excerpt = "OpenAI's mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. OpenAI will build safe and beneficial AGI directly, but will also consider its mission fulfilled if its work aids others to achieve this outcome. OpenAI follows several key principles for this purpose. First, broadly distributed benefits - any influence over AGI's deployment will be used for the benefit of all, and to avoid harmful uses or undue concentration of power. Second, long-term safety - OpenAI is committed to doing the research to make AGI safe, and to promote the adoption of such research across the AI community. Third, technical leadership - OpenAI aims to be at the forefront of AI capabilities. Fourth, a cooperative orientation - OpenAI actively cooperates with other research and policy institutions, and seeks to create a global community working together to address AGI's global challenges."
ref_summary = "OpenAI aims to ensure artificial general intelligence (AGI) is used for everyone's benefit, avoiding harmful uses or undue power concentration. It is committed to researching AGI safety, promoting such studies among the AI community. OpenAI seeks to lead in AI capabilities and cooperates with global research and policy institutions to address AGI's challenges."
eval_summary_1 = "OpenAI aims to AGI benefits all humanity, avoiding harmful uses and power concentration. It pioneers research into safe and beneficial AGI and promotes adoption globally. OpenAI maintains technical leadership in AI while cooperating with global institutions to address AGI challenges. It seeks to lead a collaborative worldwide effort developing AGI for collective good."
eval_summary_2 = "OpenAI aims to ensure AGI is for everyone's use, totally avoiding harmful stuff or big power concentration. Committed to researching AGI's safe side, promoting these studies in AI folks. OpenAI wants to be top in AI things and works with worldwide research, policy groups to figure AGI's stuff."
```

###### ROUGE
- Word Overlap Metrics: Metrics like ROUGE (Recall-Oriented Understudy for Gisting Evaluation) often compare the overlap of words or phrases between the generated summary and a reference summary. If both summaries are of similar length, the likelihood of a higher overlap increases, potentially leading to higher scores.

```python
# Python Implementation of the ROUGE Metric
from rouge import Rouge

# function to calculate the Rouge score
def get_rouge_scores(text1, text2):
    rouge = Rouge()
    return rouge.get_scores(text1, text2)


rouge_scores_out = []

# Calculate the ROUGE scores for both summaries using reference
eval_1_rouge = get_rouge_scores(eval_summary_1, ref_summary)
eval_2_rouge = get_rouge_scores(eval_summary_2, ref_summary)
```
- While ROUGE and similar metrics, such as BLEU and METEOR, offer quantitative measures, they often fail to capture the true essence of a well-generated summary. They also correlate worse with human scores.

- It determines the overlap of groups of words or tokens (N-grams) between the reference text and the generated summary.

- ROUGE has multiple variants, such as ROUGE-N (for N-grams), ROUGE-L (for the longest common subsequence), and ROUGE-S (for skip-bigram co-occurrence statistics).

- If the summarization is limited to key term extraction, ROUGE-1 is the preferred choice. For simple summarization tasks, it is better to use ROUGE-2 metrics. For a more structured summarization, ROUGE-L and ROUGE-S might be the best fit.

- While ROUGE is popular for extractive summarization, it can also be used for abstractive summarization. A high value of the ROUGE score indicates that the generated summary preserves the most essential information from the original text.

<b>How does the ROUGE metric work?</b>
- let’s consider the following example:
    - Human-crafted reference summary: The cat sat on the mat and looked out the window at the birds.
    - LLM-generated summary: The cat looked at the birds from the mat.

- Methodology
    - Tokenize the summaries
    - Calculate the overlap:
        - <b>ROUGE-1: Overlapping unigrams</b>
        - <b>ROUGE-2: Overlapping bigrams</b>
        - <b>ROUGE-L: largest overlap ( longest common sequence ) </b>
    - Calculate precision, recall, and F1 score
        - Precision = Number of overlapping unigrams / Total number of unigrams in generated summary
        - Recall = Number of overlapping unigrams / Total number of unigrams in reference summary
        - F1 score = 2 × (Precision×Recall) / (Precision+Recall)

    - <b>ROUGE-S</b>
        - To calculate the ROUGE-S (ROUGE-Skip) score, we need to count skip-bigram co-occurrences between the reference and generated summaries. A skip-bigram is any pair of words in their respective order, allowing for gaps.
        - Tokenize the summaries
        - Generate the skip-bigrams for reference and generate summaries
        - Count the total number of skip-bigrams in the reference and the generated summary
        - Calculate ROUGE-S score 
            - <code>ROUGE-S = (2 × count of matching skip-bigrams) /  (total skip-bigrams in reference summary + total skip-bigrams in generated summary)</code>

- <b>Interpretation of ROUGE metrics</b>
    - Focusing solely on achieving high ROGUE precision can result in missing important details, as we might generate fewer words to boost precision. Focusing too much on recall favors long summaries that include additional but irrelevant information. Typically, looking at the F1 score that balances both measures is best.


- <b>Problems with ROUGE metrics</b>
    - <i>Surface-level matching</i>: ROUGE matches the exact N-grams from the reference and generated summaries. It fails to capture the semantic meaning and context of the text. ROUGE does not handle synonyms, meaning two semantically identical summaries with different wording have low ROUGE scores. Paraphrased content, which conveys the same meaning with different wording, receives a low ROUGE score despite being a good summary.
    
    - <i>Recall-oriented nature</i>: ROUGE’s primary goal is to measure the completeness of the generated summary in terms of how much of the important content from the reference summary it captures. This can lead to high scores for longer summaries that include many reference terms, even if they contain irrelevant information.
    
    - <i>Lack of evaluation for coherence and fluency</i>: ROUGE does not evaluate the text’s coherence, fluency, or overall readability. A summary that contains the right N-grams achieves a high ROUGE score, even if it is disjointed or grammatically incorrect.


###### METEOR (Metric for Evaluation of Translation with Explicit Ordering)

- METEOR is a summarization metric similar to ROGUE that matches words by reducing them to their root or base form through stemming and lemmatization. For example, “playing,” “plays,” “played,” and “playful” all become “play.”

- Additionally, METEOR assigns higher scores to summaries that focus on the most important information from the source. Information that is repeated multiple times or irrelevant receives lower scores. It does so by calculating a fragmentation penalty by checking if a chunk is a sequence of matched words in the same order as they appear in the reference summary.

- Methodology
    - Tokenize the summaries
    - Identify exact matches
    - Calculate precision, recall, and F-mean
        - Harmonic mean of precision and recall = (10×Precision×Recall) / (Recall+9×Precision)
    - <b>Calculate the fragmentation penalty</b>
        - Determine the number of “chunks.” A “chunk” is a sequence of matched tokens in the same order as they appear in the reference summary.
        - <code>P = 0.5 × (Number of chunks) / (Number of matched words</code>
    - Final METEOR score
        - <code>METEOR = F-mean × (1−P)</code>

- Interpreting the METEOR score
    - The METEOR score ranges from 0 to 1, where a score close to 1 indicates a better match between the generated and reference text. METEOR is recall-oriented and ensures that the generated text captures as much information from the reference text.

    - The harmonic mean between precision and recall F-mean is biased towards recall and is the key indicator for the summary’s completeness. A low fragmentation penalty indicates that the summary is coherent and concise.

- Drawbacks
    - Limited contextual understanding: METEOR does not capture the contextual relationship between words and sentences. As it focuses on word-level matching rather than sentence or paragraph coherence, it might misjudge the relevance and importance of information in the summary.
    
    - Despite improvements over ROUGE, METEOR still relies on surface forms of words and their alignments. This can lead to an overemphasis on specific words and phrases rather than understanding the deeper meaning and intent behind the text.

    - Sensitivity to paraphrasing and synonym use: Although METEOR uses stemming for synonyms and paraphrasing, its effectiveness in capturing all possible variations is limited. It does not recognize semantically equivalent phrases that use different syntactic structures or less common synonyms.


###### BLEU (Bilingual Evaluation Understudy)

- Initially designed to evaluate machine translation, it is also used to evaluate summaries.
- One advantage of BLEU compared to ROGUE and METEOR is that it can compare the generated text to multiple reference texts for a more robust evaluation. Also, BLEU includes a brevity penalty to prevent the generation of overly short texts that achieve high precision but omit important information.

- <b>Methodology</b>
    - Tokenize the summaries
    - Calculate matching N-grams
        - we find matching unigrams, bigrams, and trigrams and calculate the precision (matching N-grams / total N-grams in generated summary).

    - <b>Determine the brevity penalty</b>  
        - Length of the reference summary: 14 tokens
        - Length of the generated summary: 9 tokens
        - <code>Brevity penalty: BP = e^(1−14 / 9) = e^(−0.5556) ≈ 0.5738</code>

    - Calculate the BLEU score
        - Combined precision: We combine the N-gram precisions with weights (usually uniform weights, e.g., 1/4 for 1-gram, 2-gram, 3-gram, 4-gram) and apply the brevity penalty.
            - <code>P = (unigram-precision ^ 0.25) × (bigram-precision ^ 0.25) × (trigram-precision ^ 0.25)</code>
        - <code> BLEU = BP × P </code>

- <b>Interpreting the BLEU score</b>
    - BLEU is a precision-oriented metric that evaluates the content present in the generated summary. The BLUE score ranges between 0 and 1, where a score close to 1 indicates a highly accurate summary.


- <b>Problems with the BLEU score</b>

    - <i>Surface-level matching</i>: Similar to ROUGE and METEOR, BLEU relies on the exact N-gram matching between the generated text and reference text and fails to capture the semantic meaning and context of the text. BLEU does not handle synonyms or paraphrases well.

    - <i>Effective short summaries are penalized</i>: BLEU’s brevity penalty was designed to discourage overly short translations. It can penalize concise and accurate summaries that are shorter than the reference summary, even if they capture the essential information effectively.

    - <i>Higher order N-grams limitation</i>: BLEU evaluates N-grams up to a certain length (typically 3 or 4). Longer dependencies and structures are not well captured, missing out on evaluating the coherence and logical flow of longer text segments.




### LLM evaluation frameworks for summarization tasks



###### BertScore
- Semantic Similarity Metrics: Tools like BertScore evaluate the semantic similarity between the generated summary and the reference. Longer summaries might cover more content from the reference text, which could result in a higher similarity score, even if the summary isn’t necessarily better in terms of quality or conciseness.


- <i>ROUGE relies on the exact presence of words in both the predicted and reference texts, failing to interpret the underlying semantics. This is where BERTScore comes in and leverages the contextual embeddings from the BERT model, aiming to evaluate the similarity between a predicted and a reference sentence in the context of machine-generated text. By comparing embeddings from both sentences, BERTScore captures semantic similarities that might be missed by traditional n-gram based metrics.</i>


-  It leverages the contextual embeddings (vector representations of each word’s meaning and context) provided by pre-trained language models like BERT (Bidirectional Encoder Representations from Transformers).


- <b>How does BERTScore work?</b>
    - Tokenization and embedding extraction
    - Cosine-similarity calculation
        - We compute the pairwise cosine similarity between each embedded token in the candidate summary and each embedded token in the reference summary. The maximum similarity scores for each token are retained and then used to compute the precision, recall, and F1 scores.

- <b>Interpreting BERTScore</b>
    - By calculating the similarity score for or all tokens, BERTScore takes into account both the syntactic and semantic relevance context of the generated summary compared to the human-crafted reference.

    - For the BERTScore, precision, recall, and F1 scores are all given equal importance. A high score for all these metrics indicates a high quality of the generated summary.


- <b>Problems with BERTScore</b>
    - High computational cost
    - Dependency on pre-trained models
    - Difficulty in interpreting scores: BERTScore, being based on dense vector representations and cosine similarity, can be less intuitive to interpret compared to simpler metrics like ROUGE or BLEU.
    - Lack of standardization: There is no single standardized version of BERTScore, leading to variations in implementations and configurations
    - <i>Overemphasis on semantic similarity: BERTScore focuses on capturing semantic similarity between texts. This emphasis can sometimes overlook other important aspects of summarization quality, such as coherence, fluency, and factual accuracy.</i> 




```python
# BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity.
from bert_score import BERTScorer

# Instantiate the BERTScorer object for English language
scorer = BERTScorer(lang="en")

# Calculate BERTScore for the summary 1 against the excerpt
# P1, R1, F1_1 represent Precision, Recall, and F1 Score respectively
P1, R1, F1_1 = scorer.score([eval_summary_1], [ref_summary])

# Calculate BERTScore for summary 2 against the excerpt
# P2, R2, F2_2 represent Precision, Recall, and F1 Score respectively
P2, R2, F2_2 = scorer.score([eval_summary_2], [ref_summary])

print("Summary 1 F1 Score:", F1_1.tolist()[0])
print("Summary 2 F1 Score:", F2_2.tolist()[0])
```


###### G-Eval
- uses GPT-4 to evaluate the quality of summaries without a ground truth. This method shows state-of-the-art results when compared to human judgements, based on meta-evaluation against SummEval benchmark


- Overview of this method:

	- We define four distinct criteria:
		- Relevance: Evaluates if the summary includes only important information and excludes redundancies.
		- Coherence: Assesses the logical flow and organization of the summary.
		- Consistency: Checks if the summary aligns with the facts in the source document.
		- Fluency: Rates the grammar and readability of the summary.

	- We craft prompts for each of these criteria, taking the original document and the summary as inputs, and leveraging chain-of-thought generation and guiding the model to output a numeric score from 1-5 for each criteria.

	- We generate scores from gpt-4 with the defined prompts, comparing them across summaries.


- <i>The implementation of the G-Eval technique involves four separate and detailed prompts designed to evaluate the summary output against each of these dimensions, with a score from 1-5 on a Likert scale (except for Fluency, which adopts a 1-3 scale). These prompts, along with the input document and summary to be evaluated, are fed to GPT-4; the score outputs are collected and final scores are calculated.</i>

- Implementation
	- [Microsoft G-eval](https://github.com/microsoft/promptflow/tree/main/examples/flows/evaluation/eval-summarization)

- Features
	- State-of-the-art abstractive summarization evaluation method
	- Reference-free evaluation
	- Adopts Chain-of-Thought (CoT), a set of intermediate instructions generated by LLM describing the detailed evaluation steps to provide more context and guidance for LLM to evaluate the generated summary
	- Evaluates in a form-filling paradigm
	- Uses the probability-weighted summation of the output scores as the final score to obtain more fine-grained, continuous scores that better reflect the quality and diversity of the generated texts

- <b>Problems with G-Eval</b>
    - Bias and fairness: Like any automated system, G-Eval can reflect biases in the training data or the choice of evaluation metrics. This can lead to unfair assessments of summaries, especially across different demographic or content categories.
    - High computational cost
    - Lack of calibration: Since an LLM provides the score based on a user-provided prompt, it is not calibrated. Thus, G-Eval is similar to asking different users to rate a summary on a five-star scale, but it is inconsistent across different summaries.




### Implementation

###### [OpenAI G-eval](https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization)
- In this demonstration, we're using a direct scoring function where gpt-4 generates a discrete score (1-5) for each metric. Normalizing the scores and taking a weighted sum could result in more robust, continuous scores that better reflect the quality and diversity of the summaries.

```python
# Evaluation prompt template based on G-Eval
EVALUATION_PROMPT_TEMPLATE = """
You will be given one summary written for an article. Your task is to rate the summary on one metric.
Please make sure you read and understand these instructions very carefully. 
Please keep this document open while reviewing, and refer to it as needed.

Evaluation Criteria:

{criteria}

Evaluation Steps:

{steps}

Example:

Source Text:

{document}

Summary:

{summary}

Evaluation Form (scores ONLY):

- {metric_name}
"""

# Metric 1: Relevance

RELEVANCY_SCORE_CRITERIA = """
Relevance(1-5) - selection of important content from the source. \
The summary should include only important information from the source document. \
Annotators were instructed to penalize summaries which contained redundancies and excess information.
"""

RELEVANCY_SCORE_STEPS = """
1. Read the summary and the source document carefully.
2. Compare the summary to the source document and identify the main points of the article.
3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.
4. Assign a relevance score from 1 to 5.
"""

# Metric 2: Coherence

COHERENCE_SCORE_CRITERIA = """
Coherence(1-5) - the collective quality of all sentences. \
We align this dimension with the DUC quality question of structure and coherence \
whereby "the summary should be well-structured and well-organized. \
The summary should not just be a heap of related information, but should build from sentence to a\
coherent body of information about a topic."
"""

COHERENCE_SCORE_STEPS = """
1. Read the article carefully and identify the main topic and key points.
2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,
and if it presents them in a clear and logical order.
3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.
"""

# Metric 3: Consistency

CONSISTENCY_SCORE_CRITERIA = """
Consistency(1-5) - the factual alignment between the summary and the summarized source. \
A factually consistent summary contains only statements that are entailed by the source document. \
Annotators were also asked to penalize summaries that contained hallucinated facts.
"""

CONSISTENCY_SCORE_STEPS = """
1. Read the article carefully and identify the main facts and details it presents.
2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.
3. Assign a score for consistency based on the Evaluation Criteria.
"""

# Metric 4: Fluency

FLUENCY_SCORE_CRITERIA = """
Fluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.
1: Poor. The summary has many errors that make it hard to understand or sound unnatural.
2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.
3: Good. The summary has few or no errors and is easy to read and follow.
"""

FLUENCY_SCORE_STEPS = """
Read the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 3.
"""


def get_geval_score(
    criteria: str, steps: str, document: str, summary: str, metric_name: str
):
    prompt = EVALUATION_PROMPT_TEMPLATE.format(
        criteria=criteria,
        steps=steps,
        metric_name=metric_name,
        document=document,
        summary=summary,
    )
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=5,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
    )
    return response.choices[0].message.content


evaluation_metrics = {
    "Relevance": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),
    "Coherence": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),
    "Consistency": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),
    "Fluency": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),
}

summaries = {"Summary 1": eval_summary_1, "Summary 2": eval_summary_2}

data = {"Evaluation Type": [], "Summary Type": [], "Score": []}

for eval_type, (criteria, steps) in evaluation_metrics.items():
    for summ_type, summary in summaries.items():
        data["Evaluation Type"].append(eval_type)
        data["Summary Type"].append(summ_type)
        result = get_geval_score(criteria, steps, excerpt, summary, eval_type)
        score_num = int(result.strip())
        data["Score"].append(score_num)

pivot_df = pd.DataFrame(data, index=None).pivot(
    index="Evaluation Type", columns="Summary Type", values="Score"
)
styled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)
display(styled_pivot_df)
```


###### [Ragas: Summarization Score](https://docs.ragas.io/en/latest/concepts/metrics/summarization_score.html)
- This metric gives a measure of how well the summary captures the important information from the contexts. The intuition behind this metric is that a good summary shall contain all the important information present in the context

```python
from ragas.metrics import summarization_score
from ragas import evaluate
from datasets import Dataset 


data_samples = {
    'contexts':[["A company is launching a new product, a smartphone app designed to help users track their fitness goals. The app allows users to set daily exercise targets, log their meals, and track their water intake. It also provides personalized workout recommendations and sends motivational reminders throughout the day."]],
    'summary':['A company is launching a fitness tracking app that helps users set exercise goals, log meals, and track water intake, with personalized workout suggestions and motivational reminders.'],
}
dataset = Dataset.from_dict(data_samples)
score = evaluate(dataset,metrics=[summarization_score])
score.to_pandas()
```


###### [TruLens Summarization Evaluation](https://www.trulens.org/examples/use_cases/summarization_eval/#create-the-app-and-wrap-it)

- Summarization metrics break down into three categories
	- Ground truth agreement: For these set of metrics, we will measure how similar the generated summary is to some human-created ground truth. We will use for different measures: BERT score, BLEU, ROUGE and a measure where an LM is prompted to produce a similarity score.
	- Groundedness: Estimate if the generated summary can be traced back to parts of the original transcript both wth LLM and NLI methods.
	- Comprehensivenss: Estimate if the generated summary contains all of the key points from the source text.

```python
import openai


class DialogSummaryApp:
    @instrument
    def summarize(self, dialog):
        client = openai.OpenAI()
        summary = (
            client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": """Summarize the given dialog into 1-2 sentences based on the following criteria: 
                     1. Convey only the most salient information; 
                     2. Be brief; 
                     3. Preserve important named entities within the conversation; 
                     4. Be written from an observer perspective; 
                     5. Be written in formal language. """,
                    },
                    {"role": "user", "content": dialog},
                ],
            )
            .choices[0]
            .message.content
        )
        return summary

# Write feedback functions
from trulens.core import Feedback
from trulens.feedback import GroundTruthAgreement

from trulens.core import Select
from trulens.providers.huggingface import Huggingface
from trulens.providers.openai import OpenAI

provider = OpenAI(model_engine="gpt-4o")
hug_provider = Huggingface()

ground_truth_collection = GroundTruthAgreement(golden_set, provider=provider)
f_groundtruth = Feedback(
    ground_truth_collection.agreement_measure, name="Similarity (LLM)"
).on_input_output()
f_bert_score = Feedback(ground_truth_collection.bert_score).on_input_output()
f_bleu = Feedback(ground_truth_collection.bleu).on_input_output()
f_rouge = Feedback(ground_truth_collection.rouge).on_input_output()
# Groundedness between each context chunk and the response.


f_groundedness_llm = (
    Feedback(
        provider.groundedness_measure_with_cot_reasons,
        name="Groundedness - LLM Judge",
    )
    .on(Select.RecordInput)
    .on(Select.RecordOutput)
)
f_groundedness_nli = (
    Feedback(
        hug_provider.groundedness_measure_with_nli,
        name="Groundedness - NLI Judge",
    )
    .on(Select.RecordInput)
    .on(Select.RecordOutput)
)
f_comprehensiveness = (
    Feedback(
        provider.comprehensiveness_with_cot_reasons, name="Comprehensiveness"
    )
    .on(Select.RecordInput)
    .on(Select.RecordOutput)
)


# Create the app and wrap it
app = DialogSummaryApp()
print(app.summarize(dev_df.dialogue[498]))

tru_recorder = TruCustomApp(
    app,
    app_name="Summarize",
    app_version="v1",
    feedbacks=[
        f_groundtruth,
        f_groundedness_llm,
        f_groundedness_nli,
        f_comprehensiveness,
        f_bert_score,
        f_bleu,
        f_rouge,
    ],
)
```







### References
- [Microsoft: Evaluating the performance of LLM summarization prompts with G-Eval](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/g-eval-metric-for-summarization)
- [OpenAI: How to evaluate a summarization task](https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization)
- [Neptune: LLM Evaluation For Text Summarization](https://neptune.ai/blog/llm-evaluation-text-summarization)
