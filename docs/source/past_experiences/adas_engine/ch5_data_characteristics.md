# Data Characteristics



| Data Type                                                          | What it contains                                                               | Typical Volume (per vehicle / day)                                                                                                                                                | Velocity (ingest rate)                                                                                                                 | Profile & Access Patterns                                                                                                                                | Where stored & format                                                                                                                                            |
| ------------------------------------------------------------------ | ------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Multi-camera video (front, rear, sides; e.g., 6–8 cams, 1080p)** | H.264/H.265 compressed video streams with timestamps; frame-level metadata     | **\~0.8–2.0 TB/day** (compression + duty cycle dependent). For six 1080p cams: **\~1.8 TB/day** is a commonly cited figure; fully instrumented platforms can reach multi-TB/hour. | **\~500 Mbps–3.5 Gbps per camera** upper bounds (in-vehicle network design range; actual logged bitrate depends on codec/compression). | **Hot** during curation/mining & training; **WORM** (write once, read many); mostly **sequential reads** (decode); queried by clip/time, route, weather. | **S3 Bronze** raw (rosbag or MP4/MKV H.264/H.265); **S3 Silver** derivatives (keyframes as JPEG/PNG, synchronized **Parquet/Zarr**); **Glacier** for cold tiers. |
| **LiDAR point clouds (if equipped)**                               | Spinning 3D point clouds; intensity, ring, timestamp                           | **\~80–200 GB/day** (20–100 Mbps sustained; 6–8 hr logging).                                                                                                                      | **20–100 Mbps** typical; HDL-64E \~1.3–2.2M pts/s.                                                                                     | **Warm**; columnar/packetized storage; used for auto-labeling, BEV occupancy, calibration checks; sampled heavily for hard slices.                       | **S3 Bronze** (pcap/rosbag); **S3 Silver** (**Parquet/Zarr/LAZ**), partitioned by time/vehicle; **Glue** catalog.                                                |
| **Automotive radar**                                               | Range-Doppler cubes or processed tracks; timestamps                            | **\~2–20 GB/day** (0.1–5 Mbps typical; multiple sensors).                                                                                                                         | **0.1–5 Mbps** per sensor (typical)                                                                                                    | **Warm/Cold**; used to enrich adverse-weather scenes and sanity-check vision; random access by event windows.                                            | **S3 Bronze** raw binaries/rosbag; processed tracks as **Parquet**; indexed in **Athena/Glue**.                                                                  |
| **IMU / GNSS**                                                     | Linear accel, gyro, orientation; GPS lat/lon/alt; fix quality                  | **\~0.5–2 GB/day** (high-rate IMU + 1–10 Hz GNSS)                                                                                                                                 | **KB/s–low MB/s**                                                                                                                      | **Hot → Warm**; used in sync, odometry, alignment, replay; indexed by time; small but high-value.                                                        | **S3 Silver** **Parquet/CSV** time-series; **DynamoDB** for fast time-window lookups.                                                                            |
| **CAN / CAN-FD**                                                   | Vehicle bus signals: speed, throttle, brake, gear, steering angle, diag codes  | **\~1–5 GB/day** (depends on signal set and logging rate)                                                                                                                         | **Up to 2–8 Mbit/s on CAN-FD**; 64-byte payloads.                                                                                      | **Hot** for trigger generation (harsh brake, disengagement); time-series queries; joins with video windows.                                              | **S3 Bronze** **MDF4/BLF**; converted to **Parquet** in **S3 Silver**; **Glue** tables.                                                                          |
| **Synchronized “shards” (Parquet/Zarr)**                           | Time-aligned camera keyframes, LiDAR packets, radar, CAN; standardized schemas | **\~10–30% of raw volume** after keyframing/transcoding                                                                                                                           | **Burst** during post-ingest transforms                                                                                                | **Hot** for feature extraction & analytics; **columnar** reads (Athena/Spark/EMR); partitioned by date/route/vehicle.                                    | **S3 Silver** **Parquet/Zarr** with Hive-style partitions; **Glue** catalog; **Athena** external tables.                                                         |
| **Event windows & scene indices**                                  | Clips around triggers (cut-in, harsh brake, stationary hazard, map mismatch)   | **\~0.5–2% of total hours** promoted to events (varies with thresholds)                                                                                                           | **Low, event-driven**                                                                                                                  | **Hot**; drives mining/labeling; **OpenSearch/DynamoDB** indexing for quick retrieval.                                                                   | Clips in **S3 Silver** (MP4 + JSON sidecars); indices in **DynamoDB/OpenSearch**; preview thumbnails in **S3**.                                                  |
| **Auto-labels / weak supervision**                                 | Model-generated boxes, masks, tracks with confidence + provenance              | **\~1–5% of raw size** (annotations + masks)                                                                                                                                      | **Batch** after offline inference                                                                                                      | **Warm**; compared against human QA; versioned with datasets (DVC/W\&B Artifacts).                                                                       | **S3 Silver** **COCO JSON** / **Parquet** label tables; mask PNGs; referenced in **W\&B Artifacts/DVC**.                                                         |
| **Human labels & QA**                                              | Polygon masks, boxes, lane graphs, attributes; adjudication logs               | **MBs per scene** (but high curation cost)                                                                                                                                        | **Human-paced**                                                                                                                        | **Hot** for golden sets; immutable once frozen; linked to scenes by hashes.                                                                              | **S3 Gold** label sets (**COCO/Cityscapes** formats); Labelbox exports archived; **DVC** manifests for versioning.                                               |
| **Vector embeddings (scene/clip)**                                 | 512–2048-D float embeddings for similarity search                              | **\~50–200 GB/week** per active fleet (depends on sampling density)                                                                                                               | **Batch** (post-compute)                                                                                                               | **Hot** in mining; stored in vector index; enables “find more like this” harvesting.                                                                     | **OpenSearch** vector fields or **Milvus**; periodic **S3 Parquet** snapshots for backup.                                                                        |
| **Map & weather enrichment**                                       | Road class, speed limits, lane topology; historical weather                    | **GBs–tens of GBs** (shared)                                                                                                                                                      | **Periodic batch**                                                                                                                     | **Warm** reference layers; **many-to-one** joins by time/geo; cached.                                                                                    | **S3** reference buckets (**Parquet/GeoParquet/GeoJSON**); **DynamoDB** cache for hot tiles/joins.                                                               |
| **Training datasets (curated)**                                    | Balanced, de-duplicated scenes; labels; metadata; file manifests               | **\~5–30 TB per release** (per perception task family)                                                                                                                            | **Burst** on build                                                                                                                     | **Hot** during training sprints, **warm** afterwards; distributed reads; tracked as W\&B Artifacts + DVC.                                                | **S3 Gold** curated bundles; manifest **JSON/YAML**; **W\&B Artifacts** pointer; **DVC** tags.                                                                   |
| **Golden & slice benchmark sets**                                  | Fixed scenes for regression checks (night/rain/occlusion/construction)         | **\~100–500 GB**                                                                                                                                                                  | **N/A**                                                                                                                                | **Hot** at evaluation time; immutable; small but business-critical.                                                                                      | **S3 Gold** immutable; **W\&B Artifacts** for eval consumption; checksums recorded.                                                                              |
| **Batch inference outputs (mining)**                               | Offline predictions across unlabeled pool; uncertainty, errors                 | **\~0.5–2× raw label volume** (JSON/Parquet)                                                                                                                                      | **High** during nightly jobs                                                                                                           | **Warm**; queried for failure buckets; triggers re-labeling and retraining.                                                                              | **S3 Silver/Gold** **Parquet** tables; **Athena/Presto** external tables; partitioned by model/version/date.                                                     |
| **Model artifacts**                                                | Checkpoints, ONNX/TensorRT engines, calibration caches                         | **\~0.2–1.0 GB per model** (engine) + checkpoints                                                                                                                                 | **Low**                                                                                                                                | **Hot** in CI/CD; **warm** archive; immutable by version; pulled by serving.                                                                             | **S3** for checkpoints/engines; **ECR** for serving containers; version tags in **W\&B Model Registry**.                                                         |
| **Telemetry & monitoring logs**                                    | Serving latency p50/p95/p99, error rates, confidences by slice, GPU metrics    | **\~5–20 GB/day** per cluster                                                                                                                                                     | **Continuous**                                                                                                                         | **Hot** for SLOs/drift; retained hot (7–14d), then warm/cold.                                                                                            | **CloudWatch Logs/Prometheus** TSDB; long-term archive to **S3 (Parquet)** via exporters.                                                                        |
| **OTA packages (edge builds)**                                     | Quantized/distilled models, config, manifests, signatures                      | **\~200–800 MB** per release                                                                                                                                                      | **Ad-hoc**                                                                                                                             | **Warm**; staged, signed, rolled out in phases; strict immutability.                                                                                     | **S3** release bucket; signed artifacts (**KMS**, cosign); distribution manifests; edge cache.                                                                   |



**Context & sources for key figures:**

* Six 1080p cameras ≈ **\~1.8 TB/day**; fully instrumented vehicles can reach **multi-TB/hour**
* Industry sensor data rates (design bounds): **cameras 500–3500 Mbps**, **LiDAR 20–100 Mbps**, **radar 0.1–5 Mbps**
* LiDAR points/sec (HDL-64E): **\~1.3–2.2M pts/s**.
* CAN-FD link capacity up to **2–8 Mbit/s**; 64-byte frames.
* Mature AV programs report **PB-scale accumulation** over weeks/months, necessitating disciplined selection/mining (NVIDIA/“MagLev”/SDC scale).



> **Practical planning note:** For a pilot fleet (e.g., **\~20 trucks**, \~6 hrs/day logging, event-triggered capture), expect **\~20–50 TB/day** across the fleet (dominated by cameras), before reduction from curation/tiering. This aligns with public ranges of **\~1–2 TB/day/vehicle** under compressed logging, and higher when logging broader modalities at higher bitrates.


