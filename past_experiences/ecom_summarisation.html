<!doctype html>
<html class="no-js" lang="en" data-content_root="">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="RAG-Based Product Discovery" href="ecom_rag.html" /><link rel="prev" title="Real-Time Purchase Intent Scoring" href="ecom_propensity.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><!-- Generated with Sphinx 7.1.2 and Furo 2024.05.06 -->
        <title>Reviews Summarisation - Home</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=387cc868" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/style.css?v=8a7ff5ee" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" /
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Home</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">Home</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Past Experiences</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Past Experiences</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="iot_anomaly.html">Anomaly Detection in Time Series IoT Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="iot_forecasting.html">Energy Demand Forecasting in Time Series IoT Data</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="adas_engine/index.html">ADAS: Data Engine</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of ADAS: Data Engine</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch0_business_challenge.html">Business Challenge and Goals</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch1_ml_problem_framing.html">ML Problem Framing</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch2_operational_strategy.html">Planning, Operational Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch3_pipelines_workflows.html">Workflows, Team, Roles</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch4_testing_strategy.html">Testing Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch6_data_ingestion_workflows.html">Data Ingestion Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch7_scene_understanding_data_mining.html">Scene Understanding &amp; Data Mining</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch8_model_training.html">Model Training &amp; Experimentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch9_packaging_promotion.html">Packaging, Evaluation &amp; Promotion Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch10_deployment_serving.html">Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch11_monitoring_continual_learning.html">Monitoring &amp; Continual Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch12_cost_lifecycle_compliance.html">Cost, Lifecycle, Compliance</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch13_reliability_capacity_maps.html">Reliability, Capacity, Maps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ecom_cltv.html">Customer Lifetime Value</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_propensity.html">Real-Time Purchase Intent Scoring</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Reviews Summarisation</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_rag.html">RAG-Based Product Discovery</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../projects/index.html">Projects</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Projects</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/nlp/index.html">Natural Language Processing</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Natural Language Processing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/airbnb_alternate_search/about/index.html">Airbnb Listing description based Semantic Search</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/cv/index.html">Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Computer Vision</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/ecommerce_image_segmentation/about/index.html">Image Segmentation for Ecommerce Products</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/ml/index.html">Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Machine Learning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/airbnb_price_modeling/about/index.html">Predictive Price Modeling for Airbnb listings</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications/index.html">Patents, Papers, Thesis</a></li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../agents/index.html">AI Agents: A Lead Engineer’s Handbook</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of AI Agents: A Lead Engineer’s Handbook</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch1_intro.html">Agent Fundamentals: What, Why, and When?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch2_patterns.html">Agentic Patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch5_context_engineering.html">Context Engineering for AI Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch6_case_studies.html">The State of the Industry: Insights from the Field</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch7_conclusion.html"><strong>Conclusion: The Lead Engineer’s Mental Model for Building Agents</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_cost.html">Cost Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_data.html">Data Management and Knowledge Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_deploy.html">Deployment and Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_guardrails.html">Guardrails</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_hitl.html">Human-in-the-Loop (HITL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_latency.html">Latency Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_llm.html">LLM – Prompts, Goals, and Persona</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_memory.html">Managing Agent Memory (Short-Term and Long-Term)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_monitor.html">Monitoring and Observability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_orchestration.html">Orchestration and Task Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_prod.html">Production Challenges and Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_security.html">Securing AI Agents and Preventing Abuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_tool.html">Tool Use and Integration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_trust.html">Building Trustworthy and Ethical AI Agents</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">MLOps</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of MLOps</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch1_problem_framing.html">ML Problem framing</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of ML Problem framing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/ch2_blueprint_operational_strategy.html">The MLOps Blueprint &amp; Operational Strategy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch2a_platform/index.html">ML Platforms</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of ML Platforms</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/ml_platforms.html">ML Platforms: How to</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/uber.html">Uber Michelangelo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/linkedin.html">LinkedIn DARWIN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/netflix.html">Netflix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/shopify.html">Shopify Merlin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/zomato.html">Zomato: Real-time ML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/coveo.html">Coveo: MLOPs at reasonable scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/monzo.html">Monzo ML Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/didact.html">Didact AI</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch3_project_planning/index.html">Project Planning</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of Project Planning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/prd.html">Project Requirements Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/tech_stack.html">Tech Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/config_management.html">Config Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/pipeline_design.html">Pipeline Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/environment_strategy.html">Environment Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/cicd_branching_model.html">CI/CD Strategy and Branching Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/directory_structure.html">Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/env_branchind_cicd_deployment.html">Environments, Branching, CI/CD, and Deployments Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/project_management.html">Project Management for MLOps</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch4_data_discovery/index.html">Data Sourcing, Discovery</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of Data Sourcing, Discovery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/data_sourcing_discovery.html">Data Sourcing, Discovery &amp; Understanding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/ch4_project.html">Project-Trending Now: Implementing Web Scraping, Ingestion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/industry_case_studies.html">Data Discovery Platforms: Industry Case Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/facebook_nemo.html">Facebook: Nemo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/netflix_metacat.html">Netflix Metacat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/uber_databook.html">Uber Databook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/linkedin_datahub.html">LinkedIn Datahub</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch7_model_development/index.html">Model Development, Tuning, Selection, Ensembles, Calibration</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of Model Development, Tuning, Selection, Ensembles, Calibration</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/ch7_model_development.html">Chapter 7: Model Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/dl_training_playbook.html">How to train DL Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/development.html">Model Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/industry_lessons.html">Model Development: Lessons from production systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/ensembles.html"><strong>Model Ensembles</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/selection.html">Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/tuning_hypopt.html">Hyperparameter Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/expt_tracking.html">ML Expt tracking, Data Lineage, Model Registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/calibration.html">Model Calibration</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch10_deployment_serving/index.html">Model Deployment &amp; Serving</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of Model Deployment &amp; Serving</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/ch10_deployment_serving.html">Chapter 10: Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/guide_deployment_serving.html">Guide: Model Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/guide_inference_stack.html">Deep Dive: Inference Stack</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/index.html">Monitoring, Observability, Drift, Interpretability</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of Monitoring, Observability, Drift, Interpretability</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/ch11_monitor_observe_drift.html">Chapter 11: Monitoring, Observability, Drifts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_monitor_observe_drift.html">Guide: ML System Failures, Data Distribution Shifts, Monitoring, and Observability</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_interpretability_shap_lime.html">Interpretability, SHAP, LIME</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_stack.html">Prometheus + Grafana and ELK Stacks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/index.html">Continual learning, Retraining, A/B Testing</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of Continual learning, Retraining, A/B Testing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/ch12_continual_learning_prod_testing.html">Chapter 12: Continual Learning &amp; Production Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_continual_learning.html">Continual Learning &amp; Model Retraining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_ab_testing.html">A/B Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_ab_testing_industry_lessons.html">A/B Testing &amp; Experimentation: Industry lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_prod_testing_expt.html">Guide: Production Testing &amp; Experimentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/dr_prod_testing_expt.html">Deep Research: Production Testing &amp; Experimentation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/index.html">PyTorch</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of PyTorch</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/general.html">General</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/state_dict.html">state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/distributed_data_parallel.html">Distributed Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/ddp_under_the_hood.html">DDP: Under the Hood</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/dp_ddp.html">DP vs DDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/fsdp.html">FSDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/tensor_parallelism.html">Tensor parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/pipeline_parallelism.html">Pipeline Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/device_mesh.html">Device Mesh</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lld/index.html">Low Level Design</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of Low Level Design</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lld/parking_lot.html">Parking Lot</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../visualization/index.html">Data Visualization Projects</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/past_experiences/ecom_summarisation.md.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="reviews-summarisation">
<h1>Reviews Summarisation<a class="headerlink" href="#reviews-summarisation" title="Permalink to this heading">¶</a></h1>
<section id="id1">
<h2><a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<hr class="docutils" />
<section id="tldr-end-to-end-llm-powered-review-summarization">
<h3><strong>TLDR: End-to-End LLM-Powered Review Summarization</strong><a class="headerlink" href="#tldr-end-to-end-llm-powered-review-summarization" title="Permalink to this heading">¶</a></h3>
<ul>
<li><p><strong>Challenge:</strong> A mid-sized European e-commerce marketplace faced a dual problem: customers were overwhelmed by thousands of unstructured, multilingual reviews, leading to decision fatigue, while the business lacked an automated way to extract actionable insights from this valuable customer feedback.</p></li>
<li><p><strong>My Role &amp; Solution:</strong> As the lead <strong>Data Scientist and MLOps/ML Engineer</strong>, I designed, built, and deployed the end-to-end system to solve this challenge. My contributions spanned the entire ML lifecycle, including <strong>Feature Engineering, Model Development, Training &amp; Inference Pipelines, Deployment, Monitoring, and Continual Learning</strong>.</p>
<p>My solution was a cost-effective, production-grade system architected around a <strong>batch-processing, Retrieval-Augmented Generation (RAG)</strong> strategy to ensure summaries were factually grounded and trustworthy. Key strategic decisions included:</p>
<ul class="simple">
<li><p><strong>Model:</strong> Fine-tuning a <code class="docutils literal notranslate"><span class="pre">Mistral-7B</span></code> model using PEFT/LoRA on a multilingual dataset to achieve near GPT-4 quality at a fraction of the cost.</p></li>
<li><p><strong>Quality Assurance:</strong> Implementing a robust, multi-layered evaluation strategy using <strong>Ragas</strong> for factual consistency and <strong>LLM-as-a-judge</strong> for coherence.</p></li>
<li><p><strong>Cost-Effective Architecture:</strong> Building the system on a <strong>fully Managed</strong> solutions (<strong>AWS Step Functions, OpenSearch</strong>) and using <strong>EKS with scale-to-zero</strong> for the inference endpoint to minimize idle costs, making the solution financially viable.</p></li>
<li><p><strong>Tech Stack:</strong> AWS (Step Functions, EKS, SageMaker, Bedrock, OpenSearch), vLLM, Ragas, MLflow, DVC, LangChain, Terraform, and GitHub Actions.</p></li>
</ul>
</li>
<li><p><strong>Impact:</strong> The system successfully transformed raw feedback into a valuable asset for both customers and the business. The primary impacts measured via A/B testing were:</p>
<ul class="simple">
<li><p>Achieved up to a <strong>2% increase in conversion rate</strong> for products with AI-generated summaries.</p></li>
<li><p>Led to a <strong>3% reduction in product returns</strong> in the pilot category (Electronics) by setting more accurate customer expectations pre-purchase.</p></li>
<li><p><strong>Automated 100%</strong> of the manual review analysis process, saving an estimated <strong>20-30 hours per week</strong> of analyst time.</p></li>
</ul>
</li>
<li><p><strong>System Architecture:</strong> The diagram below illustrates the serverless, event-driven architecture, with the components I owned and delivered highlighted.</p></li>
</ul>
<img src="../_static/past_experiences/ecom_summarisation/contributions.png" width="100%" style="background-color: #FCF1EF;"/>
<hr class="docutils" />
<p>A Note on This Series</p>
<p>In my seven years as a Machine Learning Engineer, I’ve noticed a significant gap between academic tutorials and the realities of production MLOps. Many guides stop at deploying a model in a FastAPI container, leaving aspiring engineers without the strategic frameworks and practical insights needed for building robust, end-to-end systems.</p>
<p>This series is a sincere attempt to provide a practitioner’s blueprint for production machine learning, moving beyond the code to explore the critical decision-making, trade-offs, and challenges involved. My goal is to eventually expand this work into a comprehensive, project-based MLOps course.</p>
<p>Important Disclaimers:</p>
<ul class="simple">
<li><p><strong>On Authenticity</strong>: The methodologies and frameworks shared here are drawn directly from my professional experience. However, to ensure client confidentiality, all specific project details and data have been anonymized and are for illustrative purposes only.</p></li>
<li><p><strong>On Collaboration</strong>: These posts were created with the assistance of AI for code and prose generation. The strategic framing, project context, and real-world insights that guide the content are entirely my own.</p></li>
<li><p><strong>On the Code</strong>: The code provided is a conceptual blueprint, not a production-ready application. It is designed to illustrate the structure and logic of a real-world system. Please use it as a learning tool and a starting point for your own projects, but do not expect it to run out-of-the-box without further development and testing.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="the-business-imperative-from-information-overload-to-actionable-intelligence">
<h3>1. The Business Imperative: From Information Overload to Actionable Intelligence<a class="headerlink" href="#the-business-imperative-from-information-overload-to-actionable-intelligence" title="Permalink to this heading">¶</a></h3>
<p>In the modern e-commerce landscape, the sheer volume of customer-generated reviews presents a dual-sided challenge. While intended to empower shoppers, the deluge of unstructured, often repetitive feedback leads to information overload and decision fatigue. This phenomenon hinders the purchasing journey, contributing directly to higher rates of cart abandonment and a diminished user experience.</p>
<p>For the business, this vast repository of customer sentiment is a potential goldmine of insights. However, manually processing thousands of reviews to identify recurring themes, product shortcomings, and emerging trends is a significant operational bottleneck. The process is labor-intensive, slow, and prone to human error, causing critical customer pain points to go unnoticed and opportunities for product improvement to be missed.</p>
<p>The fundamental business challenge is to effectively distill this massive volume of unstructured text into a format that is both concise for the customer and insightful for the business. The goal is to transform raw feedback from a liability of noise into a strategic asset that enhances customer confidence and drives internal innovation.</p>
<section id="project-objectives-and-goals">
<h4><strong>Project Objectives and Goals</strong><a class="headerlink" href="#project-objectives-and-goals" title="Permalink to this heading">¶</a></h4>
<p>The primary objective is to develop and deploy an automated, scalable system that summarizes customer reviews to enhance the user shopping experience and provide actionable intelligence to internal teams.</p>
<p>This objective is broken down into two core goals:</p>
<ol class="arabic simple">
<li><p><strong>Enhance the Customer Decision-Making Process:</strong> By providing clear, balanced, and digestible summaries of peer reviews, the system aims to reduce customer friction and build trust. This allows shoppers to make faster, better-informed purchasing decisions, moving from a state of uncertainty to one of confidence.</p></li>
<li><p><strong>Unlock Actionable Business Intelligence:</strong> By automating the analysis of review data, the system will identify and surface key product strengths, weaknesses, and recurring customer issues. This data-driven feedback loop empowers product, marketing, and support teams to improve product quality, refine marketing messages, and enhance brand value.</p></li>
</ol>
</section>
<section id="measuring-success-key-performance-indicators-kpis">
<h4><strong>Measuring Success: Key Performance Indicators (KPIs)</strong><a class="headerlink" href="#measuring-success-key-performance-indicators-kpis" title="Permalink to this heading">¶</a></h4>
<p>The success of this initiative will be measured through a rigorous A/B testing framework, focusing on both direct business impact and user engagement.</p>
<p><strong>Primary Business KPIs:</strong></p>
<ul class="simple">
<li><p><strong>Increase in Conversion Rate:</strong> A measurable lift in the percentage of users who purchase a product after interacting with the summary feature.</p></li>
<li><p><strong>Reduction in Average Time-to-Purchase:</strong> A decrease in the time it takes for a user to make a purchase, indicating accelerated decision-making.</p></li>
<li><p><strong>Decrease in Product Return Rates:</strong> A measurable reduction in returns for products where summaries are displayed, suggesting that the feature sets more accurate customer expectations.</p></li>
</ul>
<p><strong>Secondary Engagement KPIs:</strong></p>
<ul class="simple">
<li><p><strong>User Interaction with Summaries:</strong> Tracking metrics such as time spent on the page and expansion of summary sections to gauge feature utility.</p></li>
<li><p><strong>“Helpful” Feedback Rate:</strong> An increase in users clicking the “This summary was helpful” button, serving as a direct signal of user satisfaction.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="ml-problem-framing-from-business-need-to-technical-blueprint">
<h3>2. ML Problem Framing: From Business Need to Technical Blueprint<a class="headerlink" href="#ml-problem-framing-from-business-need-to-technical-blueprint" title="Permalink to this heading">¶</a></h3>
<p>Translating a business objective into a well-defined machine learning task is the most critical step in the MLOps lifecycle. A precise problem frame acts as the foundational blueprint, guiding data strategy, model selection, and success measurement. A flaw at this stage will inevitably propagate through the entire system, regardless of subsequent engineering excellence.</p>
<section id="setting-the-business-objectives">
<h4>2.1 Setting the Business Objectives<a class="headerlink" href="#setting-the-business-objectives" title="Permalink to this heading">¶</a></h4>
<p>The project originates from a clear business need to improve customer experience and operational efficiency. Before any technical solution is considered, the objectives must be aligned with all relevant stakeholders—including Product Management, Marketing, Engineering, and Business Leadership—to ensure a shared vision of success.</p>
<p>The primary business objectives are:</p>
<ul class="simple">
<li><p><strong>Increase purchase conversion and customer trust</strong> by providing concise, unbiased summaries of user reviews.</p></li>
<li><p><strong>Improve product quality</strong> by creating an automated feedback loop that delivers actionable insights from customer sentiment to product teams.</p></li>
<li><p><strong>Enhance operational efficiency</strong> by reducing the manual effort required to analyze and understand large volumes of customer feedback.</p></li>
</ul>
</section>
<section id="is-machine-learning-the-right-approach">
<h4>2.2 Is Machine Learning the Right Approach?<a class="headerlink" href="#is-machine-learning-the-right-approach" title="Permalink to this heading">¶</a></h4>
<p>While a simple rule-based system (e.g., extracting sentences with “love” or “hate”) could be a baseline, it would fail to capture the nuance, slang, and context inherent in user-generated reviews. Machine Learning is the appropriate approach for this problem due to several key factors:</p>
<ul class="simple">
<li><p><strong>Complex Patterns:</strong> Summarizing requires understanding semantic meaning, identifying key themes, and paraphrasing, tasks too complex for static rules.</p></li>
<li><p><strong>Need for Scale:</strong> The solution must operate across thousands of products, each with hundreds or thousands of reviews, making manual summarization impossible.</p></li>
<li><p><strong>Dynamic Environment:</strong> Customer language, product features, and feedback trends evolve. An ML model can be retrained to adapt, whereas a rule-based system would become brittle.</p></li>
<li><p><strong>Tolerance for Error:</strong> While factual accuracy is paramount, minor stylistic imperfections in the generated summary are acceptable if the core meaning is preserved and the feature provides value.</p></li>
</ul>
<p>This project also presents a classic opportunity to create a <strong>Data Flywheel</strong>, a virtuous cycle where the product improves as more data is generated.</p>
</section>
<section id="defining-the-ml-problem">
<h4>2.3 Defining the ML Problem<a class="headerlink" href="#defining-the-ml-problem" title="Permalink to this heading">¶</a></h4>
<p>The business goal must be translated into a precise technical task for the model.</p>
<ul class="simple">
<li><p><strong>Ideal Outcome:</strong> A customer effortlessly understands the collective opinion on a product, leading to a confident purchase decision.</p></li>
<li><p><strong>Model’s Goal:</strong> Ingest all unstructured review texts for a single product and generate a single, concise, factually grounded, and stylistically neutral summary paragraph.</p></li>
</ul>
<p>This formally classifies the task as <strong>many-to-one, multi-document abstractive summarization</strong>.</p>
<ul class="simple">
<li><p><strong>Many-to-one:</strong> It synthesizes multiple source documents (reviews) into a single output.</p></li>
<li><p><strong>Multi-document:</strong> It must handle and reconcile information from numerous distinct text sources.</p></li>
<li><p><strong>Abstractive:</strong> The goal is to generate novel, fluent sentences that capture the essence of the input, rather than simply extracting and concatenating existing sentences.</p></li>
</ul>
<p>The choice of an abstractive paradigm over an extractive one is deliberate. While an extractive summary (copying key sentences) offers high factual grounding, the informal grammar, slang, and typos common in reviews would result in a disjointed and poor user experience. A purely abstractive approach, however, introduces an unacceptable risk of “hallucination”—generating statements not supported by the source reviews.</p>
<p>Therefore, the optimal solution is a <strong>hybrid approach</strong>, specifically one implemented via a <strong>Retrieval-Augmented Generation (RAG)</strong> architecture. This strategy grounds the abstractive model in retrieved facts, instructing it to generate a summary <em>based only on</em> a curated selection of the most relevant review snippets. This balances the need for readability with the non-negotiable requirement for user trust.</p>
</section>
<section id="assessing-feasibility-risks">
<h4>2.4 Assessing Feasibility &amp; Risks<a class="headerlink" href="#assessing-feasibility-risks" title="Permalink to this heading">¶</a></h4>
<p>A candid assessment of feasibility and potential risks is necessary before committing significant resources.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Category</p></th>
<th class="head text-left"><p>Checkpoint</p></th>
<th class="head text-left"><p>Assessment</p></th>
<th class="head text-left"><p>Notes &amp; Mitigation Strategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Data</strong></p></td>
<td class="text-left"><p>Sufficient quantity &amp; quality?</p></td>
<td class="text-left"><p><strong>Green</strong></p></td>
<td class="text-left"><p>Abundant review data exists. Quality is variable, requiring robust preprocessing to handle spam, short reviews, and noise.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Labeling for fine-tuning feasible?</p></td>
<td class="text-left"><p><strong>Yellow</strong></p></td>
<td class="text-left"><p>Creating high-quality, human-written “golden” summaries for fine-tuning and evaluation is expensive and time-consuming. This will be a primary cost driver.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Problem Difficulty</strong></p></td>
<td class="text-left"><p>High reliability required?</p></td>
<td class="text-left"><p><strong>Red</strong></p></td>
<td class="text-left"><p><strong>Factual consistency is paramount.</strong> A single prominent hallucination could destroy user trust. The RAG architecture is the primary mitigation strategy.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Adversarial attacks</strong></p></td>
<td class="text-left"><p>Adversarial attacks likely?</p></td>
<td class="text-left"><p><strong>Yellow</strong></p></td>
<td class="text-left"><p>Potential for review bombing or spam. Data ingestion pipelines must include anomaly and spam detection.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Technical Reqs</strong></p></td>
<td class="text-left"><p>Latency target achievable?</p></td>
<td class="text-left"><p><strong>Yellow</strong></p></td>
<td class="text-left"><p>Low latency is critical for user experience. This necessitates using optimized inference engines (e.g., vLLM, TGI) and may constrain the size of the production model.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>RoI</strong></p></td>
<td class="text-left"><p>Compute cost manageable?</p></td>
<td class="text-left"><p><strong>Yellow</strong></p></td>
<td class="text-left"><p>GPU resources for fine-tuning and serving are costly. The project requires a clear budget and cost-optimization strategies like quantization and efficient model selection.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Ethics</strong></p></td>
<td class="text-left"><p>Potential for bias?</p></td>
<td class="text-left"><p><strong>Red</strong></p></td>
<td class="text-left"><p><strong>High risk of bias.</strong> The model could learn a “positivity bias” from imbalanced data or fail to represent minority opinions. Mitigation requires data-centric re-tuning on balanced datasets and specific prompt constraints.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="defining-success-metrics">
<h4>2.5 Defining Success Metrics<a class="headerlink" href="#defining-success-metrics" title="Permalink to this heading">¶</a></h4>
<p>To measure progress and validate the final impact, metrics are defined across three distinct domains: Business, Model Quality, and Operations.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Metric Type</p></th>
<th class="head text-left"><p>Success Metric</p></th>
<th class="head text-left"><p>How to Measure &amp; Target</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Business</strong></p></td>
<td class="text-left"><p><strong>Conversion Rate</strong></p></td>
<td class="text-left"><p>A/B Testing. Target: Statistically significant lift in purchases for users shown the summary vs. a control group.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Product Return Rate</strong></p></td>
<td class="text-left"><p>A/B Testing. Target: Statistically significant decrease in returns for products with summaries.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Average Time-to-Purchase</strong></p></td>
<td class="text-left"><p>A/B Testing. Target: Measurable reduction in the session duration leading to a purchase.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Quality</strong></p></td>
<td class="text-left"><p><strong>Faithfulness / Factual Consistency</strong></p></td>
<td class="text-left"><p><strong>(Primary)</strong> LLM-as-a-judge or automated metrics (e.g., SummaC) on a golden dataset. Target: Aim for &gt;95% factual consistency.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Relevance</strong></p></td>
<td class="text-left"><p>ROUGE-L score against reference summaries and human evaluation. Target: Optimize for capturing key pros and cons.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Coherence &amp; Fluency</strong></p></td>
<td class="text-left"><p>Human evaluation or LLM-as-a-judge (Likert scale 1-5). Target: Average score &gt; 4.0.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Operational</strong></p></td>
<td class="text-left"><p><strong>P95 Inference Latency</strong></p></td>
<td class="text-left"><p>Monitoring dashboards (e.g., Prometheus, Grafana). Target: &lt; 500ms for a real-time user experience.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Cost Per 1,000 Summaries</strong></p></td>
<td class="text-left"><p>Cloud billing analysis. Target: Continuously optimize and reduce cost based on model and hardware choices.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>System Throughput</strong></p></td>
<td class="text-left"><p>Load testing and monitoring. Target: System must handle peak traffic loads without violating latency SLAs.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="genai-application-end-to-end-planning">
<h3>3. GenAI Application: End to end planning<a class="headerlink" href="#genai-application-end-to-end-planning" title="Permalink to this heading">¶</a></h3>
<section id="llmops-tech-stack">
<h4>3.1 LLMOps Tech Stack<a class="headerlink" href="#llmops-tech-stack" title="Permalink to this heading">¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Category</p></th>
<th class="head text-left"><p>Canvas Block</p></th>
<th class="head text-left"><p>Tool / Service Chosen</p></th>
<th class="head text-left"><p>Rationale, LLM-Specific Considerations &amp; Trade-offs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Data &amp; Code</strong></p></td>
<td class="text-left"><p>Data Sources &amp; Versioning</p></td>
<td class="text-left"><p><strong>Amazon S3</strong> (Data Lake)<br><strong>DVC</strong> (Data Version Control)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> S3 is the scalable foundation. DVC versions datasets and human-curated evaluation sets, linking them to Git commits for full reproducibility.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Experimentation</strong></p></td>
<td class="text-left"><p>Experiment Management</p></td>
<td class="text-left"><p><strong>Amazon SageMaker Studio</strong> (Notebooks)<br><strong>LangChain</strong> (App Framework)<br><strong>LangSmith</strong> (Debugging/Tracing)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> We use <strong>LangChain</strong> to rapidly prototype the RAG logic. <strong>LangSmith</strong> is indispensable for debugging these chains by visualizing the retrieved context and prompts. This combination accelerates development.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Feature Engineering</strong></p></td>
<td class="text-left"><p>Feature Store &amp; Workflows</p></td>
<td class="text-left"><p><strong>Amazon OpenSearch with k-NN</strong> (Vector DB)<br><strong>Amazon MWAA (Airflow)</strong> (Orchestration)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> The <strong>Vector Database is the new Feature Store</strong> for LLMs. OpenSearch provides the managed retrieval backend. Airflow orchestrates the batch workflow that populates the vector DB.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>ML Lifecycle</strong></p></td>
<td class="text-left"><p>Model &amp; Experiment Tracking</p></td>
<td class="text-left"><p><strong>MLflow Tracking Server</strong></p></td>
<td class="text-left"><p><strong>Rationale:</strong> MLflow remains the central server of record. It logs experiment results, including the final evaluation scores generated by Ragas. It provides the high-level audit trail of what was tried and what worked.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>LLM Quality &amp; Evaluation</strong></p></td>
<td class="text-left"><p><strong>Ragas</strong> (RAG Evaluation)<br><strong>Giskard</strong> (Behavioral Testing)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> This is the critical, missing piece. <strong>Ragas</strong> is used to score our summaries on RAG-specific metrics (faithfulness, context precision). <strong>Giskard</strong> is used to create a suite of “unit tests” for the LLM’s behavior (e.g., robustness to typos, bias checks).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Continuous Training (CT)</p></td>
<td class="text-left"><p><strong>Amazon SageMaker Training Jobs</strong></p></td>
<td class="text-left"><p><strong>Rationale:</strong> SageMaker provides the managed, scalable GPU infrastructure needed for LoRA fine-tuning without the overhead of managing a cluster manually.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Production</strong></p></td>
<td class="text-left"><p>Model Registry &amp; Versioning</p></td>
<td class="text-left"><p><strong>MLflow Model Registry</strong></p></td>
<td class="text-left"><p><strong>Rationale:</strong> <strong>This is the core governance component.</strong> A “model version” in MLflow includes the LoRA weights, the specific LangChain prompt template, and the base model ID. Promoting a model from Staging to Production in this registry is the official, auditable act that triggers deployment.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Model Deployment &amp; Serving</p></td>
<td class="text-left"><p><strong>vLLM/TGI on Amazon EKS</strong><br><strong>Amazon API Gateway</strong></p></td>
<td class="text-left"><p><strong>Rationale:</strong> Specialized LLM serving engines are non-negotiable for performance. EKS provides the necessary control. The inference service itself will use the <strong>LangChain</strong> library to execute the RAG logic at runtime.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Monitoring &amp; Observability</p></td>
<td class="text-left"><p><strong>Amazon CloudWatch, Prometheus</strong> (System)<br><strong>Custom Evaluation Pipeline</strong> (Model Quality)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> Standard tools for system health. For model quality, a custom pipeline is triggered periodically. It samples production requests/responses, runs them through the <strong>Ragas/Giskard</strong> evaluation suite, and logs the quality scores. Alerts are triggered on significant quality degradation (e.g., a drop in faithfulness).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Foundation</strong></p></td>
<td class="text-left"><p>DevOps &amp; Foundations</p></td>
<td class="text-left"><p><strong>Git (GitHub)</strong><br><strong>Docker</strong><br><strong>AWS CDK / Terraform</strong> (IaC)<br><strong>GitHub Actions</strong> (CI/CD)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> Foundational tools for software and infrastructure best practices. GitHub Actions automates testing and the deployment process, which is kicked off by a model promotion event in the MLflow Registry.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="key-pipelines-and-workflows">
<h4>3.2 Key Pipelines and Workflows<a class="headerlink" href="#key-pipelines-and-workflows" title="Permalink to this heading">¶</a></h4>
<p>The system’s automation is realized through a set of interconnected pipelines. Each pipeline is a directed acyclic graph (DAG) of tasks responsible for a specific stage of the data and model lifecycle.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Pipeline / Workflow</p></th>
<th class="head text-left"><p>Trigger</p></th>
<th class="head text-left"><p>Inputs</p></th>
<th class="head text-left"><p>Key Steps</p></th>
<th class="head text-left"><p>Outputs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Data Ingestion &amp; Validation</strong></p></td>
<td class="text-left"><p>Real-time events from user devices and backend services.</p></td>
<td class="text-left"><p>- Raw review JSON objects.<br>- Clickstream events (e.g., “helpful” clicks).</p></td>
<td class="text-left"><p>1. <strong>Receive:</strong> Data streams into a message queue (e.g., Kinesis/Kafka).<br>2. <strong>Validate Structure:</strong> Check for schema conformance (correct fields and types).<br>3. <strong>Validate Semantics:</strong> Pass text through a lightweight model to check for intelligibility and filter out obvious spam/garbled content.<br>4. <strong>Persist:</strong> Load validated raw data into the S3 Data Lake.</p></td>
<td class="text-left"><p>- Clean, validated review data in the S3 Data Lake.<br>- Dead-letter queue for failed records.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Embedding Generation (Feature Engineering)</strong></p></td>
<td class="text-left"><p>A new validated review is added to the S3 Data Lake.</p></td>
<td class="text-left"><p>- A single, validated customer review text.<br>- <code class="docutils literal notranslate"><span class="pre">product_id</span></code>, <code class="docutils literal notranslate"><span class="pre">review_id</span></code>.</p></td>
<td class="text-left"><p>1. <strong>Chunk:</strong> Split the review text into semantically meaningful segments (e.g., sentences or small paragraphs).<br>2. <strong>Embed:</strong> Call a pre-trained embedding model (e.g., <code class="docutils literal notranslate"><span class="pre">amazon.titan-embed-text-v1</span></code>) for each chunk.<br>3. <strong>Index:</strong> Store the resulting embedding vectors and associated metadata (<code class="docutils literal notranslate"><span class="pre">product_id</span></code>, <code class="docutils literal notranslate"><span class="pre">review_id</span></code>, original text chunk) in the Vector Database (Amazon OpenSearch).</p></td>
<td class="text-left"><p>- Indexed, searchable review embeddings in the Vector Database.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Continuous Training (CT)</strong></p></td>
<td class="text-left"><p>- <strong>Scheduled:</strong> e.g., monthly.<br>- <strong>On-demand:</strong> Triggered by a significant model quality degradation alert.</p></td>
<td class="text-left"><p>- A high-quality, curated dataset of reviews and human-annotated summaries from the S3 Data Lake.<br>- Base model ID (e.g., <code class="docutils literal notranslate"><span class="pre">mistral.mistral-7b-instruct-v0:2</span></code>).</p></td>
<td class="text-left"><p>1. <strong>Provision:</strong> Spin up a GPU cluster using a SageMaker Training Job.<br>2. <strong>Fine-tune:</strong> Execute a Parameter-Efficient Fine-Tuning (PEFT) script using the LoRA technique.<br>3. <strong>Artifact Creation:</strong> Package the resulting LoRA adapter weights and the associated prompt template.</p></td>
<td class="text-left"><p>- A new candidate model artifact (LoRA adapter + config) stored in S3.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Model Evaluation &amp; Registration</strong></p></td>
<td class="text-left"><p>Successful completion of a Continuous Training job.</p></td>
<td class="text-left"><p>- Candidate model artifact.<br>- “Golden” evaluation dataset (versioned with DVC).</p></td>
<td class="text-left"><p>1. <strong>Load:</strong> Load the candidate model for inference.<br>2. <strong>Generate:</strong> Create summaries for the entire evaluation dataset.<br>3. <strong>Score:</strong> Evaluate the generated summaries using a specialized framework (e.g., Ragas) for faithfulness, relevance, etc.<br>4. <strong>Compare:</strong> Compare the new model’s scores against the currently deployed production model.<br>5. <strong>Register:</strong> If the new model shows a statistically significant improvement, version and register it in the MLflow Model Registry.</p></td>
<td class="text-left"><p>- A go/no-go decision for deployment.<br>- (If successful) A new, versioned model in the MLflow Model Registry, promoted to “Staging”.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. CI/CD for Deployment</strong></p></td>
<td class="text-left"><p>A model is promoted to the “Production” stage in the MLflow Model Registry.</p></td>
<td class="text-left"><p>- The approved model artifact from the MLflow Registry.</p></td>
<td class="text-left"><p>1. <strong>Package:</strong> GitHub Actions builds a new Docker container including the inference server (vLLM/TGI) and the model artifacts.<br>2. <strong>Test:</strong> Run integration and smoke tests.<br>3. <strong>Deploy:</strong> Orchestrate a safe, phased rollout (e.g., Canary) to the Amazon EKS serving environment.</p></td>
<td class="text-left"><p>- A new version of the inference service running in production, serving a small percentage of live traffic.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>6. Batch Inference</strong></p></td>
<td class="text-left"><p><strong>Scheduled:</strong> Runs every hour.</p></td>
<td class="text-left"><p>- List of <code class="docutils literal notranslate"><span class="pre">product_ids</span></code> that have received new reviews in the last hour.</p></td>
<td class="text-left"><p>1. <strong>Retrieve:</strong> For each product, query the Vector Database to get the most relevant review snippets.<br>2. <strong>Invoke:</strong> Call the production LLM serving endpoint to generate a summary.<br>3. <strong>Cache:</strong> Store the generated summary in the low-latency database (DynamoDB) with a TTL.</p></td>
<td class="text-left"><p>- Freshly computed summaries available in the low-latency cache for fast retrieval.</p></td>
</tr>
</tbody>
</table>
</div>
<!--
| **7. Real-Time Inference** | An API call from the application for a product whose summary is **not** in the low-latency cache. | - A single `product_id`. | 1. **Retrieve:** Query the Vector Database to get the most relevant review snippets.<br>2. **Invoke:** Call the production LLM serving endpoint to generate a summary.<br>3. **Respond & Cache:** Return the summary to the user and simultaneously write it to the low-latency cache. | - A JSON summary response returned to the user (SLO: P95 < 500ms).<br>- A new entry in the low-latency cache. |
-->
</section>
<section id="why-rag-for-reviews-summarization">
<h4>3.3 Why RAG for Reviews Summarization ?<a class="headerlink" href="#why-rag-for-reviews-summarization" title="Permalink to this heading">¶</a></h4>
<p>Let me clarify why, even without a direct user query and in a purely batch context, the RAG pattern (using embeddings and a Vector DB) is still the superior production architecture for this specific application. The reason is not about enabling real-time, but about <strong>quality, control, and cost-efficiency at scale.</strong></p>
<p>Let’s compare the two approaches:</p>
<section id="approach-1-recursive-summarization">
<h5>Approach 1: Recursive Summarization<a class="headerlink" href="#approach-1-recursive-summarization" title="Permalink to this heading">¶</a></h5>
<ol class="arabic simple">
<li><p>For a product with 500 reviews, group them into chunks that fit the LLM context window.</p></li>
<li><p>Send each chunk to the LLM to get an intermediate summary.</p></li>
<li><p>Take all the intermediate summaries, and if there are still too many, repeat the process.</p></li>
<li><p>Finally, send the last set of summaries to the LLM for a final summary.</p></li>
</ol>
<p>This seems straightforward, but it suffers from several critical production-level flaws:</p>
<ul class="simple">
<li><p><strong>1. The “Lost in the Middle” Problem:</strong> LLMs have a known weakness with very long contexts. Information presented at the beginning and end of a prompt is recalled much more effectively than information buried in the middle. In a recursive process, critical details from an early review (e.g., a specific safety concern or a major product flaw) are highly likely to be averaged out and lost in the initial summarization pass. The final summary will then be based on “washed out” intermediate texts, making it generic and potentially misleading.</p></li>
<li><p><strong>2. Lack of Control and Bias Amplification:</strong> This approach is a black box. You cannot control <em>what</em> the LLM focuses on. If a product has 450 generic positive reviews (“Great product!”) and 50 highly detailed, recent negative reviews (“The battery dies in 2 hours”), the recursive approach will almost certainly produce a generic, positive summary. The negative signal will be drowned out. This leads directly to the “helpful hallucination” crisis we aim to avoid, where the summary does not reflect the most critical user feedback.</p></li>
<li><p><strong>3. Inability to Prioritize High-Signal Reviews:</strong> Not all reviews are created equal. A “Verified Purchase” review marked as “helpful” by 100 users is far more valuable than a one-word, anonymous review. The recursive approach treats all text as equal, giving the same weight to low-quality and high-quality feedback.</p></li>
<li><p><strong>4. High Computational Cost:</strong> This approach can be deceptively expensive. For a product with reviews requiring 10 chunks, this strategy involves <strong>11 separate LLM calls</strong>. At scale, across thousands of products, this becomes computationally inefficient and costly.</p></li>
</ul>
</section>
<section id="approach-2-rag-for-batch-summarization-the-recommended-architecture">
<h5>Approach 2: RAG for Batch Summarization (The Recommended Architecture)<a class="headerlink" href="#approach-2-rag-for-batch-summarization-the-recommended-architecture" title="Permalink to this heading">¶</a></h5>
<p>Here, we reframe the purpose of RAG. The “query” is not from an end-user; it’s an <strong>automated, internal query that represents a business logic or a summarization strategy.</strong></p>
<ol class="arabic simple">
<li><p><strong>One-time (cheap) cost:</strong> As new reviews come in, they are chunked, embedded, and stored in the Vector DB. This is a fast, inexpensive background process.</p></li>
<li><p><strong>Batch Inference (with RAG):</strong> When the nightly batch job runs for a product, it does not fetch all reviews. Instead, it executes an <em>automated query</em> against the Vector DB. This query is our point of control. For example:</p>
<ul class="simple">
<li><p><em>“For product 123, retrieve: a) the 5 most helpful positive review snippets, b) the 5 most helpful negative review snippets, and c) the 5 most recent review snippets.”</em></p></li>
</ul>
</li>
<li><p><strong>Controlled Context:</strong> The retrieved snippets—a small, curated, and highly relevant set of texts—are then assembled into a compact context.</p></li>
<li><p><strong>Single, High-Quality LLM Call:</strong> This balanced and signal-rich context is passed to the LLM in a <strong>single call</strong> with a prompt like, “Based <em>only</em> on these reviews, summarize the pros, cons, and most recent feedback.”</p></li>
</ol>
<p><strong>Why this is Superior for a Production System:</strong></p>
<ul class="simple">
<li><p><strong>1. Full Control over Content:</strong> This architecture allows us to <em>programmatically enforce a balanced summary</em>. We can guarantee that the most critical negative feedback is included, preventing bias and building user trust.</p></li>
<li><p><strong>2. Prioritization of High-Signal Data:</strong> We can use the metadata in our Vector DB (helpfulness scores, verified purchase status) to ensure the LLM sees the <em>most important</em> reviews, not just a random assortment.</p></li>
<li><p><strong>3. Mitigates the “Lost in the Middle” Problem:</strong> By feeding the LLM a much smaller, more salient context, we ensure it can effectively process all the provided information.</p></li>
<li><p><strong>4. Cost-Effective and Scalable:</strong> This approach involves <strong>only one LLM call per product</strong> during the batch job, making it significantly cheaper and faster at scale than the 11 calls required by the recursive method. The upfront cost of embedding is minimal in comparison.</p></li>
<li><p><strong>5. Future-Proofing:</strong> This architecture—a Vector DB of all review content—becomes a strategic asset. It can later be used for other features like semantic search (“find me laptops with a great keyboard”) with no extra work.</p></li>
</ul>
<p>The RAG pattern (Embeddings + Vector DB) should be retained as the core of the <strong>batch inference workflow</strong>. Its primary benefit here is not real-time speed, but <strong>unmatched control over summary quality, bias, and cost-efficiency</strong>. It transforms the summarization task from a black-box text-stuffing exercise into a deliberate, controllable, and production-ready engineering process.</p>
</section>
</section>
<hr class="docutils" />
<section id="project-management-and-stages">
<h4>3.3 Project Management and Stages<a class="headerlink" href="#project-management-and-stages" title="Permalink to this heading">¶</a></h4>
<p>This project will follow an iterative, stage-based approach. Each stage has a distinct focus and set of outcomes, allowing for structured progress and regular checkpoints to validate assumptions and de-risk the project.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Key Activities</p></th>
<th class="head text-left"><p>Primary Outcome</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Ideation &amp; Planning</strong></p></td>
<td class="text-left"><p>- Finalize business objectives and KPIs.<br>- Conduct feasibility study and risk assessment.<br>- Define the ML problem (Batch Abstractive Summarization via RAG).<br>- Select and document the core tech stack (as detailed in 3.1 &amp; 3.2).<br>- Develop an initial project plan and timeline.</p></td>
<td class="text-left"><p>A clear, documented project charter that aligns all stakeholders on the goals, scope, and technical approach.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Model Experimentation</strong></p></td>
<td class="text-left"><p>- Establish baseline performance using a simple, non-LLM heuristic.<br>- <strong>Prompt Engineering:</strong> Meticulously craft and test various prompt templates.<br>- <strong>RAG Strategy Evaluation:</strong> Experiment with different retrieval strategies (e.g., balancing positive/negative/recent reviews).<br>- <strong>Model Selection:</strong> Compare performance of different open-source base models (e.g., Llama 3, Mistral, Gemma).<br>- <strong>Fine-Tuning (PEFT):</strong> Conduct LoRA fine-tuning on a curated dataset to improve domain-specific quality.<br>- <strong>Evaluation:</strong> Use LangSmith for debugging and Ragas for quantitative scoring of all experiments.</p></td>
<td class="text-left"><p>The selection of the optimal combination of base model, LoRA adapter, prompt template, and RAG strategy that meets the predefined quality metrics, ready for productionizing.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Pipeline Development</strong></p></td>
<td class="text-left"><p>- <strong>Data Pipelines:</strong> Build the Airflow DAGs for data ingestion, validation, and batch embedding generation.<br>- <strong>Continuous Training (CT) Pipeline:</strong> Automate the fine-tuning and evaluation process.<br>- <strong>Batch Inference Pipeline:</strong> Build the core workflow that orchestrates the RAG-based summary generation and caching.</p></td>
<td class="text-left"><p>A suite of automated, version-controlled pipelines that manage the entire data and model lifecycle from ingestion to inference.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Deployment &amp; Serving</strong></p></td>
<td class="text-left"><p>- <strong>Infrastructure as Code (IaC):</strong> Define the serving infrastructure (EKS cluster, node groups) using Terraform.<br>- <strong>CI/CD Pipeline:</strong> Create the GitHub Actions workflow to containerize and deploy the inference service.<br>- <strong>Serving Endpoint:</strong> Deploy the optimized inference engine (vLLM/TGI) with the selected model.</p></td>
<td class="text-left"><p>A scalable, production-ready inference endpoint that can serve summaries based on the batch pipeline’s logic.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Monitoring &amp; In-Production Testing</strong></p></td>
<td class="text-left"><p>- <strong>Set up Dashboards:</strong> Configure Grafana/CloudWatch for system and operational metrics.<br>- <strong>Implement Quality Monitoring:</strong> Deploy the custom pipeline to sample and evaluate production summaries against quality metrics (e.g., faithfulness).<br>- <strong>Set up Alerting:</strong> Configure alerts for system failures, latency spikes, and model quality degradation.<br>- <strong>A/B Testing:</strong> Once stable, design and run an A/B test to measure the impact on the primary business KPIs.</p></td>
<td class="text-left"><p>A fully observable system with automated monitoring and alerting, providing the data needed to ensure reliability and validate business impact.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="cross-functional-team-roles">
<h4>3.4 Cross-Functional Team &amp; Roles<a class="headerlink" href="#cross-functional-team-roles" title="Permalink to this heading">¶</a></h4>
<p>For this project, we operated as a small, agile, cross-functional team. Clear role definition was crucial for efficiency and accountability.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Role</p></th>
<th class="head text-left"><p>Primary Responsibilities</p></th>
<th class="head text-left"><p>Key Artifacts &amp; Deliverables</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Product Manager</strong></p></td>
<td class="text-left"><p>- Defines the business vision, requirements, and success KPIs.<br>- Manages the project roadmap and prioritizes work.<br>- Acts as the voice of the customer and internal business stakeholders.</p></td>
<td class="text-left"><p>- Product Requirements Document (PRD).<br>- A/B Test plan and success criteria.<br>- Go-to-market and stakeholder communication plan.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data Engineer</strong></p></td>
<td class="text-left"><p>- Owns the data sourcing, ingestion, and validation pipelines.<br>- Manages the Data Lake (S3) and the Vector Database (OpenSearch).<br>- Builds and maintains the Airflow DAGs for populating the Vector DB.</p></td>
<td class="text-left"><p>- Deployed and monitored data ingestion pipelines.<br>- A populated, up-to-date Vector Database.<br>- Data quality dashboards and alerts.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>ML/MLOps Engineer<br>(My Role)</strong></p></td>
<td class="text-left"><p>- Leads all model experimentation and evaluation.<br>- Owns the Continuous Training (CT) and Batch Inference pipelines.<br>- Develops the inference service application (using LangChain).<br>- Manages the CI/CD process for model deployment.<br>- Owns the model monitoring and alerting strategy.</p></td>
<td class="text-left"><p>- MLflow experiments and model artifacts in the Registry.<br>- Deployed CT and Batch Inference pipelines.<br>- Production inference service on EKS.<br>- Model quality monitoring dashboards.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="versioning-and-governance-strategy">
<h4>3.5 Versioning and Governance Strategy<a class="headerlink" href="#versioning-and-governance-strategy" title="Permalink to this heading">¶</a></h4>
<p>To ensure reproducibility, auditability, and stability, a “version everything” philosophy is adopted. This creates a complete lineage from the raw data to the final summary served to a user.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Artifact</p></th>
<th class="head text-left"><p>Tool for Versioning</p></th>
<th class="head text-left"><p>Rationale &amp; LLM-Specific Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Code</strong></p></td>
<td class="text-left"><p><strong>Git (GitHub)</strong></p></td>
<td class="text-left"><p>The single source of truth for all application code, pipeline definitions (DAGs), and infrastructure-as-code scripts.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data</strong></p></td>
<td class="text-left"><p><strong>DVC</strong></p></td>
<td class="text-left"><p>Used specifically for versioning the critical, human-curated “golden” datasets used for evaluation. This ensures that when we compare model v1.1 to v1.2, we know they were tested against the exact same data.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Prompts</strong></p></td>
<td class="text-left"><p><strong>Git (in a config file)</strong></p></td>
<td class="text-left"><p><strong>Prompts are treated as code.</strong> A change to the prompt template is a critical change to the model’s behavior. Prompts are versioned in Git and loaded by the application at runtime. A new prompt version can trigger a new model registration.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model</strong></p></td>
<td class="text-left"><p><strong>MLflow Model Registry</strong></p></td>
<td class="text-left"><p>The central governance hub. A “model” version in the registry is a composite artifact that bundles: <br>1. The LoRA adapter weights.<br>2. A pointer to the base model (e.g., <code class="docutils literal notranslate"><span class="pre">mistral-7b-v0.2</span></code>).<br>3. The version of the prompt template to be used.<br>4. The inference configuration.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Infrastructure</strong></p></td>
<td class="text-left"><p><strong>Terraform / AWS CDK</strong></p></td>
<td class="text-left"><p>The entire cloud infrastructure (EKS clusters, S3 buckets, IAM roles) is defined as code and versioned in Git, enabling disaster recovery and preventing manual configuration drift.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="comprehensive-evaluation-strategy">
<h4>3.6 Comprehensive Evaluation Strategy<a class="headerlink" href="#comprehensive-evaluation-strategy" title="Permalink to this heading">¶</a></h4>
<p>Testing an LLM-powered application requires a multi-layered approach that moves far beyond traditional software QA. The strategy must validate not only the code and infrastructure but also the quality of the data, the nuances of the model’s learned behavior, and the integrity of the end-to-end generation process. This “crucible” ensures that what we build is reliable, fair, and trustworthy.</p>
<p>The following table outlines the four core pillars of our testing strategy, detailing <em>what</em> we test, <em>why</em> it’s critical for an LLM system, <em>how</em> we’ll implement it, and <em>when</em> in the lifecycle it occurs.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Test Category</p></th>
<th class="head text-left"><p>Specific Test</p></th>
<th class="head text-left"><p>Purpose &amp; LLM-Specific Focus</p></th>
<th class="head text-left"><p>Tools / Method</p></th>
<th class="head text-left"><p>Stage / Trigger</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Data &amp; Prompt Quality Testing</strong> (The Foundation)</p></td>
<td class="text-left"><p><strong>Input Schema &amp; Value Validation</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Ensure the structural integrity of incoming review data. <br> <strong>LLM Focus:</strong> Prevent “Garbage In, Garbage Out.” A malformed review text can lead to nonsensical embeddings and poor summaries.</p></td>
<td class="text-left"><p>Great Expectations</p></td>
<td class="text-left"><p>During the data ingestion pipeline (CI/CD)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>PII &amp; Toxicity Screening (Inputs)</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Identify and redact Personally Identifiable Information (PII) and filter out toxic content <em>before</em> it ever reaches the LLM. <br> <strong>LLM Focus:</strong> A critical data privacy and safety guardrail.</p></td>
<td class="text-left"><p>Custom regex, open-source libraries (e.g., <code class="docutils literal notranslate"><span class="pre">presidio</span></code>), or a lightweight classification model.</p></td>
<td class="text-left"><p>During the data ingestion pipeline (CI/CD)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Prompt Template Unit Tests</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Validate that the prompt formatting logic is correct. <br> <strong>LLM Focus:</strong> Treating the prompt as code. A bug in the prompt templating can break the entire system.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">pytest</span></code> with mocked review snippets.</p></td>
<td class="text-left"><p>On code commit (CI)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Offline Model &amp; Application Logic Evaluation</strong> (The Quality Gate)</p></td>
<td class="text-left"><p><strong>Behavioral “Unit” Tests</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Test for basic, expected model behaviors on simple inputs. <br> <strong>LLM Focus:</strong> Invariance (paraphrasing a review shouldn’t drastically change the summary); Directionality (adding a strong negative fact should make the summary sentiment more negative).</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">Giskard</span></code> or custom <code class="docutils literal notranslate"><span class="pre">pytest</span></code> scripts.</p></td>
<td class="text-left"><p>On code commit, and within the model evaluation pipeline (CI/CD).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>RAG-Specific Evaluation</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Quantitatively measure the quality of the Retrieval-Augmented Generation process. <br> <strong>LLM Focus:</strong> This is the core of our quality assessment. We measure: <br> • <strong>Faithfulness:</strong> Does the summary contain “hallucinations” or is it factually grounded in the retrieved reviews? <br> • <strong>Context Precision:</strong> Are the retrieved review snippets relevant to the final summary?</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">Ragas</span></code> framework.</p></td>
<td class="text-left"><p>Within the automated model evaluation pipeline after every training run (CD).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Holistic Quality Evaluation</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Assess the overall readability and usefulness of the summary. <br> <strong>LLM Focus:</strong> Measure human-like qualities: <strong>Coherence</strong> (is it logical?), <strong>Conciseness</strong> (is it to the point?), and <strong>Fluency</strong> (is it well-written?).</p></td>
<td class="text-left"><p><strong>LLM-as-a-judge:</strong> Using a powerful model (e.g., GPT-4o) with a detailed rubric to score the summaries on a Likert scale.</p></td>
<td class="text-left"><p>Within the model evaluation pipeline (CD).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Safety &amp; Bias Evaluation</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Ensure the generated summaries are not toxic, biased, or unfair. <br> <strong>LLM Focus:</strong> We run slice-based evaluation, specifically testing performance on products with highly polarized reviews to ensure both positive and negative viewpoints are represented fairly.</p></td>
<td class="text-left"><p>Custom evaluation scripts and toxicity classifiers (e.g., Detoxify).</p></td>
<td class="text-left"><p>Within the model evaluation pipeline (CD).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>3. Pipeline &amp; Infrastructure Testing</strong></p></td>
<td class="text-left"><p><strong>Component Integration Tests</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Verify that the different components of the application logic work together. <br> <strong>LLM Focus:</strong> Test the full RAG flow: mock the Vector DB -&gt; test the retrieval logic -&gt; verify correct prompt construction -&gt; mock the LLM call.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">pytest</span></code> with mocked services (<code class="docutils literal notranslate"><span class="pre">unittest.mock</span></code>).</p></td>
<td class="text-left"><p>On code commit (CI).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Batch Inference Pipeline E2E Test</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Run the entire batch inference pipeline on a small, representative sample of data in a staging environment. <br> <strong>LLM Focus:</strong> Ensures the orchestration, RAG process, and caching mechanism work end-to-end.</p></td>
<td class="text-left"><p>Airflow DAG testing triggered via GitHub Actions.</p></td>
<td class="text-left"><p>Before deploying changes to production (CD).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Serving Endpoint Load Test</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Ensure the deployed model serving endpoint can handle the expected load for the batch pipeline. <br> <strong>LLM Focus:</strong> Measure key performance metrics like <strong>tokens/second throughput</strong> and latency under load to validate cost-effectiveness and scaling.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">Locust</span></code>.</p></td>
<td class="text-left"><p>Before deploying new model versions to production (CD).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>4. In-Production Evaluation</strong> (The Final Verdict)</p></td>
<td class="text-left"><p><strong>Shadow Deployment Comparison</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Safely compare a new “challenger” model against the current “champion” model using live data without impacting users. <br> <strong>LLM Focus:</strong> The nightly batch job runs for <em>both</em> models. The generated summaries are stored separately and compared offline using our Ragas/LLM-as-a-judge pipeline to provide the highest-fidelity signal of real-world performance.</p></td>
<td class="text-left"><p>Custom comparison script within the batch inference pipeline.</p></td>
<td class="text-left"><p>Ongoing for any new model candidate.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Automated Quality Monitoring</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> Continuously monitor the quality of summaries being produced by the live model. <br> <strong>LLM Focus:</strong> A scheduled pipeline periodically samples the latest generated summaries and runs them through the Ragas/LLM-as-a-judge evaluation suite. Alerts are triggered if key metrics like <strong>Faithfulness</strong> drop below a predefined threshold.</p></td>
<td class="text-left"><p>Custom Airflow DAG.</p></td>
<td class="text-left"><p>Scheduled (e.g., daily).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>A/B Testing</strong></p></td>
<td class="text-left"><p><strong>Purpose:</strong> To definitively measure the business impact of a new model or major system change. <br> <strong>LLM Focus:</strong> Even with a batch system, we can run A/B tests by showing a subset of users summaries generated by the challenger model. We then measure the impact on our primary business KPIs (Conversion Rate, Return Rate).</p></td>
<td class="text-left"><p>A/B Testing Platform (e.g., Optimizely, or in-house).</p></td>
<td class="text-left"><p>After a new model has passed all other tests and is deemed stable and safe for production traffic.</p></td>
</tr>
</tbody>
</table>
</div>
<hr class="docutils" />
<!--
### 4. Data Sourcing, Discovery, and Exploratory Data Analysis

A successful ML system is built upon a deep understanding of its foundational data. Before any models are trained or pipelines are built, we must first discover and characterize the data sources, their scale, and their inherent qualities. This exploratory phase informs everything from data cleaning strategies to architectural choices.

#### 4.1 Data Characteristics

The following table provides an estimated profile of the primary data sources for this project, tailored to a mid-sized European marketplace.

| Data Source / Type | Description | Estimated Volume, Velocity & Profile |
| :--- | :--- | :--- |
| **Product Catalog Data** | Static or slowly changing data about the products sold on the marketplace, including SKUs, categories, descriptions, and vendor information. | **Volume:** ~75,000 active SKUs across ~15-20 main categories (e.g., Electronics, Fashion, Home & Kitchen).<br>**Velocity:** Low. ~500 - 1,000 new products added per month. |
| **Historical Customer Reviews** | The existing corpus of all customer reviews collected to date. This forms the primary dataset for our summarization task. | **Volume:** ~4 million total reviews. The distribution is highly skewed; a few popular products may have 5,000+ reviews, while most have <50.<br>**Storage Footprint:** ~1-2 GB of raw text data. |
| **New Customer Reviews (Velocity)** | The stream of new reviews being submitted by customers daily. This data must be ingested to keep summaries fresh. | **Velocity:** ~1,000 - 2,000 new reviews per day.<br>**Profile:** Follows a J-curve distribution: ~60% 5-star, ~15% 4-star, with a long tail of 1-3 star reviews. |
| **Review Content Profile** | The qualitative nature of the review text itself. | **Length:** Average of ~45 words per review, but with significant variance. ~5% are very short (<5 words), and ~5% are very long (>200 words).<br>**Language:** Multilingual. Primarily English, German, and French, with smaller volumes in Spanish, Italian, and Dutch. Many non-native English speakers contribute, leading to grammatical variations. |
| **User Interaction Data (Clickstream)** | High-velocity event data tracking user behavior, such as page views, clicks on review sections, and interactions with the "helpful" button. | **Velocity:** High. For a site with ~250,000 Daily Active Users (DAU), this stream can generate **~5 - 10 million events per day**. |
| **User/Customer Data** | Information about registered users, including purchase history and basic demographic data (country, language preference). | **Volume:** Tied to the customer base. Data is used to enrich interaction data (e.g., linking a "helpful" click to a "verified purchaser"). |

#### **Key Takeaways & Implications for the Project:**

*   **Multilingual Requirement:** The presence of multiple languages is a core challenge. The chosen LLM and embedding models **must** have strong multilingual capabilities.
*   **Need for Robust Preprocessing:** The wide variance in review length and the presence of very short "noise" reviews necessitates a robust filtering and cleaning step in our data ingestion pipeline.
*   **Imbalanced Star Ratings:** The high percentage of 5-star reviews confirms the risk of generating overly positive summaries. Our RAG retrieval strategy *must* be explicitly designed to counteract this by intentionally sampling from negative reviews.
*   **High-Velocity Clickstream:** The volume of interaction data validates the decision to use a dedicated, scalable streaming pipeline (e.g., Kinesis/Kafka) to capture this information without overwhelming the main application servers. This data is critical for determining which reviews are "helpful" and should be prioritized in summaries.
-->
</section>
</section>
<hr class="docutils" />
<section id="data-engineering-pipelines-the-foundation-for-summarization">
<h3>5. Data Engineering &amp; Pipelines: The Foundation for Summarization<a class="headerlink" href="#data-engineering-pipelines-the-foundation-for-summarization" title="Permalink to this heading">¶</a></h3>
<p>The quality of our automated summaries is directly proportional to the quality of the data we feed our model. The following pipelines are designed to be automated, reliable, and auditable, forming the data backbone of the entire system.</p>
<p>We need to transform raw, multilingual reviews into a clean, indexed, and searchable knowledge base (the Vector DB) that our RAG-based summarization model can query efficiently during the batch inference job.To achieve this in a modular and robust way, we will design <strong>two distinct but interconnected data pipelines</strong>, orchestrated by Airflow. This separation of concerns is a critical design choice for maintainability and scalability.</p>
<ol class="arabic simple">
<li><p><strong>Pipeline 1: Data Ingestion &amp; Cleaning:</strong> Its sole responsibility is to process raw reviews and produce a clean, validated, and versioned dataset in our S3 Data Lake.</p></li>
<li><p><strong>Pipeline 2: Embedding Generation:</strong> This downstream pipeline listens for new, clean data from Pipeline 1 and is responsible for creating and indexing the vector embeddings required for RAG.</p></li>
</ol>
<p>Here is the detailed plan for each pipeline.</p>
<section id="pipeline-1-daily-data-ingestion-cleaning">
<h4><strong>Pipeline 1: Daily Data Ingestion &amp; Cleaning</strong><a class="headerlink" href="#pipeline-1-daily-data-ingestion-cleaning" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><strong>Objective:</strong> To ingest all new reviews from the past 24 hours, clean them, validate their quality, and store them as a versioned dataset in the S3 Data Lake.</p></li>
<li><p><strong>Orchestrator:</strong> Amazon MWAA (Airflow)</p></li>
<li><p><strong>Schedule:</strong> Runs once every 24 hours.</p></li>
</ul>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Operation Details</p></th>
<th class="head text-left"><p>Tools</p></th>
<th class="head text-left"><p>Rationale &amp; LLM-Specific Focus</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Ingestion</strong></p></td>
<td class="text-left"><p><strong>Extract New Reviews:</strong> Fetch new customer reviews from the source database (e.g., based on a <code class="docutils literal notranslate"><span class="pre">created_at</span></code> timestamp).</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">psycopg2</span></code>, custom Python scripts.</p></td>
<td class="text-left"><p>Initial step to gather the daily delta of reviews to be processed.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Preprocessing &amp; Cleaning</strong></p></td>
<td class="text-left"><p><strong>HTML &amp; Special Character Removal:</strong> Strip out any HTML tags, URLs, and non-standard characters from the review text. <br> <strong>Text Normalization:</strong> Lowercase text, normalize whitespace.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">BeautifulSoup</span></code>, <code class="docutils literal notranslate"><span class="pre">regex</span></code>, <code class="docutils literal notranslate"><span class="pre">pandas</span></code>.</p></td>
<td class="text-left"><p>This creates a uniform text format, which is crucial for the consistency of both embedding models and the summarization LLM.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Safety &amp; Privacy</strong></p></td>
<td class="text-left"><p><strong>PII Redaction:</strong> Scan review text for Personally Identifiable Information (email addresses, phone numbers) and redact it. <br> <strong>Toxicity Filtering:</strong> Score reviews for toxicity. Flag or filter out highly toxic content to prevent it from being used in summaries.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">presidio</span></code> (for PII), <code class="docutils literal notranslate"><span class="pre">detoxify</span></code> or similar library (for toxicity).</p></td>
<td class="text-left"><p><strong>Crucial LLM Guardrail:</strong> We must ensure that sensitive customer data or toxic language is never passed to the LLM or inadvertently included in a generated summary.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Filtering &amp; Sampling</strong></p></td>
<td class="text-left"><p><strong>Filter “Noise” Reviews:</strong> Remove reviews that are too short to be useful (e.g., &lt; 5 words) or identified as spam.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">pandas</span></code>.</p></td>
<td class="text-left"><p>Improves the signal-to-noise ratio of our dataset. There is no value in summarizing or embedding reviews like “ok” or “good.”</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Schema &amp; Metadata Extraction</strong></p></td>
<td class="text-left"><p><strong>Structure Data:</strong> Format the cleaned data into a defined schema (<code class="docutils literal notranslate"><span class="pre">review_id</span></code>, <code class="docutils literal notranslate"><span class="pre">product_id</span></code>, <code class="docutils literal notranslate"><span class="pre">star_rating</span></code>, <code class="docutils literal notranslate"><span class="pre">cleaned_text</span></code>, <code class="docutils literal notranslate"><span class="pre">language</span></code>, <code class="docutils literal notranslate"><span class="pre">toxicity_score</span></code>).<br> <strong>Language Detection:</strong> Identify the language of each review.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">langdetect</span></code>, <code class="docutils literal notranslate"><span class="pre">pandas</span></code>.</p></td>
<td class="text-left"><p>The detected language is critical metadata. It allows for potential language-specific RAG strategies and helps in debugging model performance on different languages.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>6. Data Validation</strong></p></td>
<td class="text-left"><p><strong>Validate Cleaned Data:</strong> Run a Great Expectations suite on the processed data frame to check: <br> • <code class="docutils literal notranslate"><span class="pre">review_id</span></code> is unique and not null. <br> • <code class="docutils literal notranslate"><span class="pre">star_rating</span></code> is between 1 and 5. <br> • <code class="docutils literal notranslate"><span class="pre">cleaned_text</span></code> is a string. <br> • <code class="docutils literal notranslate"><span class="pre">language</span></code> is in the set of expected languages.</p></td>
<td class="text-left"><p>Great Expectations</p></td>
<td class="text-left"><p>This is an automated quality gate. If the daily batch of cleaned data fails validation, the pipeline stops, and an alert is sent, preventing corrupted data from entering the S3 Lake and the Vector DB.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>7. Versioning &amp; Storage</strong></p></td>
<td class="text-left"><p><strong>Store &amp; Version Data:</strong> Save the final, validated dataset as a Parquet file in the S3 Data Lake. Use DVC to version this dataset and commit the <code class="docutils literal notranslate"><span class="pre">.dvc</span></code> file to Git.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">pyarrow</span></code>, <code class="docutils literal notranslate"><span class="pre">DVC</span></code>, <code class="docutils literal notranslate"><span class="pre">Git</span></code>.</p></td>
<td class="text-left"><p>This creates an immutable, auditable history of our training data. We can always trace a model’s behavior back to the exact version of the data it was trained or evaluated on.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="architecture-diagram">
<h5>Architecture Diagram<a class="headerlink" href="#architecture-diagram" title="Permalink to this heading">¶</a></h5>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_ingestion.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
</section>
<section id="pipeline-2-embedding-generation">
<h4><strong>Pipeline 2: Embedding Generation</strong><a class="headerlink" href="#pipeline-2-embedding-generation" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><strong>Objective:</strong> To take newly cleaned reviews, generate vector embeddings for them, and index them in the Vector Database for retrieval during the batch summarization job.</p></li>
<li><p><strong>Orchestrator:</strong> Amazon MWAA (Airflow)</p></li>
<li><p><strong>Trigger:</strong> Runs upon the successful completion of “Pipeline 1: Data Ingestion &amp; Cleaning.”</p></li>
</ul>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Operation Details</p></th>
<th class="head text-left"><p>Tools</p></th>
<th class="head text-left"><p>Rationale &amp; LLM-Specific Focus</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Data Retrieval</strong></p></td>
<td class="text-left"><p><strong>Load Cleaned Data:</strong> Retrieve the latest versioned dataset of cleaned reviews from the S3 Data Lake.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">DVC</span> <span class="pre">API</span></code>, <code class="docutils literal notranslate"><span class="pre">pandas</span></code>.</p></td>
<td class="text-left"><p>Ensures this pipeline always works on the most recent, validated data.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Text Chunking</strong></p></td>
<td class="text-left"><p><strong>Split Reviews into Segments:</strong> For each review, split the <code class="docutils literal notranslate"><span class="pre">cleaned_text</span></code> into smaller, semantically coherent chunks (e.g., sentences or groups of sentences, max 256 tokens).</p></td>
<td class="text-left"><p>LangChain (<code class="docutils literal notranslate"><span class="pre">RecursiveCharacterTextSplitter</span></code>).</p></td>
<td class="text-left"><p><strong>Core RAG Requirement:</strong> We embed chunks, not entire reviews. This provides more granular retrieval, allowing the RAG system to find the <em>exact sentences</em> that are most relevant, rather than an entire, potentially long review.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Embedding</strong></p></td>
<td class="text-left"><p><strong>Generate Vector Embeddings:</strong> For each text chunk, call a pre-trained, multilingual embedding model to generate a high-dimensional vector representation.</p></td>
<td class="text-left"><p>Amazon Bedrock (<code class="docutils literal notranslate"><span class="pre">amazon.titan-embed-text-v2</span></code>), <code class="docutils literal notranslate"><span class="pre">boto3</span></code>.</p></td>
<td class="text-left"><p>This is the heart of the “feature engineering” for our LLM. The embeddings capture the semantic meaning of the text, enabling the similarity search that powers our RAG strategy.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Indexing</strong></p></td>
<td class="text-left"><p><strong>Store in Vector DB:</strong> Ingest the embeddings into the Vector Database, along with their associated metadata (the original text chunk, <code class="docutils literal notranslate"><span class="pre">review_id</span></code>, <code class="docutils literal notranslate"><span class="pre">product_id</span></code>, <code class="docutils literal notranslate"><span class="pre">star_rating</span></code>, <code class="docutils literal notranslate"><span class="pre">language</span></code>).</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">opensearch-py</span></code>.</p></td>
<td class="text-left"><p>This makes the review data searchable based on semantic meaning. The rich metadata allows our batch inference job to execute sophisticated retrieval logic (e.g., “find helpful negative reviews for product X”).</p></td>
</tr>
</tbody>
</table>
</div>
<section id="id2">
<h5>Architecture Diagram<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_embeddings.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
</section>
</section>
<hr class="docutils" />
<section id="feature-engineering-from-hand-crafted-features-to-semantic-vectors">
<h3>6. Feature Engineering: From Hand-Crafted Features to Semantic Vectors<a class="headerlink" href="#feature-engineering-from-hand-crafted-features-to-semantic-vectors" title="Permalink to this heading">¶</a></h3>
<p>In traditional machine learning, feature engineering is often a laborious process of manually creating signals from raw data (e.g., TF-IDF scores, n-gram counts, sentiment analysis). In the modern LLM era, this paradigm has fundamentally shifted. The primary and most powerful feature is the <strong>semantic meaning of the text itself</strong>, captured through high-dimensional vector embeddings.</p>
<p>For this project, our feature engineering strategy is not about creating dozens of columns in a table. Instead, it’s about creating a rich, searchable knowledge base.</p>
<section id="the-features-for-our-rag-system">
<h4>6.1 The “Features” for our RAG System<a class="headerlink" href="#the-features-for-our-rag-system" title="Permalink to this heading">¶</a></h4>
<p>Our system relies on two types of features, which are generated by our data pipelines and stored together in our Vector Database.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Feature Type</p></th>
<th class="head text-left"><p>Description</p></th>
<th class="head text-left"><p>Generation Process</p></th>
<th class="head text-left"><p>Role in the System</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Primary Feature: <br> Semantic Embeddings</strong></p></td>
<td class="text-left"><p>High-dimensional vectors (e.g., 1024 dimensions) that represent the semantic meaning of a chunk of review text. Words and sentences with similar meanings will have vectors that are close to each other in the vector space.</p></td>
<td class="text-left"><p>Generated by passing text chunks through a pre-trained, multilingual embedding model (e.g., Amazon Titan) during the <strong>Embedding Generation Pipeline</strong>.</p></td>
<td class="text-left"><p><strong>The Engine of Retrieval.</strong> These embeddings allow us to perform a similarity search to find the most relevant review snippets for the summarization task.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Secondary Features: <br> Retrieval Metadata</strong></p></td>
<td class="text-left"><p>Standard data attributes that are stored alongside each embedding vector. This includes <code class="docutils literal notranslate"><span class="pre">star_rating</span></code>, <code class="docutils literal notranslate"><span class="pre">language</span></code>, <code class="docutils literal notranslate"><span class="pre">review_date</span></code>, a calculated <code class="docutils literal notranslate"><span class="pre">helpfulness_score</span></code> (from clickstream data), and a <code class="docutils literal notranslate"><span class="pre">is_verified_purchase</span></code> flag.</p></td>
<td class="text-left"><p>Extracted or calculated during the <strong>Data Ingestion &amp; Cleaning Pipeline</strong>.</p></td>
<td class="text-left"><p><strong>The Brain of Retrieval.</strong> This metadata is not passed to the LLM directly but is used by our <em>retrieval logic</em> to filter and rank the results of the semantic search. It allows us to ask sophisticated internal questions like, <em>“Find me the most helpful, 5-star, verified purchase reviews written in German in the last 90 days.”</em></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="the-vector-database-the-new-feature-store">
<h4>6.2 The Vector Database: The New Feature Store<a class="headerlink" href="#the-vector-database-the-new-feature-store" title="Permalink to this heading">¶</a></h4>
<p>In this LLM-powered, RAG-based architecture, the <strong>Vector Database (Amazon OpenSearch) serves as our de facto Feature Store</strong>.</p>
<ul class="simple">
<li><p><strong>Traditional Feature Store:</strong> Stores versioned, tabular features (e.g., <code class="docutils literal notranslate"><span class="pre">user_7_day_purchase_count</span></code>) for training and low-latency serving.</p></li>
<li><p><strong>Our “Feature Store”:</strong> Stores versioned vector embeddings (the primary feature) and their associated metadata (the secondary features). It is optimized for the core operation we need: efficient, low-latency similarity search and metadata filtering.</p></li>
</ul>
<p>By completing the Data Ingestion and Embedding Generation pipelines, we have effectively built our feature engineering process and populated our feature store.</p>
<hr class="docutils" />
<!--

### 7. Model Development & Iteration: The Path to a Production-Ready Summarizer

The goal of this phase is not to simply pick a model, but to systematically identify the most effective and efficient solution that balances performance, cost, and operational control. This requires a structured series of experiments, starting with simple baselines and progressively incorporating more sophisticated techniques.

#### 7.1 Phase 0: Aligning on Evaluation Metrics

Before the first experiment, we established a non-negotiable evaluation framework to objectively measure the quality of every generated summary. This framework combines automated metrics with LLM-as-a-judge for qualitative assessment.

| Metric | Tool/Method | What It Measures |
| :--- | :--- | :--- |
| **Faithfulness** | **Ragas** | **Factual Grounding.** Does the summary contain fabricated information ("hallucinations"), or is every statement directly supported by the provided reviews? (Scale 0-1, higher is better) |
| **Context Precision** | **Ragas** | **Relevance of Retrieval.** In our RAG system, are the retrieved review snippets actually relevant and used in the final summary? (Scale 0-1, higher is better) |
| **Coherence** | **LLM-as-judge (GPT-4)**| **Logical Flow.** Is the summary well-structured and easy to understand? Does it present a logical argument? (Scale 1-5, higher is better) |
| **Conciseness** | **LLM-as-judge (GPT-4)**| **Brevity & Information Density.** Does the summary get straight to the point without unnecessary filler words? (Scale 1-5, higher is better) |

#### 7.2 The Experimentation Log

The following table documents our iterative journey, capturing the hypothesis, method, results, and decision at each step.

| Exp. # | Hypothesis | Model & Method | Key Results | Decision & Next Steps |
| :--- | :--- | :--- | :--- | :--- |
| **1** | Establish a non-ML baseline to quantify the value of an LLM approach. | **Heuristic Baseline:** Extract the top 3 longest sentences from 5-star reviews and the top 3 longest from 1-star reviews. Concatenate them. | **Faithfulness:** N/A <br> **Coherence:** 1.5/5 | **Result:** The output is disjointed, often ungrammatical, and lacks narrative flow. **Decision:** Confirms that a simple heuristic is inadequate and a sophisticated summarization model is necessary. |
| **2** | A general-purpose open-source LLM can produce a decent summary with a basic prompt. | **Model:** `Llama-2-7B-Chat` <br> **Method:** Zero-shot with a simple prompt: "Summarize these reviews." | **Faithfulness:** 0.75 <br> **Coherence:** 3.0/5 | **Result:** Output is readable but often misses key negative points and fails to follow a consistent "Pros/Cons" format. **Decision:** A good starting point, but prompt and model need significant improvement. |
| **3** | Advanced prompt engineering will improve structural consistency and control model output. | **Model:** `Llama-2-7B-Chat` <br> **Method:** Multi-shot prompting with examples. Chain-of-Thought ("think step-by-step"). Strict instructions for JSON output with "pros" and "cons" keys. | **Faithfulness:** 0.80 <br> **Coherence:** 3.8/5 | **Result:** Structure and formatting are dramatically improved. The model is more steerable. **Decision:** Prompt engineering is a high-leverage activity. This becomes our new baseline prompt structure. |
| **4** | A state-of-the-art commercial model will define the "upper bound" for quality, regardless of cost. | **Model:** `GPT-4 API` <br> **Method:** Using the best-performing prompt from Exp #3. | **Faithfulness:** 0.92 <br> **Coherence:** 4.7/5 | **Result:** Exceptionally high-quality, fluent, and nuanced summaries. It sets the gold standard for what is possible. **Decision:** This becomes our "North Star" metric to aim for with a more cost-effective, self-hosted solution. |
| **5** | A newer, more efficient open-source model can approach the quality of larger models. | **Model:** `Mistral-7B-Instruct-v0.1` <br> **Method:** Zero-shot with our best prompt. | **Faithfulness:** 0.88 <br> **Coherence:** 4.2/5 | **Result:** Remarkably strong performance, significantly outperforming Llama 2 7B and getting closer to the GPT-4 benchmark. **Decision:** Mistral 7B becomes our primary candidate model for further optimization. |
| **6** | The base Mistral model will struggle with our non-English reviews. | **Model:** `Mistral-7B-Instruct-v0.1` <br> **Method:** Evaluate specifically on a slice of German and French reviews. | **English Faithfulness:** 0.88 <br> **DE/FR Faithfulness:** 0.65 | **Result:** **Critical finding.** The model's performance degrades significantly on non-English text, producing more generic or factually inconsistent summaries. The base model's multilingual capability is insufficient. |
| **7** | Fine-tuning on a multilingual dataset will improve performance on non-English languages and overall domain-specific quality. | **Model:** `Mistral-7B-Instruct-v0.1` <br> **Method:** Parameter-Efficient Fine-Tuning (PEFT) using LoRA. <br> **Dataset:** A curated sample from the public `amazon_reviews_multi` dataset. | **Overall Faithfulness:** 0.95 <br> **DE/FR Faithfulness:** 0.91 <br> **Coherence:** 4.5/5 | **Result:** **Major breakthrough.** Fine-tuning dramatically improves performance, closing the gap with GPT-4, especially on the critical multilingual weakness. **Decision:** The fine-tuned model is now our lead production candidate. |
| **8** | Refining the RAG retrieval strategy will further boost factual consistency by providing the LLM with higher-signal context. | **Model:** Fine-tuned Mistral 7B <br> **Method:** Evolve RAG from "top-k semantic similarity" to a rule-based retrieval: "Fetch top 3 most helpful positive snippets, top 3 most helpful negative, and 2 most recent." | **Faithfulness:** 0.97 <br> **Context Precision:** 0.96 | **Result:** This change leads to the highest faithfulness score yet by ensuring the LLM is always presented with a balanced and highly relevant context, mitigating model bias. **Decision:** This advanced RAG strategy is adopted for the production system. |

#### 7.3 Final Decision: Selecting the Production Candidate

The experimentation process led to a clear winner. While the GPT-4 API provided the highest out-of-the-box quality, the **fine-tuned Mistral-7B model combined with the advanced RAG strategy** emerged as the optimal choice for our production system.

| Factor | GPT-4 API | Fine-Tuned Mistral 7B (Self-Hosted) | Decision Rationale |
| :--- | :--- | :--- | :--- |
| **Performance** | **Excellent (Coherence: 4.7)** | **Excellent (Coherence: 4.5)** | Performance is comparable and meets the quality bar for a production experience. |
| **Factual Consistency** | Good (Faithfulness: 0.92) | **Superior (Faithfulness: 0.97)** | The combination of domain-specific fine-tuning and a targeted RAG strategy makes our self-hosted model *more* factually grounded. |
| **Cost** | High ($ per 1k summaries) | **Low** (Fixed hourly cost for GPU instance) | At our scale, the self-hosted model is an order of magnitude cheaper, making it economically viable. |
| **Latency** | Moderate-High (API network dependent) | **Low & Predictable** (Within our own VPC) | Self-hosting provides full control over the serving infrastructure, allowing us to meet our strict latency SLOs. |
| **Data Privacy & Control** | Data sent to third-party | **Full Control** | All data remains within our own cloud environment, eliminating third-party data privacy concerns—a critical factor for European user data. |

This data-driven, iterative process allowed us to develop a model that not only meets the quality bar set by the best commercial offerings but exceeds them on the most critical metric (faithfulness) while providing significant advantages in cost, latency, and data governance.

-->
</section>
</section>
<hr class="docutils" />
<section id="ml-training-pipeline-planning-the-continuous-fine-tuning-workflow">
<h3>8. ML Training Pipeline: Planning the Continuous Fine-Tuning Workflow<a class="headerlink" href="#ml-training-pipeline-planning-the-continuous-fine-tuning-workflow" title="Permalink to this heading">¶</a></h3>
<p>To ensure our summarization model adapts to new product types, evolving customer language, and emerging review patterns, a “train-once” approach is insufficient. We need an automated <strong>Continuous Training (CT) pipeline</strong> that can periodically fine-tune our model on fresh data, evaluate its performance, and register it for production deployment if it proves superior.</p>
<p>This section outlines the comprehensive plan for the artifacts required to build this critical pipeline.</p>
<section id="python-scripts-pipeline-components">
<h4><strong>8.1 Python Scripts (Pipeline Components)</strong><a class="headerlink" href="#python-scripts-pipeline-components" title="Permalink to this heading">¶</a></h4>
<p>The pipeline’s logic will be encapsulated in a series of modular Python scripts, each designed to be executed as a containerized task orchestrated by Airflow.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Component Script</p></th>
<th class="head text-left"><p>Description</p></th>
<th class="head text-left"><p>Key Libraries / Frameworks</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">1_data_selection.py</span></code></strong></p></td>
<td class="text-left"><p>Selects the data for the fine-tuning job. It combines a fresh, curated sample of the public multilingual dataset with a sample of high-quality, recently collected internal reviews (from the S3 Data Lake).</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">pandas</span></code>, <code class="docutils literal notranslate"><span class="pre">DVC</span> <span class="pre">API</span></code>, <code class="docutils literal notranslate"><span class="pre">pyarrow</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">2_data_validation.py</span></code></strong></p></td>
<td class="text-left"><p>Validates the selected training dataset against a predefined set of rules (e.g., schema checks, text length validation, no nulls) to prevent training on corrupted data.</p></td>
<td class="text-left"><p>Great Expectations.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">3_model_training.py</span></code></strong></p></td>
<td class="text-left"><p>The core fine-tuning component. It loads the base model (<code class="docutils literal notranslate"><span class="pre">Mistral-7B</span></code>), applies the PEFT/LoRA configuration, and runs the training job on the validated dataset using a managed GPU cluster.</p></td>
<td class="text-left"><p>Hugging Face <code class="docutils literal notranslate"><span class="pre">transformers</span></code>, <code class="docutils literal notranslate"><span class="pre">peft</span></code>, <code class="docutils literal notranslate"><span class="pre">accelerate</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">4_model_evaluation.py</span></code></strong></p></td>
<td class="text-left"><p>Evaluates the newly fine-tuned model artifact against the “golden” evaluation dataset. It generates summaries and scores them using a multi-faceted approach.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">Ragas</span></code> (for RAG metrics), <code class="docutils literal notranslate"><span class="pre">Giskard</span></code> (for behavioral tests), <code class="docutils literal notranslate"><span class="pre">openai</span></code> (for LLM-as-a-judge).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">5_model_registration.py</span></code></strong></p></td>
<td class="text-left"><p>The final governance gate. It compares the new model’s evaluation scores against the currently deployed production model. If the new model meets the promotion criteria, it is versioned and registered in the MLflow Model Registry.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">mlflow</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="unit-tests-pytest">
<h4><strong>8.2 Unit Tests (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong><a class="headerlink" href="#unit-tests-pytest" title="Permalink to this heading">¶</a></h4>
<p>To ensure the reliability of each component, we will implement a suite of unit tests.</p>
<ul class="simple">
<li><p><strong>Test Data Selection:</strong> Verify that <code class="docutils literal notranslate"><span class="pre">1_data_selection.py</span></code> correctly loads data from mock S3 paths and handles empty or missing data gracefully.</p></li>
<li><p><strong>Test Training Script Logic:</strong> Test the setup functions within <code class="docutils literal notranslate"><span class="pre">3_model_training.py</span></code> to ensure it correctly parses configurations and loads the base model and tokenizer, without running a full training loop.</p></li>
<li><p><strong>Test Evaluation Logic:</strong> For <code class="docutils literal notranslate"><span class="pre">4_model_evaluation.py</span></code>, provide a small, fixed set of pre-generated summaries and verify that the metric calculation functions (e.g., parsing Ragas output) work as expected.</p></li>
<li><p><strong>Test Registration Logic:</strong> For <code class="docutils literal notranslate"><span class="pre">5_model_registration.py</span></code>, test the decision-making function by passing it various mock metric scores (e.g., “new model better,” “new model worse,” “new model marginally better but below threshold”) and asserting the correct outcome (register or skip).</p></li>
</ul>
</section>
<section id="pipeline-code-airflow-dag">
<h4><strong>8.3 Pipeline Code (Airflow DAG)</strong><a class="headerlink" href="#pipeline-code-airflow-dag" title="Permalink to this heading">¶</a></h4>
<p>The components will be orchestrated into a single, cohesive pipeline using an Airflow DAG.</p>
<ul class="simple">
<li><p><strong>File:</strong> <code class="docutils literal notranslate"><span class="pre">llm_finetuning_dag.py</span></code></p></li>
<li><p><strong>Trigger:</strong> Scheduled to run monthly and can be triggered manually for on-demand retraining.</p></li>
<li><p><strong>Tasks:</strong></p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data_selection_task</span></code>: Executes the data selection script.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_validation_task</span></code>: Executes the validation script. Fails the pipeline if data quality checks do not pass.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_training_task</span></code>: Executes the fine-tuning script as an <strong>Amazon SageMaker Training Job</strong>. This offloads the heavy GPU computation to a managed, ephemeral cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_evaluation_task</span></code>: Executes the evaluation script on a CPU instance once training is complete.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_registration_task</span></code>: Executes the registration script, making the final go/no-go decision.</p></li>
</ol>
</li>
</ul>
</section>
<section id="infrastructure-as-code-terraform">
<h4><strong>8.4 Infrastructure as Code (Terraform)</strong><a class="headerlink" href="#infrastructure-as-code-terraform" title="Permalink to this heading">¶</a></h4>
<p>The necessary AWS infrastructure for this pipeline will be defined declaratively.</p>
<ul class="simple">
<li><p><strong>IAM Role for SageMaker:</strong> A dedicated IAM role for SageMaker Training Jobs, granting it least-privilege access to read data from S3, pull the base model from Hugging Face, and write artifacts back to S3.</p></li>
<li><p><strong>IAM Role for Airflow:</strong> An execution role for the MWAA environment, allowing it to trigger SageMaker jobs and log to CloudWatch.</p></li>
<li><p><strong>ECR Repository:</strong> A repository to store the Docker images for our custom pipeline components.</p></li>
</ul>
</section>
<section id="integration-test">
<h4><strong>8.5 Integration Test</strong><a class="headerlink" href="#integration-test" title="Permalink to this heading">¶</a></h4>
<p>A CI/CD workflow will run an automated integration test on the entire pipeline.</p>
<ul class="simple">
<li><p><strong>Process:</strong> The test will trigger the Airflow DAG in a staging environment. It will use a tiny, dedicated test dataset. The <code class="docutils literal notranslate"><span class="pre">model_training_task</span></code> will be configured to run for only a few steps to produce a dummy model artifact quickly.</p></li>
<li><p><strong>Goal:</strong> The test passes if the DAG runs to completion without errors and a new (dummy) model version is successfully registered in a staging MLflow instance. This validates that all components, permissions, and configurations work together correctly.</p></li>
</ul>
</section>
<section id="id3">
<h4><strong>8.6 Architecture Diagram</strong><a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_training.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="ci-cd-workflow-github-actions">
<h4><strong>8.7 CI/CD Workflow (GitHub Actions)</strong><a class="headerlink" href="#ci-cd-workflow-github-actions" title="Permalink to this heading">¶</a></h4>
<p>The deployment and testing of the pipeline code itself will be automated.</p>
<ul class="simple">
<li><p><strong>File:</strong> <code class="docutils literal notranslate"><span class="pre">.github/workflows/deploy_finetuning_pipeline.yml</span></code></p></li>
<li><p><strong>Trigger:</strong> On push to the <code class="docutils literal notranslate"><span class="pre">main</span></code> branch.</p></li>
<li><p><strong>Jobs:</strong></p>
<ol class="arabic simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">lint-and-unit-test</span></code>:</strong> Runs static code analysis (<code class="docutils literal notranslate"><span class="pre">flake8</span></code>) and <code class="docutils literal notranslate"><span class="pre">pytest</span></code> for all unit tests.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">build-and-push-images</span></code>:</strong> Builds Docker images for the pipeline components and pushes them to our Amazon ECR repository.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">run-integration-test</span></code>:</strong> (If previous job succeeds) Triggers the integration test on the staging Airflow environment.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">deploy-to-production</span></code>:</strong> (If integration test succeeds) Syncs the updated Airflow DAG file to the production S3 bucket, allowing MWAA to deploy the new pipeline version automatically.</p></li>
</ol>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="batch-inference-pipeline-planning-the-production-summarization-workflow">
<h3>9. Batch Inference Pipeline: Planning the Production Summarization Workflow<a class="headerlink" href="#batch-inference-pipeline-planning-the-production-summarization-workflow" title="Permalink to this heading">¶</a></h3>
<p>This pipeline is the workhorse of the system. It runs frequently to ensure summaries are kept up-to-date with the latest customer feedback, using the RAG-based strategy we’ve established for quality and control. This pipeline’s objective is to efficiently generate and cache high-quality summaries for all products that have received new reviews, ensuring that the information presented to users is fresh and relevant.</p>
<section id="id4">
<h4><strong>9.1 Python Scripts (Pipeline Components)</strong><a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h4>
<p>The batch inference process is broken down into a sequence of focused, testable Python scripts.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Component Script</p></th>
<th class="head text-left"><p>Description</p></th>
<th class="head text-left"><p>Key Libraries / Frameworks</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">1_get_products_to_update.py</span></code></strong></p></td>
<td class="text-left"><p>Queries the application’s production database to get a list of <code class="docutils literal notranslate"><span class="pre">product_id</span></code>s that have received new reviews since the last successful pipeline run.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">psycopg2</span></code> (or other DB driver), <code class="docutils literal notranslate"><span class="pre">pandas</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">2_retrieve_rag_context.py</span></code></strong></p></td>
<td class="text-left"><p>For each product ID, executes the defined RAG strategy: queries the Vector DB (OpenSearch) to retrieve the most relevant review snippets (e.g., top positive, top negative, most recent).</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">opensearch-py</span></code>, LangChain (for prompt templating).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">3_generate_summaries.py</span></code></strong></p></td>
<td class="text-left"><p>Takes the retrieved contexts, formats them into prompts, and sends them in batches to the production LLM serving endpoint. Handles API responses and error conditions.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">requests</span></code>, <code class="docutils literal notranslate"><span class="pre">httpx</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">4_validate_and_cache.py</span></code></strong></p></td>
<td class="text-left"><p>Receives the raw JSON summaries from the LLM. Performs a basic validation (e.g., checks for required keys). Writes the valid summaries to the low-latency cache (DynamoDB) with a defined TTL.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">boto3</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="id5">
<h4><strong>9.2 Unit Tests (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong><a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h4>
<p>Each script will be accompanied by unit tests to guarantee its logic and error handling.</p>
<ul class="simple">
<li><p><strong>Test Product Retrieval:</strong> For <code class="docutils literal notranslate"><span class="pre">1_get_products_to_update.py</span></code>, mock the database connection and test that the correct product IDs are returned based on mock timestamps.</p></li>
<li><p><strong>Test RAG Logic:</strong> For <code class="docutils literal notranslate"><span class="pre">2_retrieve_rag_context.py</span></code>, mock the OpenSearch client. Verify that the script constructs the correct, complex query based on our RAG strategy and correctly assembles the final prompt.</p></li>
<li><p><strong>Test API Invocation:</strong> For <code class="docutils literal notranslate"><span class="pre">3_generate_summaries.py</span></code>, mock the inference service endpoint. Test its ability to handle successful (200) responses, rate limiting (429), and server errors (500).</p></li>
<li><p><strong>Test Caching Logic:</strong> For <code class="docutils literal notranslate"><span class="pre">4_validate_and_cache.py</span></code>, mock the DynamoDB client. Test that valid JSON summaries are written correctly and that malformed summaries are logged as errors.</p></li>
</ul>
</section>
<section id="id6">
<h4><strong>9.3 Pipeline Code (Airflow DAG)</strong><a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h4>
<p>An Airflow DAG will orchestrate the execution of these components in a reliable, scheduled manner.</p>
<ul class="simple">
<li><p><strong>File:</strong> <code class="docutils literal notranslate"><span class="pre">batch_inference_dag.py</span></code></p></li>
<li><p><strong>Trigger:</strong> Scheduled to run hourly.</p></li>
<li><p><strong>Tasks:</strong></p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">get_products_to_update_task</span></code>: Executes the product retrieval script.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">check_if_products_exist_task</span></code>: A <code class="docutils literal notranslate"><span class="pre">BranchPythonOperator</span></code> that checks if the previous task returned any products. If not, the DAG skips to a final “success” state to avoid running expensive tasks unnecessarily.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">retrieve_rag_context_task</span></code>: Executes the RAG context retrieval script.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">generate_summaries_task</span></code>: Executes the summary generation script.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">validate_and_cache_task</span></code>: Executes the final caching script.</p></li>
</ol>
</li>
</ul>
</section>
<section id="id7">
<h4><strong>9.4 Infrastructure as Code (Terraform)</strong><a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h4>
<p>The AWS resources supporting this pipeline will be defined as code.</p>
<ul class="simple">
<li><p><strong>Amazon DynamoDB Table:</strong> Definition of the low-latency cache table, including its partition key (<code class="docutils literal notranslate"><span class="pre">product_id</span></code>), attributes (<code class="docutils literal notranslate"><span class="pre">summary_json</span></code>, <code class="docutils literal notranslate"><span class="pre">last_updated</span></code>), and TTL configuration.</p></li>
<li><p><strong>IAM Role for Airflow:</strong> An execution role for MWAA granting permissions to:</p>
<ul>
<li><p>Read from the production application database.</p></li>
<li><p>Query the Amazon OpenSearch cluster.</p></li>
<li><p>Invoke the production EKS inference endpoint (requires VPC networking and security group configuration).</p></li>
<li><p>Write items to the DynamoDB cache table.</p></li>
</ul>
</li>
</ul>
</section>
<section id="id8">
<h4><strong>9.5 Integration Test</strong><a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h4>
<p>A CI/CD workflow will validate the entire pipeline end-to-end in a staging environment.</p>
<ul class="simple">
<li><p><strong>Process:</strong> The test will pre-populate a staging database and staging Vector DB with a few sample reviews for a test product. It will then trigger the <code class="docutils literal notranslate"><span class="pre">batch_inference_dag</span></code> in the staging Airflow environment.</p></li>
<li><p><strong>Goal:</strong> The test passes if, after the DAG completes, the script can successfully query the staging DynamoDB table and find a newly generated summary for the test product. This verifies that all permissions, connections, and logic work in concert.</p></li>
</ul>
</section>
<section id="id9">
<h4><strong>9.6 Architecture Diagram</strong><a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_inference.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="id10">
<h4><strong>9.7 CI/CD Workflow (GitHub Actions)</strong><a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h4>
<p>This workflow automates the testing and deployment of the <em>pipeline code itself</em>.</p>
<ul class="simple">
<li><p><strong>File:</strong> <code class="docutils literal notranslate"><span class="pre">.github/workflows/deploy_batch_inference_pipeline.yml</span></code></p></li>
<li><p><strong>Trigger:</strong> On push to the <code class="docutils literal notranslate"><span class="pre">main</span></code> branch.</p></li>
<li><p><strong>Jobs:</strong></p>
<ol class="arabic simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">lint-and-unit-test</span></code>:</strong> Runs static code analysis and <code class="docutils literal notranslate"><span class="pre">pytest</span></code> for all batch pipeline scripts.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">deploy-dag-to-staging</span></code>:</strong> Deploys the updated Airflow DAG to the staging environment.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">run-integration-test</span></code>:</strong> Triggers the end-to-end integration test on the staging Airflow environment.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">trigger-load-test</span></code>:</strong></p>
<ul>
<li><p><strong>Condition:</strong> This job <strong>only runs if</strong> specific files that affect the load pattern have changed (e.g., <code class="docutils literal notranslate"><span class="pre">generate_summaries.py</span></code>, <code class="docutils literal notranslate"><span class="pre">retrieve_rag_context.py</span></code>). We can use path filters in GitHub Actions for this.</p></li>
<li><p><strong>Action:</strong> Calls the <code class="docutils literal notranslate"><span class="pre">reusable_load_test.yml</span></code> workflow, targeting the <strong>main production serving endpoint</strong>. This validates that our <em>new client code</em> works correctly with the <em>current production server</em>.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">deploy-dag-to-production</span></code>:</strong> (If all previous jobs succeed) Syncs the updated Airflow DAG to the production MWAA environment.</p></li>
</ol>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="model-llm-serving-endpoint-deployment-pipeline">
<h3>Model (LLM Serving Endpoint) Deployment Pipeline<a class="headerlink" href="#model-llm-serving-endpoint-deployment-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="why-a-shared-reusable-load-test">
<h4>Why a shared, reusable Load Test ?<a class="headerlink" href="#why-a-shared-reusable-load-test" title="Permalink to this heading">¶</a></h4>
<p>There are two distinct deployment processes,</p>
<ol class="arabic simple">
<li><p><strong>Deploying the Batch Inference Pipeline Logic (Airflow DAG):</strong> This happens when we change the <em>orchestration code</em>. Its CI/CD checks that the DAG runs correctly.</p></li>
<li><p><strong>Deploying the Model Endpoint (EKS Service):</strong> This happens when a <em>new model is promoted</em>. This is where we must validate the performance and stability of the model server itself before it handles production requests from our batch pipeline. This is the critical workflow that is triggered whenever a new model is approved and needs to be rolled out to production. It ensures that the new model not only works correctly but also performs efficiently under load.</p></li>
</ol>
<p>The key insight is that the <strong>Batch Inference Pipeline <em>is the primary client</em></strong> for our LLM Serving Endpoint. The performance of the endpoint is not just a property of the model itself; it’s a function of the interaction between the server and the client that calls it.</p>
<p>Therefore:</p>
<ol class="arabic simple">
<li><p><strong>Changing the Server (Model Endpoint):</strong> Deploying a new model (e.g., a larger one, or one with a different quantization level) will obviously change performance characteristics. This <strong>must</strong> be load tested.</p></li>
<li><p><strong>Changing the Client (Batch Pipeline):</strong> A seemingly innocuous change in the batch pipeline’s code can dramatically alter the load pattern. For example:</p>
<ul class="simple">
<li><p>Changing the RAG retrieval logic in <code class="docutils literal notranslate"><span class="pre">retrieve_rag_context.py</span></code> could create much larger prompt contexts, increasing the processing time per request.</p></li>
<li><p>Changing the parallelism or batching logic in <code class="docutils literal notranslate"><span class="pre">generate_summaries.py</span></code> could alter the number of concurrent requests sent to the endpoint.</p></li>
</ul>
</li>
</ol>
<p>A change in either component can break the system’s performance. Therefore, any significant change to either the client (the pipeline) or the server (the endpoint) must trigger a load test to validate their interaction. The optimal solution is not to have two separate load tests, but to have a single, <strong>reusable load testing workflow</strong> that can be called by <em>both</em> CI/CD pipelines.</p>
</section>
<section id="ci-cd-for-the-llm-serving-endpoint-with-the-shared-load-test">
<h4>CI/CD for the LLM Serving Endpoint (with the shared Load Test)<a class="headerlink" href="#ci-cd-for-the-llm-serving-endpoint-with-the-shared-load-test" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><strong>File:</strong> <code class="docutils literal notranslate"><span class="pre">.github/workflows/deploy_llm_serving_endpoint.yml</span></code></p></li>
<li><p><strong>Trigger:</strong> A model is manually promoted from “Staging” to “Production” in the MLflow Model Registry.</p></li>
<li><p><strong>Jobs:</strong></p></li>
</ul>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Job</p></th>
<th class="head text-left"><p>Description</p></th>
<th class="head text-left"><p>Key Artifacts / Outputs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Build Container</strong></p></td>
<td class="text-left"><p>- Fetches the newly promoted model artifacts (LoRA weights, config) from MLflow.<br>- Builds a new Docker container with the inference server (vLLM/TGI) and the model artifacts baked in.<br>- Pushes the versioned container image to Amazon ECR.</p></td>
<td class="text-left"><p>- A new, immutable Docker image in ECR tagged with the model version.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Deploy to Staging</strong></p></td>
<td class="text-left"><p>- Uses Terraform/CDK to deploy the new container to a <em>separate staging endpoint</em> in the EKS cluster. This endpoint is identical to production but does not receive live traffic.</p></td>
<td class="text-left"><p>- A live, but isolated, staging/canary model endpoint.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Smoke &amp; Integration Test</strong></p></td>
<td class="text-left"><p>- Runs a small suite of tests against the staging endpoint:<br>  • <strong>Health Check:</strong> Is the endpoint responsive?<br>  • <strong>API Contract Test:</strong> Does the request/response schema match?<br>  • <strong>Consistency Check:</strong> Does the endpoint give the exact same prediction for a sample input as it did during offline evaluation?</p></td>
<td class="text-left"><p>- A go/no-go signal for further testing.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Load Test</strong></p></td>
<td class="text-left"><p>- If smoke tests pass, a <code class="docutils literal notranslate"><span class="pre">Locust</span></code> load test is automatically triggered against the <strong>staging endpoint</strong>.<br>- The test simulates the expected load from our hourly batch inference job (e.g., thousands of requests in a short burst).<br>- It measures and asserts that P95 latency and tokens/second throughput are within our defined SLOs.</p></td>
<td class="text-left"><p>- Performance metrics (latency, throughput, error rate).<br>- A go/no-go signal based on whether the endpoint meets its performance targets.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Phased Rollout to Production</strong></p></td>
<td class="text-left"><p>- If the load test passes, the workflow begins a safe, automated rollout to the main production endpoint.<br>- <strong>Canary Strategy:</strong> It updates the EKS service to direct a small percentage of traffic (e.g., 10%) from the live batch pipeline to the new model version.<br>- The workflow pauses and monitors key metrics (latency, error rates from CloudWatch) for a predefined period.</p></td>
<td class="text-left"><p>- A new model version serving a small fraction of production traffic.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>6. Promote to 100%</strong></p></td>
<td class="text-left"><p>- If the canary phase shows no issues, the workflow automatically proceeds to shift 100% of the production traffic to the new model version.<br>- The old model version is kept running for a short period to allow for a fast rollback if needed.</p></td>
<td class="text-left"><p>- The new model is now the fully live “champion” model in production.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="monitoring-and-observability-ensuring-production-health-and-quality">
<h3>10. Monitoring and Observability: Ensuring Production Health and Quality<a class="headerlink" href="#monitoring-and-observability-ensuring-production-health-and-quality" title="Permalink to this heading">¶</a></h3>
<p>Once deployed, our system enters its most critical phase. Monitoring is not a passive activity but a proactive, multi-layered strategy designed to provide a holistic view of the system’s health, from the underlying infrastructure to the data flowing through it, and—most importantly—the quality of the LLM’s generated summaries.</p>
<p>Our strategy is built on three distinct pillars:</p>
<ol class="arabic simple">
<li><p><strong>System &amp; Operational Health:</strong> Is the infrastructure running correctly?</p></li>
<li><p><strong>Data Quality &amp; Drift:</strong> Are the inputs to our model trustworthy?</p></li>
<li><p><strong>Model Quality &amp; Performance:</strong> Are the outputs of our model accurate, safe, and helpful?</p></li>
</ol>
<section id="monitoring-and-alerting-plan">
<h4><strong>Monitoring and Alerting Plan</strong><a class="headerlink" href="#monitoring-and-alerting-plan" title="Permalink to this heading">¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Category</p></th>
<th class="head text-left"><p>Specific Metric</p></th>
<th class="head text-left"><p>Tool / Method</p></th>
<th class="head text-left"><p>Alerting Threshold &amp; Recipient (Example)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. System &amp; Operational Health</strong></p></td>
<td class="text-left"><p><strong>Batch Inference DAG Success Rate</strong></p></td>
<td class="text-left"><p><strong>Amazon CloudWatch</strong> (from MWAA logs)</p></td>
<td class="text-left"><p><strong>Alert:</strong> On any DAG run failure.<br><strong>Recipient:</strong> On-call Data Engineer, On-call MLOps Engineer.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>EKS Pod Health (Serving Endpoint)</strong></p></td>
<td class="text-left"><p><strong>Prometheus / Grafana</strong></p></td>
<td class="text-left"><p><strong>Alert:</strong> When the number of ready pods is less than the desired replica count for &gt; 5 mins.<br><strong>Recipient:</strong> On-call MLOps Engineer.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>GPU Utilization (Serving Endpoint)</strong></p></td>
<td class="text-left"><p><strong>Prometheus</strong> (via DCGM exporter)</p></td>
<td class="text-left"><p><strong>Alert:</strong> If average GPU utilization during a batch run is &lt; 30% (indicates inefficiency) or &gt; 95% for an extended period (indicates system is overloaded).<br><strong>Recipient:</strong> On-call MLOps Engineer.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>API Error Rate (Serving Endpoint)</strong></p></td>
<td class="text-left"><p><strong>Prometheus / Grafana</strong></p></td>
<td class="text-left"><p><strong>Alert:</strong> If the rate of HTTP 5xx errors exceeds 2% over a 10-minute window.<br><strong>Recipient:</strong> On-call MLOps Engineer.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Cache Performance (DynamoDB)</strong></p></td>
<td class="text-left"><p><strong>Amazon CloudWatch</strong></p></td>
<td class="text-left"><p><strong>Alert:</strong> On sustained increases in read/write latency or throttled requests.<br><strong>Recipient:</strong> On-call Data Engineer.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Data Quality &amp; Drift</strong></p></td>
<td class="text-left"><p><strong>Input Data Drift (Semantic)</strong></p></td>
<td class="text-left"><p><strong>Custom Airflow DAG</strong> using Python (<code class="docutils literal notranslate"><span class="pre">scipy</span></code>, <code class="docutils literal notranslate"><span class="pre">evidently.ai</span></code>).</p></td>
<td class="text-left"><p><strong>Alert:</strong> If Population Stability Index (PSI) between daily review embeddings and the training set baseline exceeds 0.25.<br><strong>Recipient:</strong> Data Engineering &amp; MLOps teams (as a non-urgent ticket).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Input Data Properties Drift</strong></p></td>
<td class="text-left"><p><strong>Custom Airflow DAG</strong></p></td>
<td class="text-left"><p><strong>Alert:</strong> If the distribution of <code class="docutils literal notranslate"><span class="pre">star_rating</span></code> or <code class="docutils literal notranslate"><span class="pre">language</span></code> changes by &gt; 20% week-over-week.<br><strong>Recipient:</strong> Data Engineering team.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>PII Leakage Rate (Input)</strong></p></td>
<td class="text-left"><p><strong>Great Expectations</strong> step in the ingestion pipeline.</p></td>
<td class="text-left"><p><strong>Alert:</strong> On <em>any</em> detection of PII. This is a critical failure.<br><strong>Recipient:</strong> High-priority alert to Legal, Security, and Data Engineering teams.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Model Quality &amp; Performance</strong></p></td>
<td class="text-left"><p><strong>Summary Faithfulness (Hallucinations)</strong></p></td>
<td class="text-left"><p><strong>Automated Quality Monitoring DAG</strong> using <code class="docutils literal notranslate"><span class="pre">Ragas</span></code>.</p></td>
<td class="text-left"><p><strong>CRITICAL ALERT:</strong> If the average Faithfulness score of the daily sample drops below 0.95. This is the most important quality metric.<br><strong>Recipient:</strong> Immediate page to On-call MLOps Engineer. May trigger an automated pause of the batch pipeline.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Summary Coherence &amp; Fluency</strong></p></td>
<td class="text-left"><p><strong>Automated Quality Monitoring DAG</strong> using LLM-as-a-judge (GPT-4o).</p></td>
<td class="text-left"><p><strong>Warning:</strong> If the average coherence score drops below 4.0/5. <br><strong>Recipient:</strong> A ticket is automatically created for the MLOps team to investigate in the next sprint.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Toxicity &amp; Safety (Output)</strong></p></td>
<td class="text-left"><p><strong>Automated Quality Monitoring DAG</strong> using a toxicity classifier.</p></td>
<td class="text-left"><p><strong>CRITICAL ALERT:</strong> If the rate of generated summaries flagged as toxic exceeds 0.1%.<br><strong>Recipient:</strong> Immediate page to MLOps and Product teams.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Format Adherence</strong></p></td>
<td class="text-left"><p><strong>Automated Quality Monitoring DAG</strong> (simple JSON schema validation).</p></td>
<td class="text-left"><p><strong>Alert:</strong> If &gt; 1% of sampled summaries fail to parse or do not contain the required “pros” and “cons” keys.<br><strong>Recipient:</strong> MLOps team.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>RAG Context Relevance</strong></p></td>
<td class="text-left"><p><strong>Automated Quality Monitoring DAG</strong> using <code class="docutils literal notranslate"><span class="pre">Ragas</span></code> (Context Precision).</p></td>
<td class="text-left"><p><strong>Warning:</strong> If the context precision score drops, indicating the retrieval logic may be degrading. <br><strong>Recipient:</strong> MLOps team ticket.</p></td>
</tr>
</tbody>
</table>
</div>
<p>This comprehensive monitoring plan transforms our system from a “fire-and-forget” deployment into a closely observed, self-diagnosing service. By setting clear SLOs and automated alerts for system health, data integrity, and model quality, we create the necessary feedback loops to maintain a high-quality user experience and build long-term trust in our GenAI application.</p>
</section>
</section>
<hr class="docutils" />
<section id="closing-the-loop-continual-learning-production-testing">
<h3>11. Closing the Loop: Continual Learning &amp; Production Testing<a class="headerlink" href="#closing-the-loop-continual-learning-production-testing" title="Permalink to this heading">¶</a></h3>
<p>A deployed model is the beginning of the journey, not the end. In the dynamic environment of e-commerce, customer language evolves, new products introduce new vocabularies, and the meaning of a “good” review can shift. Our system must adapt to these changes to remain effective. This phase focuses on the strategies for model evolution and the rigorous methods for validating its real-world business impact.</p>
<section id="continual-learning-retraining-strategy">
<h4>11.1 Continual Learning &amp; Retraining Strategy<a class="headerlink" href="#continual-learning-retraining-strategy" title="Permalink to this heading">¶</a></h4>
<p>The goal is to keep our summarization model accurate and relevant without incurring the prohibitive cost of retraining from scratch.</p>
<ul class="simple">
<li><p><strong>Triggers for Retraining:</strong> Our Continuous Training (CT) pipeline is not run arbitrarily. It is triggered by two specific conditions:</p>
<ol class="arabic simple">
<li><p><strong>Reactive (Alert-Driven):</strong> An alert from our automated quality monitoring system signals a significant degradation in production model performance (e.g., the Faithfulness score drops below our 0.95 SLO). This is an emergency trigger to remediate a live issue.</p></li>
<li><p><strong>Proactive (Scheduled):</strong> A scheduled trigger (e.g., quarterly) ensures the model is periodically refreshed with the latest data, capturing gradual concept drift and new language patterns before they become a major issue.</p></li>
</ol>
</li>
<li><p><strong>The Retraining Process:</strong>
When triggered, the planned <code class="docutils literal notranslate"><span class="pre">llm_finetuning_dag</span></code> executes the PEFT/LoRA fine-tuning process on a newly curated dataset. This dataset includes a fresh sample of high-quality reviews collected since the last run, combined with a representative sample of older data to mitigate catastrophic forgetting.</p></li>
</ul>
</section>
<section id="mitigating-catastrophic-forgetting-a-core-llm-challenge">
<h4>11.2 Mitigating Catastrophic Forgetting: A Core LLM Challenge<a class="headerlink" href="#mitigating-catastrophic-forgetting-a-core-llm-challenge" title="Permalink to this heading">¶</a></h4>
<p><strong>The Problem:</strong> The primary risk of continual learning is <strong>catastrophic forgetting</strong>. If we fine-tune our model solely on a new wave of “Home &amp; Kitchen” reviews, it might become an expert in that domain but forget the specific nuances and vocabulary required to effectively summarize “Fashion” or “Electronics” reviews. This would silently degrade the user experience for a large portion of our product catalog.</p>
<p><strong>Our Mitigation Strategy: A Modular, Multi-Adapter Architecture</strong></p>
<p>Instead of fine-tuning a single, monolithic model, we will adopt a more robust, <strong>federated adapter strategy</strong>.</p>
<ol class="arabic simple">
<li><p><strong>Domain-Specific Adapters:</strong> We will train separate, lightweight LoRA adapters for distinct, high-level product categories (e.g., <code class="docutils literal notranslate"><span class="pre">electronics_adapter</span></code>, <code class="docutils literal notranslate"><span class="pre">fashion_adapter</span></code>, <code class="docutils literal notranslate"><span class="pre">home_adapter</span></code>).</p></li>
<li><p><strong>Isolated Training:</strong> When the “Electronics” domain requires an update, we <em>only</em> retrain the <code class="docutils literal notranslate"><span class="pre">electronics_adapter</span></code> on new electronics reviews. The weights of the base model and all other adapters remain untouched.</p></li>
<li><p><strong>Dynamic Serving:</strong> Our inference service on EKS will be architected to support <strong>Multi-LoRA serving (e.g., using LoRAX)</strong>. When a request comes for a product, the service identifies its category and dynamically loads the appropriate LoRA adapter on top of the shared base model to generate the summary.</p></li>
</ol>
<p>This architecture is the ultimate defense against catastrophic forgetting. It contains the impact of retraining to a specific domain, ensuring that improvements in one area do not cause regressions in another.</p>
</section>
<section id="phased-production-testing-from-safety-to-business-impact">
<h4>11.3 Phased Production Testing: From Safety to Business Impact<a class="headerlink" href="#phased-production-testing-from-safety-to-business-impact" title="Permalink to this heading">¶</a></h4>
<p>Before a newly trained model (or adapter) can become the new production “champion,” it must pass a rigorous, phased testing process in the live environment.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Test Method</p></th>
<th class="head text-left"><p>Purpose &amp; LLM-Specific Focus</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Shadow Deployment</strong></p></td>
<td class="text-left"><p>The new “challenger” model/adapter runs in parallel with the current “champion.”</p></td>
<td class="text-left"><p><strong>Purpose:</strong> Safely validate the challenger’s performance on live, messy production data with <strong>zero user impact</strong>. <br> <strong>Process:</strong> Our batch inference pipeline calls both models. The challenger’s summaries are logged but not served. We then compare them offline to the champion’s outputs using our Ragas/LLM-as-a-judge pipeline. This is our final quality gate before exposing the model to users.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. A/B Testing</strong></p></td>
<td class="text-left"><p>A subset of users are shown summaries generated by the challenger model, while the control group sees summaries from the champion model.</p></td>
<td class="text-left"><p><strong>Purpose:</strong> To definitively and quantitatively measure the <strong>business impact</strong> of the new model. This moves beyond offline metrics to answer the ultimate question: “Does this new model help our users and our business?” <br> <strong>Primary KPIs Measured:</strong> Conversion Rate, Product Return Rate, User Engagement (“Helpful” clicks).</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="a-b-testing-framework-for-a-batch-system">
<h4>11.4 A/B Testing Framework for a Batch System<a class="headerlink" href="#a-b-testing-framework-for-a-batch-system" title="Permalink to this heading">¶</a></h4>
<p>Executing an A/B test in our batch-oriented system requires a specific workflow:
<img src="../_static/past_experiences/ecom_summarisation/ab_testing.png" width="100%" style="background-color: #FCF1EF;"/></p>
<p>This framework allows us to rigorously test the real-world impact of our model updates. A new model is only fully rolled out and becomes the new “champion” if it demonstrates a statistically significant improvement in our primary business KPIs, completing the virtuous cycle of our MLOps process.</p>
</section>
</section>
<hr class="docutils" />
<section id="governance-ethics-the-human-element">
<h3>12. Governance, Ethics &amp; The Human Element<a class="headerlink" href="#governance-ethics-the-human-element" title="Permalink to this heading">¶</a></h3>
<p>A production AI system’s success is ultimately measured not just by its accuracy but by its reliability, fairness, and the trust it earns from users and stakeholders. This section outlines the governance framework and ethical guardrails that ensure our summarization feature is developed and operated responsibly.</p>
<section id="comprehensive-model-governance">
<h4>12.1 Comprehensive Model Governance<a class="headerlink" href="#comprehensive-model-governance" title="Permalink to this heading">¶</a></h4>
<p>Governance is integrated throughout our MLOps lifecycle to ensure compliance, reproducibility, and control.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Lifecycle Stage</p></th>
<th class="head text-left"><p>Governance Component</p></th>
<th class="head text-left"><p>Key Tasks &amp; Artifacts for this Project</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Development</strong></p></td>
<td class="text-left"><p><strong>Reproducibility</strong></p></td>
<td class="text-left"><p><strong>Versioning Everything:</strong> All components are versioned to enable full reproducibility. <br> • <strong>Code:</strong> Git commits.<br> • <strong>Data:</strong> DVC for evaluation datasets.<br> • <strong>Prompts:</strong> Versioned as config files in Git.<br> • <strong>Models:</strong> Registered and versioned in the MLflow Model Registry.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Validation</strong></p></td>
<td class="text-left"><p><strong>Model Cards:</strong> For each production-promoted model, a Model Card is created, documenting its intended use, evaluation metrics (including on key data slices), limitations, and any known biases.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Deployment &amp; Operations</strong></p></td>
<td class="text-left"><p><strong>Control &amp; Security</strong></p></td>
<td class="text-left"><p><strong>Access Control:</strong> IAM roles enforce least-privilege access. EKS service accounts and SageMaker execution roles ensure services can only access the resources they need.<br> <strong>Secret Management:</strong> The production endpoint API key for LLM-as-a-judge is stored securely in AWS Secrets Manager.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Monitoring &amp; Alerting</strong></p></td>
<td class="text-left"><p><strong>Automated Alerts:</strong> The monitoring system (as defined in Section 10) automatically sends alerts for performance degradation, data drift, or quality issues, creating an immediate feedback loop.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Auditability</strong></p></td>
<td class="text-left"><p><strong>MLflow as Audit Trail:</strong> The MLflow Tracking Server provides a complete, traceable log linking a specific model version back to the data it was trained on, the code that trained it, and its evaluation results.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="responsible-ai-rai-principles-in-practice">
<h4>12.2 Responsible AI (RAI) Principles in Practice<a class="headerlink" href="#responsible-ai-rai-principles-in-practice" title="Permalink to this heading">¶</a></h4>
<p>We proactively address ethical considerations, focusing on fairness, explainability, and privacy.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>RAI Principle</p></th>
<th class="head text-left"><p>Risk in this Project</p></th>
<th class="head text-left"><p>Mitigation Strategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Fairness &amp; Bias</strong></p></td>
<td class="text-left"><p><strong>Positivity Bias:</strong> The model, trained on a majority of positive reviews, might learn to over-emphasize positive statements and downplay or ignore valid negative feedback, creating misleadingly positive summaries.</p></td>
<td class="text-left"><p><strong>Data-Centric Mitigation:</strong> Our RAG retrieval strategy is explicitly designed to be <strong>bias-aware</strong>. It programmatically retrieves a <em>balanced</em> set of review snippets (e.g., top 3 helpful positive, top 3 helpful negative) to ensure the LLM is always presented with a fair and representative context.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Explainability (XAI)</strong></p></td>
<td class="text-left"><p><strong>“Black Box” Summaries:</strong> Users may not understand <em>why</em> the summary makes a particular claim (e.g., “battery life is poor”), which can reduce trust.</p></td>
<td class="text-left"><p><strong>RAG-based Explainability (“Grounding”):</strong> This is a key feature of our system. The user interface can be designed to allow users to click on a sentence in the summary, which would then highlight the original review snippets from which that statement was derived. This directly connects the generated output to its source evidence.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Transparency</strong></p></td>
<td class="text-left"><p><strong>Undisclosed AI Generation:</strong> Users may not be aware that the summary is AI-generated, leading to misplaced expectations.</p></td>
<td class="text-left"><p><strong>Clear Labeling:</strong> The user interface will clearly label the summary section with a disclaimer such as “AI-generated summary based on customer reviews” to manage user expectations appropriately. The Model Card serves as internal transparency.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Privacy</strong></p></td>
<td class="text-left"><p><strong>PII Leakage:</strong> Customer reviews might contain Personally Identifiable Information (e.g., “the seller John Doe at john.doe&#64;email.com was very helpful”) which could be repeated in the summary.</p></td>
<td class="text-left"><p><strong>Proactive PII Redaction:</strong> The <strong>Data Ingestion &amp; Cleaning Pipeline</strong> includes a dedicated, automated step using libraries like <code class="docutils literal notranslate"><span class="pre">presidio</span></code> to detect and redact PII from all review texts <em>before</em> they are stored, embedded, or ever seen by the LLM.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="holistic-testing-production-readiness-ml-test-score">
<h4>12.3 Holistic Testing &amp; Production Readiness (ML Test Score)<a class="headerlink" href="#holistic-testing-production-readiness-ml-test-score" title="Permalink to this heading">¶</a></h4>
<p>We use the principles from Google’s “ML Test Score” rubric to self-assess the production readiness of our system.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Test Category</p></th>
<th class="head text-left"><p>Key Checks for Our Project</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Features &amp; Data</strong></p></td>
<td class="text-left"><p>✓ Feature expectations are captured (Great Expectations). <br> ✓ Data pipeline has PII controls. <br> ✓ All input feature code (<code class="docutils literal notranslate"><span class="pre">embedding_generation_pipeline</span></code>) is unit tested.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Development</strong></p></td>
<td class="text-left"><p>✓ Model specs (LoRA config, prompt) are versioned in Git.<br> ✓ Offline metrics (Faithfulness, Coherence) correlate with desired business outcomes.<br> ✓ A simpler model (heuristic baseline) was proven to be worse.<br> ✓ Model quality is checked on key slices (multilingual data, polarized reviews).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>ML Infrastructure</strong></p></td>
<td class="text-left"><p>✓ Training pipeline is integration tested.<br> ✓ Model quality is validated before being registered.<br> ✓ Models are tested via a canary/shadow process before full production rollout.<br> ✓ A safe rollback mechanism to a previous model version is in place.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Monitoring</strong></p></td>
<td class="text-left"><p>✓ Data invariants (schema, distributions) are monitored.<br> ✓ Training-serving skew is addressed by design (same embedding model used everywhere).<br> ✓ Model quality (Faithfulness, etc.) is continuously monitored.<br> ✓ System performance (latency, throughput) is monitored.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="the-human-element-team-user-experience">
<h4>12.4 The Human Element: Team &amp; User Experience<a class="headerlink" href="#the-human-element-team-user-experience" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><strong>Team Collaboration:</strong> As a small, cross-functional team (Product, Data Engineering, MLOps), clear communication and shared ownership were paramount. Blameless post-mortems for any production issues are standard practice to encourage a culture of continuous improvement.</p></li>
<li><p><strong>User Feedback Loops:</strong> The user interface will include “Was this summary helpful?” (👍/👎) buttons. This direct, explicit feedback is a crucial source of data. A sudden increase in “👎” clicks for a specific product category is a powerful signal that can trigger an investigation and potentially a targeted retraining of the relevant LoRA adapter.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="overall-system-architecture">
<h3>13. Overall System Architecture<a class="headerlink" href="#overall-system-architecture" title="Permalink to this heading">¶</a></h3>
<p>The end-to-end system for customer review summarization is a sophisticated interplay of data engineering pipelines, MLOps automation, and a high-performance model serving infrastructure. The following diagram provides a unified view of how these components interact to deliver the final product feature.</p>
<section id="aws-system-architecture-diagram">
<h4>13.1 AWS System Architecture Diagram<a class="headerlink" href="#aws-system-architecture-diagram" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_summarisation/system_architecture.png" width="100%" style="background-color: #FCF1EF;"/>
<p>The system is logically divided into four distinct planes:</p>
<ol class="arabic simple">
<li><p><strong>CI/CD &amp; Governance Plane (Purple):</strong> This is the developer-facing control loop where all changes originate.</p>
<ul class="simple">
<li><p><strong>GitHub</strong> is the single source of truth for all artifacts: application code, pipeline definitions (DAGs), Infrastructure as Code (Terraform), and data pointers (DVC).</p></li>
<li><p><strong>GitHub Actions</strong> automates the entire CI/CD process. It runs tests, builds Docker containers, and deploys the updated pipeline and serving code to the appropriate AWS services.</p></li>
<li><p>The <strong>MLflow Model Registry</strong> acts as the central governance gate for models. A manual promotion of a model in the registry is the explicit, auditable action that triggers the workflow to deploy a new model version to production.</p></li>
</ul>
</li>
<li><p><strong>Control Plane - MLOps Automation (Orange):</strong> This is the automated “brain” of the MLOps system.</p>
<ul class="simple">
<li><p><strong>Amazon MWAA (Managed Workflows for Apache Airflow)</strong> orchestrates all our data and ML pipelines as DAGs. It is responsible for scheduling the batch inference jobs, running the data ingestion pipelines, and triggering the continuous training workflows.</p></li>
</ul>
</li>
<li><p><strong>Data Plane (Blue):</strong> This plane represents the flow and storage of all data.</p>
<ul class="simple">
<li><p>It begins with the <strong>Application DB</strong>, the source of new reviews.</p></li>
<li><p>The <strong>S3 Data Lake</strong> serves as the central, cost-effective storage for raw and cleaned review data.</p></li>
<li><p><strong>Amazon Bedrock</strong> provides the Titan embedding model used to convert text into semantic vectors.</p></li>
<li><p><strong>Amazon OpenSearch</strong> acts as our Vector Database, indexing the embeddings to enable efficient similarity search for our RAG strategy.</p></li>
</ul>
</li>
<li><p><strong>Serving &amp; Caching Plane (Light Orange):</strong> This is where the model is hosted and the final summaries are made available to the application.</p>
<ul class="simple">
<li><p>The <strong>LLM Serving Endpoint</strong>, running on <strong>Amazon EKS</strong> with an optimized engine like vLLM, is a high-performance microservice responsible for generating summaries. It is deployed as a container from <strong>Amazon ECR</strong>.</p></li>
<li><p>The <strong>Amazon DynamoDB</strong> table is the low-latency Summary Cache. Our batch inference pipeline writes its results here.</p></li>
<li><p>The main <strong>E-commerce Application</strong> reads from this DynamoDB cache to display summaries to users, completely decoupling it from the complexity and latency of the live model inference.</p></li>
</ul>
</li>
</ol>
</section>
<section id="sequence-diagram-batch-inference-workflow">
<h4>13.2 Sequence Diagram: Batch Inference Workflow<a class="headerlink" href="#sequence-diagram-batch-inference-workflow" title="Permalink to this heading">¶</a></h4>
<p>This sequence diagram illustrates a highly efficient and parallelized workflow.</p>
<img src="../_static/past_experiences/ecom_summarisation/sequence_diagram.png" width="100%" style="background-color: #FCF1EF;"/>
<ul class="simple">
<li><p><strong>Total Estimated Pipeline Runtime:</strong> For a typical hourly run involving <strong>500 products</strong>, the entire end-to-end process is expected to complete in well under a minute (<strong>~30-50 seconds</strong>).</p></li>
<li><p><strong>Dominant Latency Step:</strong> The most time-consuming part of the process is the actual LLM inference step (<code class="docutils literal notranslate"><span class="pre">generate_summaries_task</span></code>). This highlights the critical importance of using an optimized serving engine like vLLM to maximize throughput and keep the batch processing time low.</p></li>
<li><p><strong>Scalability:</strong> The architecture is designed to scale.</p>
<ul>
<li><p><strong>OpenSearch and DynamoDB</strong> can handle massive throughput with consistent low latency.</p></li>
<li><p>The <strong>LLM Serving Endpoint on EKS</strong> can be scaled horizontally by adding more pods/nodes if the number of products to be updated per hour grows significantly, although this would increase cost.</p></li>
<li><p>The primary bottleneck for a much larger workload would likely become the cost and time associated with the EKS inference step.</p></li>
</ul>
</li>
</ul>
</section>
<section id="potential-bottlenecks-and-performance-optimizations">
<h4>13.3 Potential Bottlenecks and Performance Optimizations<a class="headerlink" href="#potential-bottlenecks-and-performance-optimizations" title="Permalink to this heading">¶</a></h4>
<p>While the architecture is designed for efficiency, several potential bottlenecks could arise as the system scales. Proactively identifying and planning for these is key to maintaining a performant and cost-effective service.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Bottleneck</p></th>
<th class="head text-left"><p>Description</p></th>
<th class="head text-left"><p>Performance Optimization Strategies</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. LLM Inference Throughput</strong></p></td>
<td class="text-left"><p>This is the <strong>primary and most critical bottleneck</strong>. The number of summaries we can generate per second is limited by the GPU’s computational power. If the number of products needing updates per hour exceeds the endpoint’s capacity, the batch job’s runtime will extend, potentially violating our “hourly” freshness guarantee and increasing compute costs.</p></td>
<td class="text-left"><p><strong>Primary Optimizations (Already Planned):</strong><br> • <strong>Optimized Serving Engine:</strong> Using <strong>vLLM</strong> is non-negotiable. Its implementation of continuous batching and PagedAttention can increase throughput by <strong>5-10x</strong> compared to a naive Hugging Face implementation.<br> • <strong>Quantization:</strong> Serving a quantized version of the model (e.g., INT8 or AWQ) can significantly increase token generation speed and reduce the GPU memory footprint, allowing for larger batch sizes. This requires careful evaluation to ensure no unacceptable drop in summary quality.<br><br><strong>Secondary Optimizations (If Needed):</strong><br> • <strong>Horizontal Scaling:</strong> Add more pods to the EKS deployment. This provides a linear increase in throughput but also a linear increase in cost.<br> • <strong>Vertical Scaling:</strong> Upgrade to a more powerful GPU instance (e.g., from an NVIDIA A10G to an H100). This is more expensive but can provide a step-change in performance.<br> • <strong>Speculative Decoding:</strong> An advanced technique where a smaller, faster “draft” model generates candidate tokens that the main model validates in chunks, speeding up generation.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. RAG Context Retrieval Latency</strong></p></td>
<td class="text-left"><p>Before we can even call the LLM, we must query the Vector DB (OpenSearch) to retrieve the review snippets. If these queries are slow or inefficient, they add directly to the overall pipeline runtime, especially when processing thousands of products.</p></td>
<td class="text-left"><p><strong>Primary Optimizations (Already Planned):</strong><br> • <strong>Batching &amp; Asynchronous Queries:</strong> Instead of querying one product at a time, our <code class="docutils literal notranslate"><span class="pre">retrieve_rag_context.py</span></code> script will use <strong>asynchronous I/O (<code class="docutils literal notranslate"><span class="pre">asyncio</span></code>, <code class="docutils literal notranslate"><span class="pre">aiohttp</span></code>)</strong> to send many queries to OpenSearch concurrently, maximizing throughput.<br> • <strong>Optimized OpenSearch Index:</strong> Ensure the OpenSearch index is correctly sharded and has the appropriate instance type to handle the query load.<br><br><strong>Secondary Optimizations (If Needed):</strong><br> • <strong>Add a Read Replica:</strong> If the OpenSearch cluster is under heavy load from other applications, add a dedicated read replica for the summarization pipeline to query.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Airflow Worker Capacity</strong></p></td>
<td class="text-left"><p>The Airflow workers orchestrating the pipeline have finite resources. If we try to parallelize the processing of too many products simultaneously, we could overwhelm the worker’s CPU and memory, causing tasks to fail or the entire DAG to slow down.</p></td>
<td class="text-left"><p><strong>Primary Optimizations (Already Planned):</strong><br> • <strong>Resource Management:</strong> Configure the Airflow DAG with a sensible <code class="docutils literal notranslate"><span class="pre">max_active_runs</span></code> and set appropriate concurrency limits for each task.<br> • <strong>Offload Heavy Lifting:</strong> The current design correctly offloads the most intensive work (LLM inference) to a dedicated EKS cluster, keeping the Airflow workers lightweight.<br><br><strong>Secondary Optimizations (If Needed):</strong><br> • <strong>Scale MWAA Environment:</strong> Increase the size or number of workers in the Amazon MWAA environment.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Cold Starts (Scaling from Zero)</strong></p></td>
<td class="text-left"><p>Our cost-optimization strategy involves scaling the EKS deployment to zero pods when idle. The first batch job after a period of inactivity will experience a “cold start” latency as the Kubernetes scheduler needs to provision a new pod on a GPU node and download the model container. This could add several minutes to the first run.</p></td>
<td class="text-left"><p><strong>Primary Optimizations (Already Planned):</strong><br> • <strong>Overprovisioning with Paused Pods (if supported):</strong> Some advanced schedulers allow for “paused” pods that keep the container image resident on the node, dramatically reducing startup time. <br> • <strong>Acceptance:</strong> For our hourly batch job, a one-time startup latency of a few minutes is generally an acceptable trade-off for the significant cost savings of scaling to zero. <br><br><strong>Secondary Optimizations (If Needed):</strong><br> • <strong>Keep a Single Warm Pod:</strong> As a compromise, configure the deployment to always keep a minimum of one pod running. This eliminates cold starts but incurs a higher baseline cost.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Database Write Throughput</strong></p></td>
<td class="text-left"><p>After generating summaries, the pipeline needs to write them to the DynamoDB cache. While DynamoDB is highly scalable, a massive burst of writes could potentially exceed the table’s provisioned write capacity units (WCUs), leading to throttled requests and task retries.</p></td>
<td class="text-left"><p><strong>Primary Optimizations (Already Planned):</strong><br> • <strong>Use BatchWriteItem:</strong> The <code class="docutils literal notranslate"><span class="pre">validate_and_cache.py</span></code> script will use DynamoDB’s <code class="docutils literal notranslate"><span class="pre">BatchWriteItem</span></code> API, which is far more efficient than writing items one by one.<br> • <strong>On-Demand Capacity:</strong> Configure the DynamoDB table to use “On-Demand” capacity mode instead of provisioned. This automatically scales to handle the workload’s peak write throughput and is more cost-effective for spiky, infrequent traffic patterns like our batch job.</p></td>
</tr>
</tbody>
</table>
</div>
<!--
#### 13.4 Monthly Cost Estimation (Weekly Batch Inference)


##### The Problem with "Stop/Start" for MWAA and OpenSearch

*   **Amazon MWAA (Airflow):** This service is designed as a persistent orchestration *environment*, not an ephemeral job runner. Spinning up a new MWAA environment from scratch can take **30-60 minutes**. Tearing it down also takes time. Creating and destroying it for a weekly job is operationally impractical and would make the pipeline incredibly slow and brittle.
*   **Amazon OpenSearch:** While you *can* stop and start OpenSearch domains, the process is not instantaneous and adds significant operational complexity. It can take several minutes to start the cluster, and you would need to build extra automation to manage this process, adding another potential point of failure.

The *right* solution is to choose different, **serverless-native services** that are designed from the ground up for pay-per-use and scale-to-zero workloads.

##### The Revised, Cost-Optimized Architecture

Let's swap our expensive, persistent services for their serverless counterparts.

1.  **Orchestrator: Replace Amazon MWAA with AWS Step Functions.**
    *   **Why:** Step Functions is a fully serverless orchestrator. It has zero cost when idle and you pay per state transition. For a simple, linear pipeline that runs once a week, the cost is practically zero. It's the perfect tool for infrequent, event-driven workflows. We can use it to chain together Lambda functions and AWS Batch jobs.

2.  **Vector DB: Replace Amazon OpenSearch with Amazon Aurora Serverless v2 with pgvector.**
    *   **Why:** Aurora Serverless v2 is designed to scale down to a very low minimum capacity when not in use. Instead of paying for two large instances 24/7, we pay for a tiny baseline capacity (e.g., 0.5 ACUs) and it will automatically scale up only when our weekly batch job is querying it. This dramatically reduces the 24/7 database cost.

#####  Monthly Cost Estimation (Weekly Batch Inference)

This new architecture leads to a vastly different and more business-friendly cost profile.

| Pipeline Component | AWS Service(s) | Detailed Cost Calculation & Rationale | Estimated Cost (USD) |
| :--- | :--- | :--- | :--- |
| **Data Ingestion & Embedding**| **AWS Step Functions**<br>**AWS Batch**<br>**Amazon Aurora Serverless v2**<br>**Amazon Bedrock** | **Step Functions & Batch:** For orchestration and running the containerized ingestion scripts once a day. The costs for a few thousand state transitions and a few hours of CPU time are negligible.<br>- We'll budget generously = **~$15**.<br><br>**Aurora Serverless v2 (Vector DB):** This is the key saving. The database will idle at a minimal capacity for most of the month and only scale up during the weekly inference and daily ingestion jobs.<br>- Baseline (idle): ~710 hours * 0.5 ACUs * ~$0.12/ACU-hour = **~$43**<br>- Active (jobs): ~10 hours * 4 ACUs * ~$0.12/ACU-hour = **~$5**<br>- Storage (200 GB): 200 GB * ~$0.10/GB-month = **$20**. <br>Total Aurora cost: **~$68**.<br><br>**Bedrock (Embeddings):** Still negligible. = **~$1**. | **$80 - $120** |
| **Continuous Training** | **Amazon SageMaker Training** | **No change.** This remains a very low, infrequent cost.<br>- 1 run/month * 4 hours/run * ~$1.196/hour = **~$4.78**. | **$10 - $20** |
| **Batch Inference** | **Amazon EKS** | **No change.** Our strategy of using EKS with **scale-to-zero** remains the most cost-effective way to get the high performance of vLLM for the short duration of the batch job.<br>- 4 runs/month * 2 hours/run * ~$1.196/hour = **~$9.57**. | **$30 - $50** |
| **Summary Caching & Logging**| **Amazon DynamoDB**<br>**Amazon CloudWatch** | **No change.** These services are already serverless and cost-effective.<br>- DynamoDB: **~$10**.<br>- CloudWatch: **~$40**. | **$50 - $100** |
| **Total Estimated Monthly Cost**| **-** | **-** | **$170 - $290** |


By switching from persistent managed services (MWAA, OpenSearch) to a **serverless-first architecture (Step Functions, Aurora Serverless v2)**, we have reduced the estimated monthly cost by over **70%**, from ~$850 down to **~$230**. This new cost profile is far more aligned with the expected ROI for a feature like this at a mid-sized company. It makes the project not just technically impressive, but also financially sound and sustainable.
-->
<hr class="docutils" />
<!--

### Challenges and learnings

## 12.1 Challenge 1: The Silent Data Corruption Bug

### Chronology of Events:
1. **Discovery (Week 1-2)**: The project's automated model quality monitoring system began to register a slow, gradual decline in the "summary relevance" score, specifically for the "Home & Kitchen" product category. The decline was subtle, remaining within the "warning" threshold and thus not triggering a high-priority alert. The first concrete signal of a problem came from a stakeholder: a senior product manager filed a bug report stating that summaries for newly launched kitchen appliances felt "generic and unhelpful," lacking specific details mentioned in the reviews.
2. **Initial Investigation (Week 2)**: The on-call ML engineering team was assigned the ticket. Their initial hypothesis was model drift. They reran the standard evaluation pipeline on the deployed model artifact against the version-controlled "golden" evaluation dataset. The model passed with flying colors, showing no degradation. This deepened the mystery. Suspecting concept drift in the live data, they triggered an emergency retraining job using the most recent two weeks of data. The newly fine-tuned model, however, exhibited the same poor performance on recent traffic, and offline metrics showed no improvement. The team was now confident the model architecture and training code were not the problem.
3. **Cross-Team Interaction (Week 3)**: The ML team escalated the issue to the Data Engineering (DE) team, providing specific examples of poor summaries and the corresponding raw reviews. The DE team investigated their data pipelines, checking monitoring dashboards for data volume, processing latency, and schema validation errors. All metrics appeared normal. From their perspective, the data was flowing correctly and conformed to the established schema. They initially closed the ticket, reporting "no issues found in the data pipeline."
4. **Deep Dive and Discovery (Week 4)**: The ML team, convinced the problem lay in the data, pushed for a joint investigation. A "war room" was established with members from the ML, DE, and mobile client teams. The team began to manually inspect the raw, byte-level JSON data being ingested into the Kafka topic for the affected products. The breakthrough came when an engineer noticed that reviews submitted from the latest version of the Android application had an unusual character encoding pattern. They discovered that a recent update to a third-party UI library used in the app's review submission form had a bug: if a user included an emoji in their review text, the entire string was silently encoded in a non-standard UTF-8 variant. The JSON structure itself remained valid, so it passed all schema validation checks. However, the text content was effectively garbled in a way that was not immediately obvious in many UI tools but was catastrophic for the downstream NLP models. The embedding model, unable to parse the garbled text, was generating near-zero or meaningless vectors, which were then treated as noise by the summarization model.
5. **Root Cause and Resolution (Week 5)**: The root cause was identified as a lack of semantic data validation at the point of ingestion. The system was checking the structure of the data, but not its content. The resolution was multi-pronged:
    - **Immediate Fix**: The mobile team developed and deployed a hotfix for the Android app to correct the encoding issue.
    - **Pipeline Enhancement**: The DE team added a new "semantic validation" microservice to the stream processing pipeline. This service used a lightweight model to check for text intelligibility and language consistency, automatically quarantining any records that failed this check into a dead-letter queue for manual inspection.
    - **Data Correction**: A large-scale backfill job was designed and executed to re-process two months' worth of corrupted review data from the data lake, regenerate the correct embeddings, and re-index them in the vector database.

### Retrospective and Lessons Learned:
- **Initial Approach vs. Better Approach**: The initial approach relied solely on schema validation, assuming that structurally correct data was semantically correct. A better approach, now implemented, is a defense-in-depth strategy for data quality that includes both structural and semantic validation checks as early as possible in the pipeline.
- **Production Bugs and Regrets**: The biggest regret was the siloed initial investigation. The assumption that the problem must be "in the model" or "in the data pipeline" led to wasted time. The issue was at the intersection of the client, the data, and the model, and could only be solved by cross-functional collaboration.
- **Key Takeaways**: This incident underscored that for NLP systems, data quality is not just about schema, it's about semantics. It also highlighted the critical need for blameless, cross-functional post-mortem cultures. Without the joint task force, the teams might have continued to blame each other's components indefinitely.

---

## 12.2 Challenge 2: The "Helpful" Hallucination Crisis

### Chronology of Events:
1. **Launch (Day 1)**: A new, fine-tuned Llama-3-8B model, which showed a 15% improvement on offline metrics, was deployed to production using a canary strategy, starting with 5% of user traffic. Initial monitoring of system and business metrics was positive: latency remained within SLA, and early data suggested a slight uplift in user engagement.
2. **Incident Declaration (Day 3)**: A sev-1 (highest severity) incident was declared. The customer service department reported a sudden, sharp increase in complaints related to "misleading" and "factually incorrect" review summaries. The issue gained public visibility when a tech influencer posted on social media a screenshot of a summary for a new flagship laptop. The summary prominently featured the line, "Users praise the exceptional 10-hour battery life," while the top three most helpful user reviews on the same page clearly stated the battery life was a major weakness, lasting only 2-3 hours. The platform was accused of intentionally manipulating summaries to hide negative feedback.
3. **Investigation (Day 3-7)**: The team immediately rolled back the canary release to the previous model version, stopping the user impact. The subsequent investigation proved difficult and stressful due to the public scrutiny. The model passed all of its offline faithfulness tests, including LLM-as-a-judge evaluations. The team could not reproduce the issue on their standard test cases. After days of analysis, they discovered the failure mode was highly specific: it only occurred on products with a large volume of reviews (over 500) and highly polarized or conflicting opinions. The RAG system was correctly retrieving a mix of positive and negative review snippets. However, the new fine-tuned model had developed a subtle but dangerous bias. The fine-tuning dataset, while large, had been sourced primarily from products with generally positive or neutral reviews. This had inadvertently taught the model a "positivity bias": when presented with conflicting information, it learned to over-emphasize positive statements, especially those containing strong, superlative keywords like "exceptional," "perfect," or "amazing," even if those statements came from older, less helpful reviews.
4. **Solution Exploration (Week 2-3)**: A multi-pronged effort was launched to address the root cause.
    - **Prompt Engineering**: The team experimented with adding stronger negative constraints and balancing instructions to the prompt, such as, "You must give equal weight to both positive and negative opinions. Explicitly mention any significant points of disagreement among reviewers." This provided a partial improvement but did not fully eliminate the bias.
    - **RAG Strategy Modification**: The retrieval logic was fundamentally changed. Instead of retrieving a generic set of "relevant" snippets, the new strategy was explicitly designed to retrieve a balanced set: the top 3 most "helpful" positive review snippets and the top 3 most "helpful" negative review snippets.
    - **Data-Centric Re-tuning**: The most intensive effort involved curating a new fine-tuning dataset. The team specifically identified 1,000 products with highly polarized reviews and had human annotators create high-quality, balanced summaries for them. They also used the production model to synthesize additional negative examples to augment the dataset and explicitly teach the model how to handle conflicting information.
5. **Resolution (Week 4)**: The final solution was a combination of the improved, balanced RAG strategy and a new model fine-tuned on the specially curated "conflicting opinions" dataset. Before redeployment, this new system was subjected to a new, rigorous evaluation process that included an "adversarial test set" of 100 of the most contentious products on the platform, with every summary being manually reviewed by the product and legal teams.

### Retrospective and Lessons Learned:
- **Initial Approach vs. Better Approach**: The initial fine-tuning approach focused on data volume but neglected data diversity and distribution. The team assumed that a large, random sample of data would be representative. The better approach is a data-centric one that actively identifies and oversamples for known edge cases and difficult scenarios, such as products with polarized feedback.
- **Production Bugs and Regrets**: The team regretted not having a more robust "red teaming" process before the initial launch. A dedicated effort to manually try and "break" the model with difficult products could have revealed this bias before it impacted customers.
- **Key Takeaways**: This incident demonstrated that fine-tuning is not a neutral process; it can introduce subtle and harmful biases that are not captured by standard offline metrics. It also showed that in a RAG system, the retrieval strategy is as important as the retrieval technology. The logic of what context to provide to the LLM is a critical, tunable part of the system that directly impacts the final output's quality and fairness.

---

## 12.3 Challenge 3: The Latency Death Spiral at Peak Traffic

### Chronology of Events:
1. **Pre-Production Testing (Month 1)**: The system underwent extensive load testing using a standard industry tool that generated a uniform, high-volume load of requests. The system performed well, comfortably meeting the P99 latency Service Level Objective (SLO) of 800ms. The deployment architecture used a standard Kubernetes Horizontal Pod Autoscaler (HPA) configured to scale the number of GPU-powered inference pods based on average CPU utilization.
2. **Production Incident (Black Friday Peak)**: During the Black Friday sales event, as site traffic surged to its annual peak, the summary service entered a catastrophic failure mode. The monitoring dashboards showed P99 latency skyrocketing to over 5,000ms. This caused timeouts in the upstream microservices that called the summary service, leading to sections of the product detail page failing to load for users. The HPA responded to the increased load by rapidly scaling up the number of inference pods. However, counter-intuitively, this action made the latency even worse. The Kubernetes cluster eventually ran out of available GPU capacity, and newly scheduled pods became stuck in a "pending" state, unable to acquire resources.
3. **War Room Debugging (Hours 1-4)**: A high-priority war room was convened, involving the on-call SRE, ML Platform, and application development teams. The initial hypotheses included a potential DDoS attack, a memory leak in the inference service, or a "poison pill" request causing the model to enter an infinite loop. After hours of frantic debugging, the team identified the root cause: a toxic interaction between the LLM serving engine's batching mechanism and the naive CPU-based autoscaling configuration. The vLLM serving engine was configured to use continuous batching to maximize GPU throughput. As traffic increased, the average batch size grew, which is generally good for efficiency. However, for very large batches, the per-request latency also increases slightly. The HPA, which was configured to scale up when pod CPU utilization crossed a threshold, correctly identified that the pods were working harder and tried to add more replicas. But as new pods were added, the incoming request stream was split among more workers. This caused the average batch size per pod to decrease. Smaller batches are less efficient on GPUs, leading to lower overall throughput and higher per-request latency. This created a vicious, positive feedback loop: increased traffic led to HPA scaling up, which led to smaller batches, which led to lower efficiency and higher latency, which was interpreted by the HPA as even higher load, causing it to scale up further, leading to a complete system meltdown.
4. **Mitigation and Resolution (Hours 5-6)**:
    - **Immediate Mitigation**: The team manually overrode the HPA, setting the number of replicas to a high, fixed number that they knew could handle the peak load. This stabilized the service, as it allowed the serving engine to form large, efficient batches again.
    - **Long-Term Resolution**: The post-mortem identified several critical architectural flaws. The long-term fix was multi-layered:
        1. The HPA configuration was changed to scale based on a more appropriate custom metric: average GPU utilization, which is a direct measure of load for a GPU-bound workload.
        2. A request queuing and load-leveling service was implemented in front of the inference service. This smoothed out the "spiky" nature of user traffic, allowing the vLLM engine to pull requests at a steady rate and form optimally sized batches.
        3. A graceful degradation mechanism was introduced. The team fine-tuned a much smaller, faster model variant (Gemma-2B). A new feature flag was added to the inference service: if the end-to-end latency exceeds the SLO, the service can be switched to use the smaller fallback model, which provides slightly lower-quality summaries but guarantees a fast response, preventing cascading failures.

### Retrospective and Lessons Learned:
- **Initial Approach vs. Better Approach**: The initial approach treated LLM serving like a standard, stateless web service, applying generic CPU-based autoscaling rules. The better approach acknowledges that LLM inference is a unique type of workload. Its performance is non-linear and heavily dependent on batching strategy and GPU utilization. The autoscaling logic must be custom-tailored to these characteristics.
- **Production Bugs and Regrets**: The team regretted the inadequacy of their pre-production load testing. The tests simulated high volume but not realistic traffic patterns with sudden spikes and lulls. A more realistic load profile would have likely triggered this failure mode in a controlled environment.
- **Key Takeaways**: This incident was a stark lesson that LLM serving is a specialized infrastructure discipline. Standard DevOps and SRE practices for microservices do not always apply directly and can even be harmful. Productionizing LLMs requires deep, collaborative expertise that spans machine learning, software engineering, and infrastructure/SRE. The ML team understands the model's performance curves, while the SRE team understands the infrastructure's failure modes. These complex, systemic issues can only be prevented and resolved when these teams work together from the initial design phase.

-->
</section>
</section>
<hr class="docutils" />
<section id="implementation-data-ingestion-pipeline">
<h3>Implementation: Data Ingestion Pipeline<a class="headerlink" href="#implementation-data-ingestion-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="id11">
<h4>Architecture Diagram<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_ingestion.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="python-scripts">
<h4>Python Scripts<a class="headerlink" href="#python-scripts" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sqlalchemy</span> <span class="kn">import</span> <span class="n">create_engine</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_new_reviews</span><span class="p">(</span><span class="n">db_connection_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">execution_date</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts new reviews from the source database created in the last 24 hours.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        db_connection_string: The database connection string.</span>
<span class="sd">        execution_date: The date of the DAG run (for reproducibility).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A pandas DataFrame with new reviews.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Connecting to the source database...&quot;</span><span class="p">)</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">create_engine</span><span class="p">(</span><span class="n">db_connection_string</span><span class="p">)</span>
        
        <span class="c1"># Calculate the time window for the query</span>
        <span class="n">end_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">fromisoformat</span><span class="p">(</span><span class="n">execution_date</span><span class="p">)</span>
        <span class="n">start_date</span> <span class="o">=</span> <span class="n">end_date</span> <span class="o">-</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        SELECT review_id, product_id, user_id, star_rating, review_text, created_at</span>
<span class="s2">        FROM public.reviews</span>
<span class="s2">        WHERE created_at &gt;= &#39;</span><span class="si">{</span><span class="n">start_date</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1"> %H:%M:%S&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;</span>
<span class="s2">        AND created_at &lt; &#39;</span><span class="si">{</span><span class="n">end_date</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1"> %H:%M:%S&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;</span>
<span class="s2">        &quot;&quot;&quot;</span>
        
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Executing query for reviews between </span><span class="si">{</span><span class="n">start_date</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">end_date</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">engine</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span> <span class="k">as</span> <span class="n">connection</span><span class="p">:</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_sql</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">connection</span><span class="p">)</span>
        
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully extracted </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="s2"> new reviews.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">df</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to extract reviews: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">from</span> <span class="nn">langdetect</span> <span class="kn">import</span> <span class="n">detect</span>
<span class="c1"># Assume presidio and detoxify are installed</span>
<span class="c1"># from presidio_analyzer import AnalyzerEngine</span>
<span class="c1"># from detoxify import Detoxify</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># For demonstration, we&#39;ll mock the PII/Toxicity models to avoid heavy dependencies</span>
<span class="c1"># In a real scenario, these would be initialized properly.</span>
<span class="c1"># analyzer = AnalyzerEngine()</span>
<span class="c1"># toxicity_classifier = Detoxify(&#39;original&#39;)</span>

<span class="k">def</span> <span class="nf">_clean_html</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Removes HTML tags from text.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="s2">&quot;html.parser&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get_text</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">_normalize_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lowercases, removes special chars, and normalizes whitespace.&quot;&quot;&quot;</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\[.*?\]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;https?://\S+|www\.\S+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;&lt;.*?&gt;+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\n&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\w*\d\w*&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^a-z\s]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">_redact_pii</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mocks PII redaction.&quot;&quot;&quot;</span>
    <span class="c1"># In production, this would use Presidio:</span>
    <span class="c1"># results = analyzer.analyze(text=text, language=&#39;en&#39;)</span>
    <span class="c1"># for result in results:</span>
    <span class="c1">#     text = text.replace(text[result.start:result.end], f&#39;[{result.entity_type}]&#39;)</span>
    <span class="n">mock_redacted_text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\S+@\S+&#39;</span><span class="p">,</span> <span class="s1">&#39;[EMAIL]&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mock_redacted_text</span>
    
<span class="k">def</span> <span class="nf">_get_toxicity_score</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mocks toxicity scoring.&quot;&quot;&quot;</span>
    <span class="c1"># In production, this would use Detoxify:</span>
    <span class="c1"># score = toxicity_classifier.predict(text)[&#39;toxicity&#39;]</span>
    <span class="k">if</span> <span class="s2">&quot;hate&quot;</span> <span class="ow">in</span> <span class="n">text</span> <span class="ow">or</span> <span class="s2">&quot;stupid&quot;</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.9</span>
    <span class="k">return</span> <span class="mf">0.1</span>

<span class="k">def</span> <span class="nf">transform_reviews</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a series of transformations to the raw reviews DataFrame.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting transformation of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="s2"> reviews.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Clean and normalize text</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleaned_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;review_text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">_clean_html</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">_normalize_text</span><span class="p">)</span>
    
    <span class="c1"># Filter out &quot;noise&quot; reviews</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleaned_text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="p">()</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="s2"> reviews remaining after noise filtering.&quot;</span><span class="p">)</span>

    <span class="c1"># Safety and Privacy</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleaned_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleaned_text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">_redact_pii</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;toxicity_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleaned_text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">_get_toxicity_score</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;toxicity_score&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">0.8</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="s2"> reviews remaining after toxicity filtering.&quot;</span><span class="p">)</span>

    <span class="c1"># Language Detection</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;language&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleaned_text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">detect</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;unknown&#39;</span><span class="p">)</span>
    
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;review_id&#39;</span><span class="p">,</span> <span class="s1">&#39;product_id&#39;</span><span class="p">,</span> <span class="s1">&#39;user_id&#39;</span><span class="p">,</span> <span class="s1">&#39;star_rating&#39;</span><span class="p">,</span> <span class="s1">&#39;cleaned_text&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">,</span> <span class="s1">&#39;toxicity_score&#39;</span><span class="p">,</span> <span class="s1">&#39;created_at&#39;</span><span class="p">]]</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Transformation complete.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">final_df</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">great_expectations</span> <span class="k">as</span> <span class="nn">ge</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">validate_cleaned_data</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Validates the cleaned data using a Great Expectations suite.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Validating cleaned data...&quot;</span><span class="p">)</span>
    <span class="n">ge_df</span> <span class="o">=</span> <span class="n">ge</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    
    <span class="c1"># Define expectations</span>
    <span class="n">ge_df</span><span class="o">.</span><span class="n">expect_column_to_exist</span><span class="p">(</span><span class="s2">&quot;review_id&quot;</span><span class="p">)</span>
    <span class="n">ge_df</span><span class="o">.</span><span class="n">expect_column_values_to_not_be_null</span><span class="p">(</span><span class="s2">&quot;review_id&quot;</span><span class="p">)</span>
    <span class="n">ge_df</span><span class="o">.</span><span class="n">expect_column_values_to_be_unique</span><span class="p">(</span><span class="s2">&quot;review_id&quot;</span><span class="p">)</span>
    <span class="n">ge_df</span><span class="o">.</span><span class="n">expect_column_values_to_be_in_set</span><span class="p">(</span><span class="s2">&quot;star_rating&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
    <span class="n">ge_df</span><span class="o">.</span><span class="n">expect_column_values_to_not_be_null</span><span class="p">(</span><span class="s2">&quot;cleaned_text&quot;</span><span class="p">)</span>
    <span class="n">ge_df</span><span class="o">.</span><span class="n">expect_column_values_to_be_in_set</span><span class="p">(</span><span class="s2">&quot;language&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="s2">&quot;de&quot;</span><span class="p">,</span> <span class="s2">&quot;fr&quot;</span><span class="p">,</span> <span class="s2">&quot;es&quot;</span><span class="p">,</span> <span class="s2">&quot;it&quot;</span><span class="p">,</span> <span class="s2">&quot;nl&quot;</span><span class="p">])</span> <span class="c1"># Example languages</span>
    
    <span class="n">validation_result</span> <span class="o">=</span> <span class="n">ge_df</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">validation_result</span><span class="p">[</span><span class="s2">&quot;success&quot;</span><span class="p">]:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Data validation failed!&quot;</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">validation_result</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span>
        
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Data validation successful.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">subprocess</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">save_and_version_data</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">local_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">s3_bucket</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">execution_date</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Saves the DataFrame to a local Parquet file and uses DVC to version and push to S3.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Save to local filesystem (accessible by Airflow worker)</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">local_path</span><span class="si">}</span><span class="s2">/cleaned_reviews_</span><span class="si">{</span><span class="n">execution_date</span><span class="si">}</span><span class="s2">.parquet&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving cleaned data to </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">df</span><span class="o">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="c1"># DVC commands to version and push the data</span>
        <span class="c1"># Assumes DVC is initialized and remote is configured</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Versioning data with DVC...&quot;</span><span class="p">)</span>
        <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;dvc&quot;</span><span class="p">,</span> <span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="n">file_path</span><span class="p">],</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Pushing data to S3 remote with DVC...&quot;</span><span class="p">)</span>
        <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;dvc&quot;</span><span class="p">,</span> <span class="s2">&quot;push&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">.dvc&quot;</span><span class="p">],</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Data successfully saved and versioned.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to save and version data: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
</pre></div>
</div>
</section>
<section id="unit-tests">
<h4>Unit Tests<a class="headerlink" href="#unit-tests" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">src.pipelines.ingestion.transform</span> <span class="kn">import</span> <span class="n">transform_reviews</span>

<span class="k">def</span> <span class="nf">test_transform_reviews</span><span class="p">():</span>
    <span class="c1"># Arrange</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;review_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
        <span class="s1">&#39;product_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">],</span>
        <span class="s1">&#39;user_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">104</span><span class="p">,</span> <span class="mi">105</span><span class="p">],</span>
        <span class="s1">&#39;star_rating&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
        <span class="s1">&#39;review_text&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s1">&#39;&lt;p&gt;This is GREAT!&lt;/p&gt;&#39;</span><span class="p">,</span>
            <span class="s1">&#39;I hate this product. It is stupid.&#39;</span><span class="p">,</span> <span class="c1"># Should be filtered by toxicity</span>
            <span class="s1">&#39;Too short.&#39;</span><span class="p">,</span> <span class="c1"># Should be filtered by length</span>
            <span class="s1">&#39;My email is test@example.com&#39;</span><span class="p">,</span> <span class="c1"># Should be redacted</span>
            <span class="s1">&#39;Un produit fantastique en français.&#39;</span>
        <span class="p">],</span>
        <span class="s1">&#39;created_at&#39;</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">([</span><span class="s1">&#39;2024-01-01&#39;</span><span class="p">,</span> <span class="s1">&#39;2024-01-01&#39;</span><span class="p">,</span> <span class="s1">&#39;2024-01-01&#39;</span><span class="p">,</span> <span class="s1">&#39;2024-01-01&#39;</span><span class="p">,</span> <span class="s1">&#39;2024-01-01&#39;</span><span class="p">])</span>
    <span class="p">}</span>
    <span class="n">raw_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>

    <span class="c1"># Act</span>
    <span class="n">transformed_df</span> <span class="o">=</span> <span class="n">transform_reviews</span><span class="p">(</span><span class="n">raw_df</span><span class="p">)</span>

    <span class="c1"># Assert</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformed_df</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span> <span class="c1"># Should filter 2 rows</span>
    <span class="k">assert</span> <span class="n">transformed_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;cleaned_text&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;my email is [EMAIL]&#39;</span>
    <span class="k">assert</span> <span class="n">transformed_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;language&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;en&#39;</span>
    <span class="k">assert</span> <span class="n">transformed_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="s1">&#39;language&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;fr&#39;</span>
    <span class="k">assert</span> <span class="s1">&#39;toxicity_score&#39;</span> <span class="ow">in</span> <span class="n">transformed_df</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</section>
<section id="pipeline-airflow-dag">
<h4>Pipeline (Airflow DAG)<a class="headerlink" href="#pipeline-airflow-dag" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.models.dag</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="nn">airflow.providers.postgres.hooks.postgres</span> <span class="kn">import</span> <span class="n">PostgresHook</span>
<span class="c1"># Assuming custom Python modules are in a package installed in the Airflow environment</span>
<span class="kn">from</span> <span class="nn">src.pipelines.ingestion</span> <span class="kn">import</span> <span class="n">extract</span><span class="p">,</span> <span class="n">transform</span><span class="p">,</span> <span class="n">validate</span><span class="p">,</span> <span class="n">load</span>

<span class="n">S3_BUCKET</span> <span class="o">=</span> <span class="s2">&quot;my-ecommerce-mlops-bucket&quot;</span>
<span class="n">LOCAL_DATA_PATH</span> <span class="o">=</span> <span class="s2">&quot;/tmp/data&quot;</span> <span class="c1"># Path on the Airflow worker</span>

<span class="k">def</span> <span class="nf">extract_task</span><span class="p">(</span><span class="n">ti</span><span class="p">):</span>
    <span class="n">hook</span> <span class="o">=</span> <span class="n">PostgresHook</span><span class="p">(</span><span class="n">postgres_conn_id</span><span class="o">=</span><span class="s2">&quot;source_db_conn&quot;</span><span class="p">)</span>
    <span class="n">conn_string</span> <span class="o">=</span> <span class="n">hook</span><span class="o">.</span><span class="n">get_uri</span><span class="p">()</span>
    <span class="n">reviews_df</span> <span class="o">=</span> <span class="n">extract</span><span class="o">.</span><span class="n">get_new_reviews</span><span class="p">(</span><span class="n">conn_string</span><span class="p">,</span> <span class="n">ti</span><span class="o">.</span><span class="n">execution_date</span><span class="o">.</span><span class="n">to_iso8601_string</span><span class="p">())</span>
    <span class="c1"># Push to XComs for the next task</span>
    <span class="n">ti</span><span class="o">.</span><span class="n">xcom_push</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;raw_reviews_df&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">reviews_df</span><span class="o">.</span><span class="n">to_json</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">transform_task</span><span class="p">(</span><span class="n">ti</span><span class="p">):</span>
    <span class="n">raw_reviews_json</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">task_ids</span><span class="o">=</span><span class="s2">&quot;extract_new_reviews&quot;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s2">&quot;raw_reviews_df&quot;</span><span class="p">)</span>
    <span class="n">raw_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">raw_reviews_json</span><span class="p">)</span>
    <span class="n">transformed_df</span> <span class="o">=</span> <span class="n">transform</span><span class="o">.</span><span class="n">transform_reviews</span><span class="p">(</span><span class="n">raw_df</span><span class="p">)</span>
    <span class="n">ti</span><span class="o">.</span><span class="n">xcom_push</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;transformed_reviews_df&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">transformed_df</span><span class="o">.</span><span class="n">to_json</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">validate_task</span><span class="p">(</span><span class="n">ti</span><span class="p">):</span>
    <span class="n">transformed_reviews_json</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">task_ids</span><span class="o">=</span><span class="s2">&quot;transform_raw_reviews&quot;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s2">&quot;transformed_reviews_df&quot;</span><span class="p">)</span>
    <span class="n">transformed_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">transformed_reviews_json</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">validate</span><span class="o">.</span><span class="n">validate_cleaned_data</span><span class="p">(</span><span class="n">transformed_df</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Data validation failed, stopping pipeline.&quot;</span><span class="p">)</span>
    <span class="c1"># If validation succeeds, the original df is passed through</span>
    <span class="n">ti</span><span class="o">.</span><span class="n">xcom_push</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;validated_reviews_df&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">transformed_df</span><span class="o">.</span><span class="n">to_json</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">load_task</span><span class="p">(</span><span class="n">ti</span><span class="p">):</span>
    <span class="n">validated_reviews_json</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">task_ids</span><span class="o">=</span><span class="s2">&quot;validate_transformed_reviews&quot;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s2">&quot;validated_reviews_df&quot;</span><span class="p">)</span>
    <span class="n">validated_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">validated_reviews_json</span><span class="p">)</span>
    <span class="n">load</span><span class="o">.</span><span class="n">save_and_version_data</span><span class="p">(</span>
        <span class="n">df</span><span class="o">=</span><span class="n">validated_df</span><span class="p">,</span> 
        <span class="n">local_path</span><span class="o">=</span><span class="n">LOCAL_DATA_PATH</span><span class="p">,</span> 
        <span class="n">s3_bucket</span><span class="o">=</span><span class="n">S3_BUCKET</span><span class="p">,</span>
        <span class="n">execution_date</span><span class="o">=</span><span class="n">ti</span><span class="o">.</span><span class="n">execution_date</span><span class="o">.</span><span class="n">to_iso8601_string</span><span class="p">()</span>
    <span class="p">)</span>


<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s2">&quot;data_ingestion_and_cleaning&quot;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2024</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="s2">&quot;UTC&quot;</span><span class="p">),</span>
    <span class="n">schedule</span><span class="o">=</span><span class="s2">&quot;0 1 * * *&quot;</span><span class="p">,</span>  <span class="c1"># Run daily at 1 AM UTC</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data-eng&quot;</span><span class="p">,</span> <span class="s2">&quot;ingestion&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">extract_new_reviews</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;extract_new_reviews&quot;</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">extract_task</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">transform_raw_reviews</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;transform_raw_reviews&quot;</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">transform_task</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">validate_transformed_reviews</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;validate_transformed_reviews&quot;</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">validate_task</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">load_and_version_data</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;load_and_version_data&quot;</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">load_task</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">extract_new_reviews</span> <span class="o">&gt;&gt;</span> <span class="n">transform_raw_reviews</span> <span class="o">&gt;&gt;</span> <span class="n">validate_transformed_reviews</span> <span class="o">&gt;&gt;</span> <span class="n">load_and_version_data</span>
</pre></div>
</div>
</section>
<section id="id12">
<h4>Integration Test<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">airflow.models.dagbag</span> <span class="kn">import</span> <span class="n">DagBag</span>

<span class="c1"># This test checks the structural integrity of the DAG</span>
<span class="k">def</span> <span class="nf">test_dag_loaded</span><span class="p">():</span>
    <span class="n">dagbag</span> <span class="o">=</span> <span class="n">DagBag</span><span class="p">(</span><span class="n">dag_folder</span><span class="o">=</span><span class="s1">&#39;dags/&#39;</span><span class="p">,</span> <span class="n">include_examples</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">dagbag</span><span class="o">.</span><span class="n">get_dag</span><span class="p">(</span><span class="n">dag_id</span><span class="o">=</span><span class="s1">&#39;data_ingestion_and_cleaning&#39;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="s1">&#39;data_ingestion_and_cleaning&#39;</span> <span class="ow">in</span> <span class="n">dagbag</span><span class="o">.</span><span class="n">dags</span>

<span class="c1"># A more complex integration test would use the Airflow API</span>
<span class="c1"># to trigger a run in a staging environment and check the output in S3.</span>
<span class="c1"># This requires a running Airflow and is often done in a separate CI/CD stage.</span>
<span class="c1">#</span>
<span class="c1"># Example using pytest-airflow:</span>
<span class="c1"># from pytest_airflow import clirunner</span>
<span class="c1">#</span>
<span class="c1"># def test_dag_run_successfully(clirunner):</span>
<span class="c1">#     result = clirunner(&quot;dags&quot;, &quot;test&quot;, &quot;data_ingestion_and_cleaning&quot;, &quot;2024-01-01&quot;)</span>
<span class="c1">#     assert result.return_code == 0, &quot;DAG run failed&quot;</span>
<span class="c1">#</span>
<span class="c1">#     # Add assertions here to check for output artifacts in a mock S3 bucket</span>
</pre></div>
</div>
</section>
<section id="id13">
<h4>CI/CD Workflow (Github Actions)<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Data Ingestion Pipeline</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/pipelines/ingestion/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;dags/data_ingestion_dag.py&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;tests/pipelines/ingestion/**&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">lint-and-unit-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Set up Python</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pip install -r requirements.txt</span>
<span class="w">          </span><span class="no">pip install -r requirements-dev.txt</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run linter</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">flake8 src/pipelines/ingestion/ dags/data_ingestion_dag.py</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run unit tests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest tests/pipelines/ingestion/</span>

<span class="w">  </span><span class="nt">deploy-to-production</span><span class="p">:</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lint-and-unit-test</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">aws-access-key-id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.AWS_ACCESS_KEY_ID }}</span>
<span class="w">          </span><span class="nt">aws-secret-access-key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.AWS_SECRET_ACCESS_KEY }}</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Sync DAG to Production MWAA Bucket</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">aws s3 sync ./dags s3://${{ secrets.MWAA_PROD_DAGS_BUCKET }}/dags --delete</span>
<span class="w">          </span><span class="no"># In a real project, you would also sync your custom Python package</span>
<span class="w">          </span><span class="no"># aws s3 sync ./src s3://${{ secrets.MWAA_PROD_PLUGINS_BUCKET }}/src</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="implementation-embeddings-generation-pipeline">
<h3>Implementation: Embeddings Generation Pipeline<a class="headerlink" href="#implementation-embeddings-generation-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="id14">
<h4>Architecture Diagram<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_embeddings.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="id15">
<h4>Python Scripts<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">subprocess</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_latest_cleaned_data</span><span class="p">(</span><span class="n">local_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">execution_date</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses DVC to pull the latest version of the cleaned data corresponding to the execution date.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">file_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">local_path</span><span class="si">}</span><span class="s2">/cleaned_reviews_</span><span class="si">{</span><span class="n">execution_date</span><span class="si">}</span><span class="s2">.parquet&quot;</span>
    <span class="n">dvc_file_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">.dvc&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using DVC to pull data for </span><span class="si">{</span><span class="n">dvc_file_path</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="c1"># Ensure the .dvc file itself is present before pulling</span>
        <span class="c1"># In a real Airflow setup, the repo would be synced.</span>
        <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;dvc&quot;</span><span class="p">,</span> <span class="s2">&quot;pull&quot;</span><span class="p">,</span> <span class="n">dvc_file_path</span><span class="p">],</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">capture_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading data from </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2"> into pandas DataFrame.&quot;</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="s2"> records.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">df</span>
    <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DVC file not found: </span><span class="si">{</span><span class="n">dvc_file_path</span><span class="si">}</span><span class="s2">. Did the ingestion pipeline run successfully?&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to retrieve data with DVC: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DVC output: </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nb">hasattr</span><span class="p">(</span><span class="n">e</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;stdout&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_embeddings</span><span class="p">(</span><span class="n">reviews_df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">bedrock_client</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Chunks review text and generates embeddings using Amazon Bedrock.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">length_function</span><span class="o">=</span><span class="nb">len</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="n">all_embeddings_data</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting embedding generation for </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">reviews_df</span><span class="p">)</span><span class="si">}</span><span class="s2"> reviews.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reviews_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;cleaned_text&#39;</span><span class="p">])</span>
        
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
            <span class="n">body</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;inputText&quot;</span><span class="p">:</span> <span class="n">chunk</span><span class="p">})</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">bedrock_client</span><span class="o">.</span><span class="n">invoke_model</span><span class="p">(</span>
                <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span>
                <span class="n">modelId</span><span class="o">=</span><span class="s2">&quot;amazon.titan-embed-text-v2:0&quot;</span><span class="p">,</span>
                <span class="n">accept</span><span class="o">=</span><span class="s2">&quot;application/json&quot;</span><span class="p">,</span>
                <span class="n">contentType</span><span class="o">=</span><span class="s2">&quot;application/json&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">response_body</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;body&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="n">response_body</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;embedding&quot;</span><span class="p">)</span>
            
            <span class="n">all_embeddings_data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s2">&quot;review_id&quot;</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;review_id&#39;</span><span class="p">],</span>
                <span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;product_id&#39;</span><span class="p">],</span>
                <span class="s2">&quot;star_rating&quot;</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;star_rating&#39;</span><span class="p">],</span>
                <span class="s2">&quot;language&quot;</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;language&#39;</span><span class="p">],</span>
                <span class="s2">&quot;chunk_text&quot;</span><span class="p">:</span> <span class="n">chunk</span><span class="p">,</span>
                <span class="s2">&quot;embedding&quot;</span><span class="p">:</span> <span class="n">embedding</span><span class="p">,</span>
            <span class="p">})</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully generated </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_embeddings_data</span><span class="p">)</span><span class="si">}</span><span class="s2"> embeddings.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_embeddings_data</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">psycopg2</span>
<span class="kn">import</span> <span class="nn">psycopg2.extras</span>
<span class="kn">from</span> <span class="nn">pgvector.psycopg2</span> <span class="kn">import</span> <span class="n">register_vector</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">index_embeddings_in_db</span><span class="p">(</span><span class="n">embedding_data</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">db_params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Indexes the generated embeddings and metadata into the Aurora PostgreSQL DB with pgvector.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Connecting to the vector database...&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">psycopg2</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="o">**</span><span class="n">db_params</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
                <span class="n">register_vector</span><span class="p">(</span><span class="n">cur</span><span class="p">)</span>
                
                <span class="n">insert_query</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                INSERT INTO review_embeddings (review_id, product_id, star_rating, language, chunk_text, embedding)</span>
<span class="s2">                VALUES (</span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">)</span>
<span class="s2">                ON CONFLICT (review_id, chunk_text) DO NOTHING; </span>
<span class="s2">                &quot;&quot;&quot;</span> <span class="c1"># Using a simple ON CONFLICT to ensure idempotency</span>

                <span class="c1"># Prepare data for batch insert</span>
                <span class="n">data_to_insert</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">(</span>
                        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;review_id&quot;</span><span class="p">],</span>
                        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;product_id&quot;</span><span class="p">],</span>
                        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;star_rating&quot;</span><span class="p">],</span>
                        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;language&quot;</span><span class="p">],</span>
                        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;chunk_text&quot;</span><span class="p">],</span>
                        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;embedding&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">embedding_data</span>
                <span class="p">]</span>
                
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Indexing </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data_to_insert</span><span class="p">)</span><span class="si">}</span><span class="s2"> embeddings in batches...&quot;</span><span class="p">)</span>
                <span class="n">psycopg2</span><span class="o">.</span><span class="n">extras</span><span class="o">.</span><span class="n">execute_batch</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="n">insert_query</span><span class="p">,</span> <span class="n">data_to_insert</span><span class="p">)</span>
                <span class="n">conn</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Indexing complete.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to index embeddings: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
</pre></div>
</div>
</section>
<section id="unit-test">
<h4>Unit Test<a class="headerlink" href="#unit-test" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">MagicMock</span>
<span class="kn">from</span> <span class="nn">src.pipelines.embedding.embed</span> <span class="kn">import</span> <span class="n">generate_embeddings</span>

<span class="k">def</span> <span class="nf">test_generate_embeddings_batching</span><span class="p">(</span><span class="n">mocker</span><span class="p">):</span>
    <span class="c1"># Arrange</span>
    <span class="n">mock_bedrock_client</span> <span class="o">=</span> <span class="n">MagicMock</span><span class="p">()</span>
    <span class="c1"># Mock the return value of invoke_model</span>
    <span class="n">mock_response_body</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;embedding&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">})</span>
    <span class="n">mock_stream</span> <span class="o">=</span> <span class="n">MagicMock</span><span class="p">()</span>
    <span class="n">mock_stream</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="n">mock_response_body</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    <span class="n">mock_bedrock_client</span><span class="o">.</span><span class="n">invoke_model</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;body&quot;</span><span class="p">:</span> <span class="n">mock_stream</span><span class="p">}</span>
    
    <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s1">&#39;boto3.client&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_bedrock_client</span><span class="p">)</span>

    <span class="n">test_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;review_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="s1">&#39;product_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">],</span>
        <span class="s1">&#39;star_rating&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span>
        <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;en&#39;</span><span class="p">],</span>
        <span class="s1">&#39;cleaned_text&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;This is the first sentence. This is the second sentence.&#39;</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

    <span class="c1"># Act</span>
    <span class="n">embedding_data</span> <span class="o">=</span> <span class="n">generate_embeddings</span><span class="p">(</span><span class="n">test_df</span><span class="p">,</span> <span class="n">mock_bedrock_client</span><span class="p">)</span>

    <span class="c1"># Assert</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">embedding_data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="c1"># The text should be split into two chunks</span>
    <span class="k">assert</span> <span class="n">mock_bedrock_client</span><span class="o">.</span><span class="n">invoke_model</span><span class="o">.</span><span class="n">call_count</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="n">embedding_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;review_id&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="s2">&quot;embedding&quot;</span> <span class="ow">in</span> <span class="n">embedding_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">embedding_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;embedding&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1024</span>
</pre></div>
</div>
</section>
<section id="id16">
<h4>Pipeline Code (Airflow DAG)<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.models.dag</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="nn">airflow.sensors.external_task</span> <span class="kn">import</span> <span class="n">ExternalTaskSensor</span>
<span class="kn">from</span> <span class="nn">airflow.providers.amazon.aws.hooks.bedrock</span> <span class="kn">import</span> <span class="n">BedrockHook</span>
<span class="kn">from</span> <span class="nn">airflow.providers.amazon.aws.hooks.secrets_manager</span> <span class="kn">import</span> <span class="n">SecretsManagerHook</span>
<span class="c1"># Assuming custom Python modules are installed</span>
<span class="kn">from</span> <span class="nn">src.pipelines.embedding</span> <span class="kn">import</span> <span class="n">retrieve</span><span class="p">,</span> <span class="n">embed</span><span class="p">,</span> <span class="n">load</span>

<span class="n">LOCAL_DATA_PATH</span> <span class="o">=</span> <span class="s2">&quot;/tmp/data&quot;</span>

<span class="k">def</span> <span class="nf">retrieve_data_task</span><span class="p">(</span><span class="n">ti</span><span class="p">):</span>
    <span class="c1"># This task gets the output from the ingestion DAG</span>
    <span class="c1"># For simplicity, we assume the execution date matches.</span>
    <span class="n">execution_date</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">execution_date</span><span class="o">.</span><span class="n">to_iso8601_string</span><span class="p">()</span>
    <span class="n">reviews_df</span> <span class="o">=</span> <span class="n">retrieve</span><span class="o">.</span><span class="n">get_latest_cleaned_data</span><span class="p">(</span><span class="n">LOCAL_DATA_PATH</span><span class="p">,</span> <span class="n">execution_date</span><span class="p">)</span>
    <span class="n">ti</span><span class="o">.</span><span class="n">xcom_push</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;reviews_df_json&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">reviews_df</span><span class="o">.</span><span class="n">to_json</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">embed_task</span><span class="p">(</span><span class="n">ti</span><span class="p">):</span>
    <span class="n">reviews_json</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">task_ids</span><span class="o">=</span><span class="s2">&quot;retrieve_cleaned_data&quot;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s2">&quot;reviews_df_json&quot;</span><span class="p">)</span>
    <span class="n">reviews_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">reviews_json</span><span class="p">)</span>
    
    <span class="n">bedrock_hook</span> <span class="o">=</span> <span class="n">BedrockHook</span><span class="p">(</span><span class="n">aws_conn_id</span><span class="o">=</span><span class="s1">&#39;aws_default&#39;</span><span class="p">)</span>
    <span class="n">bedrock_client</span> <span class="o">=</span> <span class="n">bedrock_hook</span><span class="o">.</span><span class="n">get_conn</span><span class="p">()</span>
    
    <span class="n">embedding_data</span> <span class="o">=</span> <span class="n">embed</span><span class="o">.</span><span class="n">generate_embeddings</span><span class="p">(</span><span class="n">reviews_df</span><span class="p">,</span> <span class="n">bedrock_client</span><span class="p">)</span>
    <span class="n">ti</span><span class="o">.</span><span class="n">xcom_push</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;embedding_data&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">embedding_data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_task</span><span class="p">(</span><span class="n">ti</span><span class="p">):</span>
    <span class="n">embedding_data</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">task_ids</span><span class="o">=</span><span class="s2">&quot;generate_review_embeddings&quot;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s2">&quot;embedding_data&quot;</span><span class="p">)</span>
    
    <span class="n">secrets_hook</span> <span class="o">=</span> <span class="n">SecretsManagerHook</span><span class="p">(</span><span class="n">aws_conn_id</span><span class="o">=</span><span class="s1">&#39;aws_default&#39;</span><span class="p">)</span>
    <span class="n">db_secret</span> <span class="o">=</span> <span class="n">secrets_hook</span><span class="o">.</span><span class="n">get_secret_value</span><span class="p">(</span><span class="s2">&quot;aurora/vector_db/credentials&quot;</span><span class="p">)</span>
    <span class="n">db_params</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">db_secret</span><span class="p">)</span>
    
    <span class="n">load</span><span class="o">.</span><span class="n">index_embeddings_in_db</span><span class="p">(</span><span class="n">embedding_data</span><span class="p">,</span> <span class="n">db_params</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s2">&quot;embedding_generation&quot;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2024</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="s2">&quot;UTC&quot;</span><span class="p">),</span>
    <span class="n">schedule</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Triggered by the ingestion DAG</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data-eng&quot;</span><span class="p">,</span> <span class="s2">&quot;embedding&quot;</span><span class="p">,</span> <span class="s2">&quot;rag&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">wait_for_ingestion</span> <span class="o">=</span> <span class="n">ExternalTaskSensor</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;wait_for_ingestion_dag&quot;</span><span class="p">,</span>
        <span class="n">external_dag_id</span><span class="o">=</span><span class="s2">&quot;data_ingestion_and_cleaning&quot;</span><span class="p">,</span>
        <span class="n">external_task_id</span><span class="o">=</span><span class="s2">&quot;load_and_version_data&quot;</span><span class="p">,</span>
        <span class="n">allowed_states</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;success&quot;</span><span class="p">],</span>
        <span class="n">execution_delta</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">duration</span><span class="p">(</span><span class="n">hours</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">)</span>
    
    <span class="n">retrieve_cleaned_data</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;retrieve_cleaned_data&quot;</span><span class="p">,</span> <span class="n">python_callable</span><span class="o">=</span><span class="n">retrieve_data_task</span><span class="p">)</span>
    <span class="n">generate_review_embeddings</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;generate_review_embeddings&quot;</span><span class="p">,</span> <span class="n">python_callable</span><span class="o">=</span><span class="n">embed_task</span><span class="p">)</span>
    <span class="n">index_embeddings</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;index_embeddings&quot;</span><span class="p">,</span> <span class="n">python_callable</span><span class="o">=</span><span class="n">load_task</span><span class="p">)</span>

    <span class="n">wait_for_ingestion</span> <span class="o">&gt;&gt;</span> <span class="n">retrieve_cleaned_data</span> <span class="o">&gt;&gt;</span> <span class="n">generate_review_embeddings</span> <span class="o">&gt;&gt;</span> <span class="n">index_embeddings</span>
</pre></div>
</div>
</section>
<section id="id17">
<h4>Integration Test<a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h4>
<p>This is a critical test that validates the entire workflow, ensuring that all components (Airflow, Python scripts, IAM permissions, and AWS services) work together correctly in a production-like environment.</p>
<p>The artifacts are structured into three parts:</p>
<ol class="arabic simple">
<li><p><strong>Setup Scripts:</strong> To prepare the staging environment for a clean, repeatable test run.</p></li>
<li><p><strong>Verification Script:</strong> The <code class="docutils literal notranslate"><span class="pre">pytest</span></code> script that runs after the pipeline execution to assert the correctness of the results.</p></li>
<li><p><strong>CI/CD Workflow:</strong> The GitHub Actions workflow that orchestrates the entire process: setup, execution, and verification.</p></li>
</ol>
<section id="setup-scripts-data">
<h5><strong>1. Setup Scripts &amp; Data</strong><a class="headerlink" href="#setup-scripts-data" title="Permalink to this heading">¶</a></h5>
<p>This script is responsible for creating the necessary preconditions for the test.</p>
<p><strong><code class="docutils literal notranslate"><span class="pre">tests/integration/setup_embedding_test.py</span></code></strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">psycopg2</span>

<span class="c1"># --- Test Configuration ---</span>
<span class="n">TEST_EXECUTION_DATE</span> <span class="o">=</span> <span class="s2">&quot;2025-01-01T00:00:00+00:00&quot;</span>
<span class="n">TEST_REVIEW_ID</span> <span class="o">=</span> <span class="s2">&quot;test_review_001&quot;</span>
<span class="n">STAGING_BUCKET</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_S3_BUCKET&quot;</span><span class="p">]</span>
<span class="n">LOCAL_DATA_PATH</span> <span class="o">=</span> <span class="s2">&quot;/tmp/staging_data&quot;</span>

<span class="c1"># Database connection params from environment variables</span>
<span class="n">DB_PARAMS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;host&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_HOST&quot;</span><span class="p">],</span>
    <span class="s2">&quot;port&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_PORT&quot;</span><span class="p">],</span>
    <span class="s2">&quot;dbname&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_NAME&quot;</span><span class="p">],</span>
    <span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_USER&quot;</span><span class="p">],</span>
    <span class="s2">&quot;password&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_PASSWORD&quot;</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_test_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a sample DataFrame for the test.&quot;&quot;&quot;</span>
    <span class="c1"># This text is designed to be split into two chunks by our splitter configuration</span>
    <span class="n">long_text</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;This is the first sentence of a moderately long review. &quot;</span>
        <span class="s2">&quot;It provides some initial positive feedback on the product&#39;s build quality. &quot;</span>
        <span class="s2">&quot;The reviewer seems generally happy with their purchase so far. &quot;</span>
        <span class="s2">&quot;Now we move on to the second part of the review which discusses the battery life. &quot;</span>
        <span class="s2">&quot;Unfortunately, the battery does not last as long as advertised, which is a significant drawback.&quot;</span>
    <span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;review_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">TEST_REVIEW_ID</span><span class="p">],</span>
        <span class="s1">&#39;product_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;product_abc&#39;</span><span class="p">],</span>
        <span class="s1">&#39;user_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">999</span><span class="p">],</span>
        <span class="s1">&#39;star_rating&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span>
        <span class="s1">&#39;cleaned_text&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">long_text</span><span class="p">],</span>
        <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;en&#39;</span><span class="p">],</span>
        <span class="s1">&#39;toxicity_score&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">],</span>
        <span class="s1">&#39;created_at&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">TEST_EXECUTION_DATE</span><span class="p">)]</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">upload_and_version_data</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Saves data locally, uploads to S3, and creates DVC file.&quot;&quot;&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">LOCAL_DATA_PATH</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Path names must match what the Airflow DAG expects</span>
    <span class="n">execution_date_str</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">TEST_EXECUTION_DATE</span><span class="p">)</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">T%H:%M:%S%z&#39;</span><span class="p">)</span>
    <span class="n">file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;cleaned_reviews_</span><span class="si">{</span><span class="n">execution_date_str</span><span class="si">}</span><span class="s2">.parquet&quot;</span>
    <span class="n">local_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LOCAL_DATA_PATH</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving test data to </span><span class="si">{</span><span class="n">local_file_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="n">local_file_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="c1"># Upload to S3 (simulating DVC remote)</span>
    <span class="n">s3_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;s3&quot;</span><span class="p">)</span>
    <span class="n">s3_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;data/</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="c1"># DVC would use a hash, but this is simpler for a test</span>
    <span class="n">s3_client</span><span class="o">.</span><span class="n">upload_file</span><span class="p">(</span><span class="n">local_file_path</span><span class="p">,</span> <span class="n">STAGING_BUCKET</span><span class="p">,</span> <span class="n">s3_key</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Uploaded test data to s3://</span><span class="si">{</span><span class="n">STAGING_BUCKET</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">s3_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># For a real DVC setup, we would run `dvc add` and `dvc push` here.</span>
    <span class="c1"># For this test, placing the file is sufficient.</span>

<span class="k">def</span> <span class="nf">clean_staging_db</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Ensures the staging DB is clean before the test run.&quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Cleaning staging database for a fresh test run.&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">psycopg2</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="o">**</span><span class="n">DB_PARAMS</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
            <span class="c1"># Truncate the table to remove any data from previous runs</span>
            <span class="n">cur</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DELETE FROM review_embeddings WHERE review_id = &#39;</span><span class="si">{</span><span class="n">TEST_REVIEW_ID</span><span class="si">}</span><span class="s2">&#39;;&quot;</span><span class="p">)</span>
            <span class="n">conn</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Staging database cleaned.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">clean_staging_db</span><span class="p">()</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">create_test_data</span><span class="p">()</span>
    <span class="n">upload_and_version_data</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Setup for embedding pipeline integration test is complete.&quot;</span><span class="p">)</span>

</pre></div>
</div>
</section>
<section id="verification-script-pytest">
<h5><strong>2. Verification Script (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong><a class="headerlink" href="#verification-script-pytest" title="Permalink to this heading">¶</a></h5>
<p>This script runs <em>after</em> the Airflow DAG has been triggered and has completed successfully.</p>
<p><strong><code class="docutils literal notranslate"><span class="pre">tests/integration/test_embedding_pipeline.py</span></code></strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">psycopg2</span>
<span class="kn">from</span> <span class="nn">pgvector.psycopg2</span> <span class="kn">import</span> <span class="n">register_vector</span>

<span class="c1"># --- Test Configuration ---</span>
<span class="n">TEST_REVIEW_ID</span> <span class="o">=</span> <span class="s2">&quot;test_review_001&quot;</span>
<span class="n">EXPECTED_CHUNKS</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">EXPECTED_EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="c1"># Database connection params from environment variables</span>
<span class="n">DB_PARAMS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;host&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_HOST&quot;</span><span class="p">],</span>
    <span class="s2">&quot;port&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_PORT&quot;</span><span class="p">],</span>
    <span class="s2">&quot;dbname&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_NAME&quot;</span><span class="p">],</span>
    <span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_USER&quot;</span><span class="p">],</span>
    <span class="s2">&quot;password&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_DB_PASSWORD&quot;</span><span class="p">],</span>
<span class="p">}</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;module&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">db_connection</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provides a reusable database connection for the test module.&quot;&quot;&quot;</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">psycopg2</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="o">**</span><span class="n">DB_PARAMS</span><span class="p">)</span>
    <span class="n">register_vector</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">conn</span>
    <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">test_embedding_generation_end_to_end</span><span class="p">(</span><span class="n">db_connection</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Verifies that the embedding generation pipeline correctly processed</span>
<span class="sd">    and indexed the test data into the staging database.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Arrange</span>
    <span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;SELECT chunk_text, embedding FROM review_embeddings WHERE review_id = &#39;</span><span class="si">{</span><span class="n">TEST_REVIEW_ID</span><span class="si">}</span><span class="s2">&#39;;&quot;</span>
    
    <span class="c1"># Act</span>
    <span class="k">with</span> <span class="n">db_connection</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">cur</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>

    <span class="c1"># Assert</span>
    <span class="k">assert</span> <span class="n">results</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;No results found for the test review ID.&quot;</span>
    
    <span class="c1"># 1. Verify the number of chunks</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">==</span> <span class="n">EXPECTED_CHUNKS</span><span class="p">,</span> \
        <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">EXPECTED_CHUNKS</span><span class="si">}</span><span class="s2"> chunks, but found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>

    <span class="c1"># 2. Verify the embedding vectors</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">chunk_text</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">),</span> \
            <span class="sa">f</span><span class="s2">&quot;Embedding for chunk </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> is not a list or array.&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span> <span class="o">==</span> <span class="n">EXPECTED_EMBEDDING_DIM</span><span class="p">,</span> \
            <span class="sa">f</span><span class="s2">&quot;Embedding for chunk </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> has dimension </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="si">}</span><span class="s2">, expected </span><span class="si">{</span><span class="n">EXPECTED_EMBEDDING_DIM</span><span class="si">}</span><span class="s2">.&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Integration test passed: Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">}</span><span class="s2"> chunks with correct embedding dimensions.&quot;</span><span class="p">)</span>

</pre></div>
</div>
</section>
</section>
<section id="id18">
<h4>CI/CD Workflow (GitHub Actions)<a class="headerlink" href="#id18" title="Permalink to this heading">¶</a></h4>
<p>This workflow automates the entire test: setup, DAG execution, and verification.</p>
<p><strong><code class="docutils literal notranslate"><span class="pre">.github/workflows/run_embedding_integration_test.yml</span></code></strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Embedding Pipeline Integration Test</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">workflow_dispatch</span><span class="p">:</span><span class="w"> </span><span class="c1"># Allows manual trigger</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/pipelines/embedding/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;dags/embedding_generation_dag.py&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">setup</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1. Setup Staging Environment</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Set up Python</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install -r requirements.txt</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">aws-access-key-id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}</span>
<span class="w">          </span><span class="nt">aws-secret-access-key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run setup script</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_S3_BUCKET</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_S3_BUCKET }}</span>
<span class="w">          </span><span class="nt">STAGING_DB_HOST</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_DB_HOST }}</span>
<span class="w">          </span><span class="c1"># ... other DB secrets</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python tests/integration/setup_embedding_test.py</span>

<span class="w">  </span><span class="nt">trigger-and-monitor-dag</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2. Trigger and Monitor Airflow DAG</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">setup</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Set up Python</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install requests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install requests</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Trigger and wait for DAG run</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">AIRFLOW_HOST</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_HOST }}</span>
<span class="w">          </span><span class="nt">AIRFLOW_USERNAME</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_USERNAME }}</span>
<span class="w">          </span><span class="nt">AIRFLOW_PASSWORD</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_PASSWORD }}</span>
<span class="w">        </span><span class="c1"># Assume a helper script to trigger and poll the Airflow API</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python scripts/trigger_airflow_dag.py --dag-id embedding_generation --execution-date &quot;2025-01-01T00:00:00+00:00&quot;</span>

<span class="w">  </span><span class="nt">verify</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3. Verify Results in Database</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">trigger-and-monitor-dag</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Set up Python</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install -r requirements-dev.txt</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">aws-access-key-id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}</span>
<span class="w">          </span><span class="nt">aws-secret-access-key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run verification script</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_DB_HOST</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_DB_HOST }}</span>
<span class="w">          </span><span class="c1"># ... other DB secrets</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest tests/integration/test_embedding_pipeline.py</span>
</pre></div>
</div>
<!--

**In the CI/CD file above, I do not see Terraform init and apply, shouldn't we be launching cloud resources using Terraform in staging environment in order to run Integration test ?**

You are absolutely correct. This is an excellent and crucial point that highlights the difference between a simple plan and a robust, production-grade DevOps strategy. My previous CI/CD workflow made a simplifying assumption that a staging environment already exists.

Your intuition is correct: for a truly automated and reliable integration test, the infrastructure the test depends on should be managed as part of the CI/CD process.

Let's refine this with a more sophisticated, real-world approach. There are two primary strategies for handling infrastructure in CI/CD, and the choice depends on a trade-off between speed and isolation.

### Strategy 1: Persistent Staging Environment (Faster CI/CD for App Code)

In this model, you have a long-lived staging environment that mirrors production. The infrastructure is created and updated *only when the Terraform code changes*. The application/pipeline CI/CD pipeline runs *against* this stable environment.

*   **Pros:** Much faster feedback loop for application code changes. The CI/CD run for a Python script change doesn't have to wait 15-30 minutes for an RDS instance to spin up.
*   **Cons:** Less isolation. A broken change from one PR could temporarily affect the staging environment for another.

### Strategy 2: Ephemeral (On-Demand) Environment (Perfect Isolation)

In this model, a completely new, isolated set of infrastructure is created from scratch for each integration test run (or for each pull request).

*   **Pros:** Perfect test isolation. There's no chance of tests interfering with each other. It's the gold standard for preventing flakes.
*   **Cons:** Extremely slow. Spinning up a new Aurora Serverless cluster and other resources for every single test run is impractical and would make the CI/CD pipeline take an hour or more.

### The Recommended Hybrid Approach (Best of Both Worlds)

For this project, the most practical and professional approach is a **hybrid one**. We will have a **Persistent Staging Environment**, but its configuration will be strictly managed by its own dedicated CI/CD workflow.

This means we will now have **two distinct CI/CD workflows**:

1.  **`terraform_ci_cd.yml`:** Manages the infrastructure. It runs *only when `.tf` files change*.
2.  **`run_embedding_integration_test.yml`:** Manages the application code. It *assumes the staging infra exists* and runs its tests against it.

This is the best balance: infrastructure is automated and version-controlled, but application tests get the fast feedback they need.

Here is the updated and complete set of artifacts, reflecting this superior strategy.

---

#### **1. Infrastructure CI/CD Workflow (Manages the Environment)**

This new workflow is responsible for creating and updating our persistent staging environment.

**`.github/workflows/terraform_ci_cd.yml`**
```yaml
name: Deploy Staging Infrastructure

on:
  push:
    branches:
      - main
    paths:
      - 'infra/**' # This workflow ONLY runs when Terraform code changes
  workflow_dispatch:

jobs:
  terraform:
    name: Terraform Plan and Apply
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-1

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_wrapper: false

      - name: Terraform Init
        run: terraform -chdir=infra init

      - name: Terraform Validate
        run: terraform -chdir=infra validate

      - name: Terraform Plan
        run: terraform -chdir=infra plan -out=tfplan

      - name: Terraform Apply
        # In a real-world scenario, you would add a manual approval step here
        # for production environments. For staging, auto-apply is often acceptable.
        run: terraform -chdir=infra apply -auto-approve tfplan
```
---
#### **2. Corrected Integration Test CI/CD Workflow (Uses the Environment)**

This is the revised workflow for our application code. It now explicitly states its assumption about the environment.

**`.github/workflows/run_embedding_integration_test.yml`**
```yaml
name: Embedding Pipeline Integration Test

# This workflow assumes a persistent staging environment has been provisioned
# by the terraform_ci_cd.yml workflow. It runs tests AGAINST this environment.

on:
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - 'src/pipelines/embedding/**'
      - 'dags/embedding_generation_dag.py'

jobs:
  setup-test-data:
    name: 1. Setup Staging Test Data
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-1
      - name: Run setup script to clean DB and upload test data
        env:
          STAGING_S3_BUCKET: ${{ secrets.STAGING_S3_BUCKET }}
          STAGING_DB_HOST: ${{ secrets.STAGING_DB_HOST }}
          STAGING_DB_PORT: ${{ secrets.STAGING_DB_PORT }}
          STAGING_DB_NAME: ${{ secrets.STAGING_DB_NAME }}
          STAGING_DB_USER: ${{ secrets.STAGING_DB_USER }}
          STAGING_DB_PASSWORD: ${{ secrets.STAGING_DB_PASSWORD }}
        run: python tests/integration/setup_embedding_test.py

  trigger-and-monitor-dag:
    name: 2. Trigger and Monitor Airflow DAG
    needs: setup-test-data
    runs-on: ubuntu-latest
    steps:
      # This part remains the same: it calls the Airflow API in the persistent staging environment
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install requests
        run: pip install requests
      - name: Trigger and wait for DAG run
        env:
          AIRFLOW_HOST: ${{ secrets.STAGING_AIRFLOW_HOST }}
          AIRFLOW_USERNAME: ${{ secrets.STAGING_AIRFLOW_USERNAME }}
          AIRFLOW_PASSWORD: ${{ secrets.STAGING_AIRFLOW_PASSWORD }}
        run: python scripts/trigger_airflow_dag.py --dag-id embedding_generation --execution-date "2025-01-01T00:00:00+00:00"

  verify-results:
    name: 3. Verify Results in Database
    needs: trigger-and-monitor-dag
    runs-on: ubuntu-latest
    steps:
      # This part also remains the same: it connects to the persistent staging DB to check the results
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install -r requirements-dev.txt
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-1
      - name: Run verification script
        env:
          STAGING_DB_HOST: ${{ secrets.STAGING_DB_HOST }}
          STAGING_DB_PORT: ${{ secrets.STAGING_DB_PORT }}
          STAGING_DB_NAME: ${{ secrets.STAGING_DB_NAME }}
          STAGING_DB_USER: ${{ secrets.STAGING_DB_USER }}
          STAGING_DB_PASSWORD: ${{ secrets.STAGING_DB_PASSWORD }}
        run: pytest tests/integration/test_embedding_pipeline.py
```

Thank you for the excellent question. This separation of concerns is a much more professional and scalable way to structure CI/CD for a real-world MLOps project.

-->
</section>
</section>
<hr class="docutils" />
<section id="implementation-llm-fine-tuning-pipeline">
<h3>Implementation: LLM Fine-tuning Pipeline<a class="headerlink" href="#implementation-llm-fine-tuning-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="id19">
<h4>Architecture Diagram<a class="headerlink" href="#id19" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_training.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="id20">
<h4>Python Scripts<a class="headerlink" href="#id20" title="Permalink to this heading">¶</a></h4>
<p><strong>src/pipelines/training/data_selection.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1"># Assume a helper module for S3 interactions</span>
<span class="c1"># from common.s3_utils import list_recent_files</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">select_finetuning_data</span><span class="p">(</span><span class="n">s3_bucket</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">s3_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Selects a sample of the most recent, high-quality reviews for fine-tuning.</span>
<span class="sd">    In a real scenario, this would also blend in a curated multilingual dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Selecting data from s3://</span><span class="si">{</span><span class="n">s3_bucket</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">s3_prefix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># This is a simplified version. A real implementation would be more robust.</span>
    <span class="c1"># recent_files = list_recent_files(s3_bucket, s3_prefix, days=30)</span>
    <span class="c1"># dfs = [pd.read_parquet(f&quot;s3://{s3_bucket}/{f}&quot;) for f in recent_files]</span>
    <span class="c1"># combined_df = pd.concat(dfs)</span>
    <span class="c1"># For now, we create a dummy dataframe.</span>
    
    <span class="c1"># Let&#39;s assume we load a dataset that needs formatting for the trainer.</span>
    <span class="c1"># The format should be a text column like:</span>
    <span class="c1"># &quot;###Instruction: Summarize these reviews. ###Input: [all review texts] ###Response: [human-written summary]&quot;</span>
    
    <span class="n">dummy_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="sa">f</span><span class="s2">&quot;###Instruction: Summarize these reviews. ###Input: review text </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">. ###Response: ideal summary </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">}</span>
    <span class="n">sample_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dummy_data</span><span class="p">)</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Selected a sample of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_df</span><span class="p">)</span><span class="si">}</span><span class="s2"> records for fine-tuning.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sample_df</span>
</pre></div>
</div>
<p><strong>src/pipelines/training/train.py</strong>
(This script is executed on SageMaker)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TrainingArguments</span>
<span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span>
<span class="kn">from</span> <span class="nn">trl</span> <span class="kn">import</span> <span class="n">SFTTrainer</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="c1"># SageMaker environments</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--model_dir&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SM_MODEL_DIR&quot;</span><span class="p">))</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--train_data_dir&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SM_CHANNEL_TRAINING&quot;</span><span class="p">))</span>
    <span class="c1"># Hyperparameters</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--base_model_id&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--epochs&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--per_device_train_batch_size&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="n">args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()</span>

    <span class="c1"># 1. Load data</span>
    <span class="n">train_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_data_dir</span><span class="p">,</span> <span class="s2">&quot;train.parquet&quot;</span><span class="p">)</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">train_file</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2"> training records.&quot;</span><span class="p">)</span>

    <span class="c1"># 2. Load model and tokenizer</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">base_model_id</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">base_model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>

    <span class="c1"># 3. Configure PEFT/LoRA</span>
    <span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
        <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
        <span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;CAUSAL_LM&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 4. Configure Training Arguments</span>
    <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">),</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">per_device_train_batch_size</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 5. Initialize Trainer</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
        <span class="n">peft_config</span><span class="o">=</span><span class="n">peft_config</span><span class="p">,</span>
        <span class="n">dataset_text_field</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
        <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 6. Start Training</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Starting model fine-tuning...&quot;</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Training complete.&quot;</span><span class="p">)</span>

    <span class="c1"># 7. Save the LoRA adapter</span>
    <span class="n">final_adapter_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;adapter&quot;</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">final_adapter_path</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LoRA adapter saved to </span><span class="si">{</span><span class="n">final_adapter_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><strong><code class="docutils literal notranslate"><span class="pre">src/pipelines/training/evaluate_and_register.py</span></code></strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="c1"># Assume other necessary imports for evaluation (Ragas, OpenAI)</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">MLFLOW_TRACKING_URI</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MLFLOW_TRACKING_URI&quot;</span><span class="p">]</span>
<span class="n">PROD_MODEL_NAME</span> <span class="o">=</span> <span class="s2">&quot;review-summarizer&quot;</span>
<span class="n">EVALUATION_THRESHOLD</span> <span class="o">=</span> <span class="mf">1.05</span> <span class="c1"># New model must be 5% better</span>

<span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">adapter_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">eval_df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mocks the evaluation process.&quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating model adapter from </span><span class="si">{</span><span class="n">adapter_path</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
    <span class="c1"># In a real scenario, this would:</span>
    <span class="c1"># 1. Load the base model + LoRA adapter.</span>
    <span class="c1"># 2. Generate summaries for the evaluation dataframe.</span>
    <span class="c1"># 3. Run Ragas and LLM-as-a-judge.</span>
    <span class="c1"># We&#39;ll return mock scores for this implementation.</span>
    <span class="n">mock_scores</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;faithfulness&quot;</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">&quot;coherence&quot;</span><span class="p">:</span> <span class="mf">4.6</span><span class="p">}</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation complete. Scores: </span><span class="si">{</span><span class="n">mock_scores</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mock_scores</span>

<span class="k">def</span> <span class="nf">register_model</span><span class="p">(</span><span class="n">model_artifact_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compares metrics and registers the model in MLflow if it&#39;s better.&quot;&quot;&quot;</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">MLFLOW_TRACKING_URI</span><span class="p">)</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">tracking</span><span class="o">.</span><span class="n">MlflowClient</span><span class="p">()</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the latest production model&#39;s metrics</span>
        <span class="n">latest_prod_version</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_latest_versions</span><span class="p">(</span><span class="n">PROD_MODEL_NAME</span><span class="p">,</span> <span class="n">stages</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Production&quot;</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prod_run</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_run</span><span class="p">(</span><span class="n">latest_prod_version</span><span class="o">.</span><span class="n">run_id</span><span class="p">)</span>
        <span class="n">prod_faithfulness</span> <span class="o">=</span> <span class="n">prod_run</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;faithfulness&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
        <span class="c1"># No production model exists yet</span>
        <span class="n">prod_faithfulness</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">candidate_faithfulness</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;faithfulness&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Candidate faithfulness: </span><span class="si">{</span><span class="n">candidate_faithfulness</span><span class="si">}</span><span class="s2">, Production faithfulness: </span><span class="si">{</span><span class="n">prod_faithfulness</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">candidate_faithfulness</span> <span class="o">&gt;</span> <span class="n">prod_faithfulness</span> <span class="o">*</span> <span class="n">EVALUATION_THRESHOLD</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Candidate model is better. Registering new version.&quot;</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span>
            <span class="n">model_uri</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;s3://</span><span class="si">{</span><span class="n">model_artifact_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="c1"># Assuming path is an S3 URI</span>
            <span class="n">name</span><span class="o">=</span><span class="n">PROD_MODEL_NAME</span><span class="p">,</span>
            <span class="c1"># Link to the run, log metrics, etc.</span>
        <span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Model registration successful.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Candidate model is not better than production. Skipping registration.&quot;</span><span class="p">)</span>

<span class="c1"># ... main execution block to run these functions</span>
</pre></div>
</div>
</section>
<section id="id21">
<h4>Unit Tests<a class="headerlink" href="#id21" title="Permalink to this heading">¶</a></h4>
<p><strong>tests/pipelines/training/test_registration.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">MagicMock</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">src.pipelines.training.evaluate_and_register</span> <span class="kn">import</span> <span class="n">register_model</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">mock_mlflow_client</span><span class="p">(</span><span class="n">mocker</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mocks the MLflow client and its methods.&quot;&quot;&quot;</span>
    <span class="n">mock_client</span> <span class="o">=</span> <span class="n">MagicMock</span><span class="p">()</span>
    <span class="c1"># Simulate an existing production model</span>
    <span class="n">mock_version</span> <span class="o">=</span> <span class="n">MagicMock</span><span class="p">()</span>
    <span class="n">mock_version</span><span class="o">.</span><span class="n">run_id</span> <span class="o">=</span> <span class="s2">&quot;prod_run_id&quot;</span>
    <span class="n">mock_client</span><span class="o">.</span><span class="n">get_latest_versions</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">[</span><span class="n">mock_version</span><span class="p">]</span>
    
    <span class="n">mock_run</span> <span class="o">=</span> <span class="n">MagicMock</span><span class="p">()</span>
    <span class="n">mock_run</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;faithfulness&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">}</span>
    <span class="n">mock_client</span><span class="o">.</span><span class="n">get_run</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="n">mock_run</span>
    
    <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s2">&quot;mlflow.tracking.MlflowClient&quot;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_client</span><span class="p">)</span>
    <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s2">&quot;mlflow.set_tracking_uri&quot;</span><span class="p">)</span>
    <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s2">&quot;mlflow.register_model&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mock_client</span>

<span class="k">def</span> <span class="nf">test_register_model_if_better</span><span class="p">(</span><span class="n">mock_mlflow_client</span><span class="p">):</span>
    <span class="c1"># Arrange</span>
    <span class="n">better_metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;faithfulness&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">}</span>
    
    <span class="c1"># Act</span>
    <span class="n">register_model</span><span class="p">(</span><span class="s2">&quot;s3://path/to/new/model&quot;</span><span class="p">,</span> <span class="n">better_metrics</span><span class="p">)</span>

    <span class="c1"># Assert</span>
    <span class="n">mock_mlflow_client</span><span class="o">.</span><span class="n">register_model</span><span class="o">.</span><span class="n">assert_called_once</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">test_do_not_register_if_worse</span><span class="p">(</span><span class="n">mock_mlflow_client</span><span class="p">):</span>
    <span class="c1"># Arrange</span>
    <span class="n">worse_metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;faithfulness&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">}</span>
    
    <span class="c1"># Act</span>
    <span class="n">register_model</span><span class="p">(</span><span class="s2">&quot;s3://path/to/new/model&quot;</span><span class="p">,</span> <span class="n">worse_metrics</span><span class="p">)</span>

    <span class="c1"># Assert</span>
    <span class="n">mock_mlflow_client</span><span class="o">.</span><span class="n">register_model</span><span class="o">.</span><span class="n">assert_not_called</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id22">
<h4>Pipeline (Airflow DAG)<a class="headerlink" href="#id22" title="Permalink to this heading">¶</a></h4>
<p><strong>dags/llm_finetuning_dag.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.models.dag</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="nn">airflow.providers.amazon.aws.operators.sagemaker</span> <span class="kn">import</span> <span class="n">SageMakerTrainingOperator</span>
<span class="c1"># ... other imports</span>

<span class="c1"># Simplified SageMaker Training Config</span>
<span class="n">sagemaker_training_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;AlgorithmSpecification&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;TrainingImage&quot;</span><span class="p">:</span> <span class="s2">&quot;123456789012.dkr.ecr.eu-west-1.amazonaws.com/llm-finetuning-image:latest&quot;</span><span class="p">,</span>
        <span class="s2">&quot;TrainingInputMode&quot;</span><span class="p">:</span> <span class="s2">&quot;File&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;RoleArn&quot;</span><span class="p">:</span> <span class="s2">&quot;arn:aws:iam::123456789012:role/SageMakerExecutionRole&quot;</span><span class="p">,</span>
    <span class="s2">&quot;InputDataConfig&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;ChannelName&quot;</span><span class="p">:</span> <span class="s2">&quot;training&quot;</span><span class="p">,</span>
            <span class="s2">&quot;DataSource&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;S3DataSource&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;S3DataType&quot;</span><span class="p">:</span> <span class="s2">&quot;S3Prefix&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;S3Uri&quot;</span><span class="p">:</span> <span class="s2">&quot;s3://my-ecommerce-mlops-bucket/data/training/{{ ds }}/&quot;</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">},</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="s2">&quot;OutputDataConfig&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;S3OutputPath&quot;</span><span class="p">:</span> <span class="s2">&quot;s3://my-ecommerce-mlops-bucket/models/training-output/&quot;</span><span class="p">},</span>
    <span class="s2">&quot;ResourceConfig&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;InstanceCount&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;InstanceType&quot;</span><span class="p">:</span> <span class="s2">&quot;ml.g5.2xlarge&quot;</span><span class="p">,</span> <span class="s2">&quot;VolumeSizeInGB&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span>
    <span class="s2">&quot;StoppingCondition&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;MaxRuntimeInSeconds&quot;</span><span class="p">:</span> <span class="mi">14400</span><span class="p">},</span>
    <span class="s2">&quot;HyperParameters&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;base_model_id&quot;</span><span class="p">:</span> <span class="s2">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span><span class="p">,</span> <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">},</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s2">&quot;llm_continuous_training&quot;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2024</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="s2">&quot;UTC&quot;</span><span class="p">),</span>
    <span class="n">schedule</span><span class="o">=</span><span class="s2">&quot;0 0 1 * *&quot;</span><span class="p">,</span>  <span class="c1"># Run on the 1st of every month</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ml-training&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="c1"># PythonOperator for data selection and validation</span>
    <span class="n">select_data_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> 
    
    <span class="n">trigger_sagemaker_training</span> <span class="o">=</span> <span class="n">SageMakerTrainingOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;trigger_sagemaker_training&quot;</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="n">sagemaker_training_config</span><span class="p">,</span>
        <span class="n">wait_for_completion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># PythonOperator to run evaluate_and_register.py</span>
    <span class="c1"># It will get the model artifact path from the SageMaker job&#39;s output (via XComs)</span>
    <span class="n">evaluate_and_register_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">select_data_task</span> <span class="o">&gt;&gt;</span> <span class="n">trigger_sagemaker_training</span> <span class="o">&gt;&gt;</span> <span class="n">evaluate_and_register_task</span>
</pre></div>
</div>
</section>
<section id="id23">
<h4>Infrastructure as Code (Terraform)<a class="headerlink" href="#id23" title="Permalink to this heading">¶</a></h4>
<div class="highlight-hcl notranslate"><div class="highlight"><pre><span></span><span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role&quot;</span><span class="w"> </span><span class="nv">&quot;sagemaker_execution_role&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;SageMakerExecutionRole&quot;</span>
<span class="c1">  # Assume role policy for SageMaker service</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_policy&quot;</span><span class="w"> </span><span class="nv">&quot;sagemaker_policy&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;SageMakerPolicy&quot;</span>
<span class="w">  </span><span class="na">policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">jsonencode</span><span class="p">({</span>
<span class="w">    </span><span class="na">Version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span>
<span class="w">    </span><span class="na">Statement</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="na">Effect</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="na">Action</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;s3:GetObject&quot;, &quot;s3:PutObject&quot;, &quot;s3:ListBucket&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="na">Resource</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;arn:aws:s3:::my-ecommerce-mlops-bucket/*&quot;</span><span class="p">]</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="na">Effect</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="na">Action</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;ecr:GetDownloadUrlForLayer&quot;, &quot;ecr:BatchGetImage&quot;, &quot;ecr:BatchCheckLayerAvailability&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="na">Resource</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_ecr_repository.training_repo.arn</span>
<span class="w">      </span><span class="p">}</span>
<span class="c1">      # Plus CloudWatch logs permissions, etc.</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">  </span><span class="p">})</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role_policy_attachment&quot;</span><span class="w"> </span><span class="nv">&quot;sagemaker_attach&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">role</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.sagemaker_execution_role.name</span>
<span class="w">  </span><span class="na">policy_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_policy.sagemaker_policy.arn</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_ecr_repository&quot;</span><span class="w"> </span><span class="nv">&quot;training_repo&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;llm-finetuning-image&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id24">
<h4>Integration Test<a class="headerlink" href="#id24" title="Permalink to this heading">¶</a></h4>
<p>The artifacts are structured into three parts:</p>
<ol class="arabic simple">
<li><p><strong>Setup Scripts &amp; Data:</strong> To prepare the staging environment with the necessary test data.</p></li>
<li><p><strong>Verification Script:</strong> The <code class="docutils literal notranslate"><span class="pre">pytest</span></code> script that runs after the pipeline execution to assert the outcome.</p></li>
<li><p><strong>CI/CD Workflow:</strong> The GitHub Actions workflow that orchestrates the entire process.</p></li>
</ol>
<p><strong>tests/integration/setup_training_test.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># --- Test Configuration ---</span>
<span class="n">STAGING_BUCKET</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_S3_BUCKET&quot;</span><span class="p">]</span>
<span class="n">EXECUTION_DATE</span> <span class="o">=</span> <span class="s2">&quot;2025-01-01&quot;</span> <span class="c1"># Matches the test execution date</span>
<span class="n">SAMPLE_SIZE</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># Small sample for a quick test run</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_finetuning_test_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a sample DataFrame for the fine-tuning test.&quot;&quot;&quot;</span>
    <span class="c1"># This format matches what the SFTTrainer expects in our train.py script</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="sa">f</span><span class="s2">&quot;###Instruction: Summarize these reviews. ###Input: Test review text </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">. ###Response: Ideal test summary </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">SAMPLE_SIZE</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">upload_data_to_s3</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Saves data locally and uploads it to the correct S3 path for the DAG.&quot;&quot;&quot;</span>
    <span class="n">s3_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;data/training/</span><span class="si">{</span><span class="n">EXECUTION_DATE</span><span class="si">}</span><span class="s2">/train.parquet&quot;</span>
    <span class="n">local_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/train.parquet&quot;</span>
    
    <span class="n">df</span><span class="o">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="n">local_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Uploading test training data to s3://</span><span class="si">{</span><span class="n">STAGING_BUCKET</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">s3_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">s3_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;s3&quot;</span><span class="p">)</span>
    <span class="n">s3_client</span><span class="o">.</span><span class="n">upload_file</span><span class="p">(</span><span class="n">local_path</span><span class="p">,</span> <span class="n">STAGING_BUCKET</span><span class="p">,</span> <span class="n">s3_key</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Upload complete.&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_mock_evaluation_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a mock evaluation dataset. Our evaluation script needs this</span>
<span class="sd">    to run, even though the results are mocked for the integration test.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eval_data</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;review_text&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;This is a test review for evaluation.&quot;</span><span class="p">]</span> <span class="p">}</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span>
    <span class="n">s3_key</span> <span class="o">=</span> <span class="s2">&quot;data/evaluation/golden_dataset.parquet&quot;</span>
    <span class="n">local_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/golden_dataset.parquet&quot;</span>
    <span class="n">df</span><span class="o">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="n">local_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Uploading mock evaluation data to s3://</span><span class="si">{</span><span class="n">STAGING_BUCKET</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">s3_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">s3_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;s3&quot;</span><span class="p">)</span>
    <span class="n">s3_client</span><span class="o">.</span><span class="n">upload_file</span><span class="p">(</span><span class="n">local_path</span><span class="p">,</span> <span class="n">STAGING_BUCKET</span><span class="p">,</span> <span class="n">s3_key</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Upload complete.&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">training_df</span> <span class="o">=</span> <span class="n">create_finetuning_test_data</span><span class="p">()</span>
    <span class="n">upload_data_to_s3</span><span class="p">(</span><span class="n">training_df</span><span class="p">)</span>
    <span class="n">create_mock_evaluation_data</span><span class="p">()</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Setup for training pipeline integration test is complete.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Verification Script (pytest)</strong>
<strong>tests/integration/test_training_pipeline.py</strong></p>
<p>This script runs after the llm_continuous_training DAG has completed successfully. It connects to the staging MLflow server to verify that a new model was registered.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking</span> <span class="kn">import</span> <span class="n">MlflowClient</span>

<span class="c1"># --- Test Configuration ---</span>
<span class="n">STAGING_MLFLOW_TRACKING_URI</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_MLFLOW_TRACKING_URI&quot;</span><span class="p">]</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s2">&quot;review-summarizer&quot;</span>
<span class="n">TEST_RUN_TAG</span> <span class="o">=</span> <span class="s2">&quot;integration_test&quot;</span>
<span class="n">EXECUTION_DATE</span> <span class="o">=</span> <span class="s2">&quot;2025-01-01&quot;</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;module&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">mlflow_client</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provides a reusable MLflow client for the test module.&quot;&quot;&quot;</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">STAGING_MLFLOW_TRACKING_URI</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MlflowClient</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">test_finetuning_pipeline_registers_new_model_version</span><span class="p">(</span><span class="n">mlflow_client</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Verifies that a new version of the summarizer model was registered by the</span>
<span class="sd">    pipeline run, tagged appropriately for this integration test.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Arrange: Find the experiment and the specific run for our test</span>
    <span class="c1"># In the real DAG, we would add a tag to the MLflow run to identify it.</span>
    <span class="n">experiment</span> <span class="o">=</span> <span class="n">mlflow_client</span><span class="o">.</span><span class="n">get_experiment_by_name</span><span class="p">(</span><span class="s2">&quot;llm-finetuning&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">experiment</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;MLflow experiment &#39;llm-finetuning&#39; not found.&quot;</span>
    
    <span class="c1"># Filter runs by tag to find our specific integration test run</span>
    <span class="n">runs</span> <span class="o">=</span> <span class="n">mlflow_client</span><span class="o">.</span><span class="n">search_runs</span><span class="p">(</span>
        <span class="n">experiment_ids</span><span class="o">=</span><span class="p">[</span><span class="n">experiment</span><span class="o">.</span><span class="n">experiment_id</span><span class="p">],</span>
        <span class="n">filter_string</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;tags.airflow_run_id LIKE &#39;scheduled__</span><span class="si">{</span><span class="n">EXECUTION_DATE</span><span class="si">}</span><span class="s2">%&#39; AND tags.dag_id = &#39;llm_continuous_training&#39;&quot;</span>
    <span class="p">)</span>
    
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">runs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;No MLflow run found for DAG &#39;llm_continuous_training&#39; on </span><span class="si">{</span><span class="n">EXECUTION_DATE</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="n">test_run</span> <span class="o">=</span> <span class="n">runs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">test_run_id</span> <span class="o">=</span> <span class="n">test_run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span>

    <span class="c1"># Act: Get all registered versions for our model</span>
    <span class="n">registered_versions</span> <span class="o">=</span> <span class="n">mlflow_client</span><span class="o">.</span><span class="n">get_latest_versions</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">stages</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;Staging&quot;</span><span class="p">])</span>
    
    <span class="c1"># Assert: Check if any of the registered versions came from our test run</span>
    <span class="n">found_match</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">version</span><span class="o">.</span><span class="n">run_id</span> <span class="o">==</span> <span class="n">test_run_id</span> <span class="k">for</span> <span class="n">version</span> <span class="ow">in</span> <span class="n">registered_versions</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">found_match</span><span class="p">,</span> \
        <span class="sa">f</span><span class="s2">&quot;Integration test failed: No model version was registered in MLflow for the test run ID </span><span class="si">{</span><span class="n">test_run_id</span><span class="si">}</span><span class="s2">.&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Integration test passed: Found a newly registered model version from run ID </span><span class="si">{</span><span class="n">test_run_id</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>CI/CD Workflow (GitHub Actions)</strong>
This workflow automates the entire test process.
<strong>.github/workflows/run_training_integration_test.yml</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Training Pipeline Integration Test</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">workflow_dispatch</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/pipelines/training/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;dags/llm_finetuning_dag.py&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">setup</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1. Setup Staging Test Data</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install -r requirements.txt</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">aws-access-key-id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}</span>
<span class="w">          </span><span class="nt">aws-secret-access-key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run setup script to upload test data</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_S3_BUCKET</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_S3_BUCKET }}</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python tests/integration/setup_training_test.py</span>

<span class="w">  </span><span class="nt">trigger-and-monitor-dag</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2. Trigger and Monitor Airflow DAG</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">setup</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install requests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install requests</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Trigger and wait for DAG run</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">AIRFLOW_HOST</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_HOST }}</span>
<span class="w">          </span><span class="nt">AIRFLOW_USERNAME</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_USERNAME }}</span>
<span class="w">          </span><span class="nt">AIRFLOW_PASSWORD</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_PASSWORD }}</span>
<span class="w">        </span><span class="c1"># This script needs to be robust, polling the Airflow API until the DAG run completes (succeeds or fails)</span>
<span class="w">        </span><span class="c1"># We also pass a special config to the DAG to set max_steps for the training job.</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">&gt;</span>
<span class="w">          </span><span class="no">python scripts/trigger_airflow_dag.py </span>
<span class="w">          </span><span class="no">--dag-id llm_continuous_training </span>
<span class="w">          </span><span class="no">--execution-date &quot;2025-01-01&quot;</span>
<span class="w">          </span><span class="no">--conf &#39;{&quot;max_steps&quot;: 2}&#39;</span>

<span class="w">  </span><span class="nt">verify</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3. Verify Model Registration in MLflow</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">trigger-and-monitor-dag</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install -r requirements-dev.txt</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run verification script</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_MLFLOW_TRACKING_URI</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_MLFLOW_TRACKING_URI }}</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest tests/integration/test_training_pipeline.py</span>
</pre></div>
</div>
</section>
<section id="id25">
<h4>CI/CD Workflow (Github Actions)<a class="headerlink" href="#id25" title="Permalink to this heading">¶</a></h4>
<p><strong>.github/workflows/deploy_training_pipeline.yml</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy LLM Fine-tuning Pipeline</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/pipelines/training/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;dags/llm_finetuning_dag.py&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">test-and-build</span><span class="p">:</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Set up Python</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies &amp; Run unit tests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pip install -r requirements-dev.txt</span>
<span class="w">          </span><span class="no">pytest tests/pipelines/training/</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">aws-access-key-id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.AWS_ACCESS_KEY_ID }}</span>
<span class="w">          </span><span class="nt">aws-secret-access-key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.AWS_SECRET_ACCESS_KEY }}</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Login to Amazon ECR</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/amazon-ecr-login@v1</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Build and push training container to ECR</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">docker build -t ${{ secrets.ECR_REGISTRY }}/llm-finetuning-image:latest -f src/pipelines/training/Dockerfile .</span>
<span class="w">          </span><span class="no">docker push ${{ secrets.ECR_REGISTRY }}/llm-finetuning-image:latest</span>

<span class="w">  </span><span class="nt">deploy</span><span class="p">:</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test-and-build</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="c1"># ... credentials setup</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Integration Test (placeholder)</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">echo &quot;Triggering and monitoring integration test...&quot;</span><span class="w"> </span><span class="c1"># This would call the test script</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Sync DAG to Production MWAA Bucket</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">success()</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws s3 sync ./dags s3://${{ secrets.MWAA_PROD_DAGS_BUCKET }}/dags --delete</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="implementation-batch-inference-pipeline">
<h3>Implementation: Batch Inference Pipeline<a class="headerlink" href="#implementation-batch-inference-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="id26">
<h4>Architecture Diagram<a class="headerlink" href="#id26" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_inference.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="id27">
<h4>Python Scripts<a class="headerlink" href="#id27" title="Permalink to this heading">¶</a></h4>
<p><strong>src/pipelines/inference/get_products.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sqlalchemy</span> <span class="kn">import</span> <span class="n">create_engine</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_products_to_update</span><span class="p">(</span><span class="n">db_connection_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">interval_hours</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets a list of product_ids that have received new reviews in the last interval.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Connecting to the application database to find products with new reviews...&quot;</span><span class="p">)</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">create_engine</span><span class="p">(</span><span class="n">db_connection_string</span><span class="p">)</span>
        
        <span class="n">end_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span>
        <span class="n">start_date</span> <span class="o">=</span> <span class="n">end_date</span> <span class="o">-</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">hours</span><span class="o">=</span><span class="n">interval_hours</span><span class="p">)</span>
        
        <span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        SELECT DISTINCT product_id</span>
<span class="s2">        FROM public.reviews</span>
<span class="s2">        WHERE created_at &gt;= &#39;</span><span class="si">{</span><span class="n">start_date</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1"> %H:%M:%S&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;</span>
<span class="s2">        &quot;&quot;&quot;</span>
        
        <span class="k">with</span> <span class="n">engine</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span> <span class="k">as</span> <span class="n">connection</span><span class="p">:</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_sql</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">connection</span><span class="p">)</span>
        
        <span class="n">product_ids</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;product_id&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">product_ids</span><span class="p">)</span><span class="si">}</span><span class="s2"> products to update.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">product_ids</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to get products to update: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
</pre></div>
</div>
<p><strong>src/pipelines/inference/retrieve_context.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">psycopg2</span>
<span class="kn">from</span> <span class="nn">pgvector.psycopg2</span> <span class="kn">import</span> <span class="n">register_vector</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="n">PROMPT_TEMPLATE</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">###Instruction: Based ONLY on the following customer reviews, provide a balanced summary of the product&#39;s pros and cons. Do not invent information.</span>

<span class="s2">###Reviews:</span>
<span class="si">{reviews_context}</span>

<span class="s2">###Response:</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">retrieve_rag_context</span><span class="p">(</span><span class="n">product_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">db_params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For each product, retrieves the RAG context from the Vector DB and constructs a prompt.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">psycopg2</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="o">**</span><span class="n">db_params</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
            <span class="n">register_vector</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">product_id</span> <span class="ow">in</span> <span class="n">product_ids</span><span class="p">:</span>
                    <span class="c1"># This query implements our advanced RAG strategy</span>
                    <span class="c1"># Note: This is a simplified example. A production query might be more complex.</span>
                    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                    (SELECT chunk_text FROM review_embeddings WHERE product_id = </span><span class="si">%s</span><span class="s2"> AND star_rating &gt;= 4 ORDER BY review_id DESC LIMIT 5)</span>
<span class="s2">                    UNION ALL</span>
<span class="s2">                    (SELECT chunk_text FROM review_embeddings WHERE product_id = </span><span class="si">%s</span><span class="s2"> AND star_rating &lt;= 2 ORDER BY review_id DESC LIMIT 5)</span>
<span class="s2">                    &quot;&quot;&quot;</span>
                    <span class="n">cur</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">(</span><span class="n">product_id</span><span class="p">,</span> <span class="n">product_id</span><span class="p">))</span>
                    <span class="n">results</span> <span class="o">=</span> <span class="n">cur</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>
                    
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">:</span>
                        <span class="k">continue</span>
                        
                    <span class="n">context_str</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
                    <span class="n">prompt_formatter</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">PROMPT_TEMPLATE</span><span class="p">)</span>
                    <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">prompt_formatter</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reviews_context</span><span class="o">=</span><span class="n">context_str</span><span class="p">)</span>
                    
                    <span class="n">prompts</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="n">product_id</span><span class="p">,</span> <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">formatted_prompt</span><span class="p">})</span>
        
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully constructed </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span><span class="si">}</span><span class="s2"> prompts.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prompts</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to retrieve RAG context: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
</pre></div>
</div>
<p><strong>src/pipelines/inference/generate_summaries.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">as_completed</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">invoke_llm_endpoint</span><span class="p">(</span><span class="n">prompts_data</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="n">endpoint_url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">api_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Invokes the LLM serving endpoint in parallel to generate summaries.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">summaries</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="n">api_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">post_request</span><span class="p">(</span><span class="n">prompt_data</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]}</span> <span class="c1"># Varies based on serving API</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">endpoint_url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
            <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="n">prompt_data</span><span class="p">[</span><span class="s2">&quot;product_id&quot;</span><span class="p">],</span> <span class="s2">&quot;summary&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;summary&quot;</span><span class="p">]}</span>
        <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to get summary for product </span><span class="si">{</span><span class="n">prompt_data</span><span class="p">[</span><span class="s1">&#39;product_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
        <span class="n">future_to_prompt</span> <span class="o">=</span> <span class="p">{</span><span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">post_request</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span> <span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts_data</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="n">future_to_prompt</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
                <span class="n">summaries</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully generated </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">summaries</span><span class="p">)</span><span class="si">}</span><span class="s2"> summaries.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">summaries</span>
</pre></div>
</div>
<p><strong>src/pipelines/inference/cache_results.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">decimal</span> <span class="kn">import</span> <span class="n">Decimal</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cache_summaries_in_dynamodb</span><span class="p">(</span><span class="n">summaries</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="n">table_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">ttl_days</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Writes the generated summaries to the DynamoDB cache table in a batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dynamodb</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">resource</span><span class="p">(</span><span class="s1">&#39;dynamodb&#39;</span><span class="p">)</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">dynamodb</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="n">table_name</span><span class="p">)</span>
    
    <span class="n">ttl_timestamp</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span> <span class="o">+</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="n">ttl_days</span><span class="p">))</span><span class="o">.</span><span class="n">timestamp</span><span class="p">())</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">table</span><span class="o">.</span><span class="n">batch_writer</span><span class="p">()</span> <span class="k">as</span> <span class="n">batch</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">summaries</span><span class="p">:</span>
                <span class="n">batch</span><span class="o">.</span><span class="n">put_item</span><span class="p">(</span>
                    <span class="n">Item</span><span class="o">=</span><span class="p">{</span>
                        <span class="s1">&#39;product_id&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;product_id&#39;</span><span class="p">],</span>
                        <span class="s1">&#39;summary_json&#39;</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;summary&#39;</span><span class="p">]),</span> <span class="c1"># Store as JSON string</span>
                        <span class="s1">&#39;last_updated&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">(),</span>
                        <span class="s1">&#39;ttl&#39;</span><span class="p">:</span> <span class="n">ttl_timestamp</span>
                    <span class="p">}</span>
                <span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully cached </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">summaries</span><span class="p">)</span><span class="si">}</span><span class="s2"> summaries in DynamoDB.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to cache summaries: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
</pre></div>
</div>
</section>
<section id="id28">
<h4>Unit Tests<a class="headerlink" href="#id28" title="Permalink to this heading">¶</a></h4>
<p><strong>tests/pipelines/inference/test_generate_summaries.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">patch</span>
<span class="kn">from</span> <span class="nn">src.pipelines.inference.generate_summaries</span> <span class="kn">import</span> <span class="n">invoke_llm_endpoint</span>

<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;requests.post&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_invoke_llm_endpoint_success</span><span class="p">(</span><span class="n">mock_post</span><span class="p">):</span>
    <span class="c1"># Arrange</span>
    <span class="n">mock_post</span><span class="o">.</span><span class="n">return_value</span><span class="o">.</span><span class="n">status_code</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">mock_post</span><span class="o">.</span><span class="n">return_value</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;summary&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;pros&quot;</span><span class="p">:</span> <span class="s2">&quot;Good&quot;</span><span class="p">,</span> <span class="s2">&quot;cons&quot;</span><span class="p">:</span> <span class="s2">&quot;Bad&quot;</span><span class="p">}}</span>
    
    <span class="n">test_prompts</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;Test prompt&quot;</span><span class="p">}]</span>
    
    <span class="c1"># Act</span>
    <span class="n">summaries</span> <span class="o">=</span> <span class="n">invoke_llm_endpoint</span><span class="p">(</span><span class="n">test_prompts</span><span class="p">,</span> <span class="s2">&quot;http://fake-url&quot;</span><span class="p">,</span> <span class="s2">&quot;fake-key&quot;</span><span class="p">)</span>

    <span class="c1"># Assert</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">summaries</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">summaries</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;product_id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;A&quot;</span>
    <span class="k">assert</span> <span class="n">summaries</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;summary&quot;</span><span class="p">][</span><span class="s2">&quot;pros&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Good&quot;</span>

<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;requests.post&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_invoke_llm_endpoint_handles_error</span><span class="p">(</span><span class="n">mock_post</span><span class="p">):</span>
    <span class="c1"># Arrange</span>
    <span class="n">mock_post</span><span class="o">.</span><span class="n">side_effect</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span><span class="p">(</span><span class="s2">&quot;API Error&quot;</span><span class="p">)</span>
    
    <span class="n">test_prompts</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;Test prompt&quot;</span><span class="p">}]</span>
    
    <span class="c1"># Act</span>
    <span class="n">summaries</span> <span class="o">=</span> <span class="n">invoke_llm_endpoint</span><span class="p">(</span><span class="n">test_prompts</span><span class="p">,</span> <span class="s2">&quot;http://fake-url&quot;</span><span class="p">,</span> <span class="s2">&quot;fake-key&quot;</span><span class="p">)</span>

    <span class="c1"># Assert</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">summaries</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="c1"># The failed request should be skipped</span>
</pre></div>
</div>
</section>
<section id="id29">
<h4>Pipeline (Airflow DAG)<a class="headerlink" href="#id29" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># dags/batch_inference_dag.py (Conceptual - showing the structure)</span>
<span class="c1"># This assumes PythonOperators calling the above functions</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span><span class="n">dag_id</span><span class="o">=</span><span class="s2">&quot;batch_inference&quot;</span><span class="p">,</span> <span class="n">schedule</span><span class="o">=</span><span class="s2">&quot;0 * * * *&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    
    <span class="n">get_products_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;get_products_to_update&quot;</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">get_products</span><span class="o">.</span><span class="n">get_products_to_update</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">check_if_products_exist</span> <span class="o">=</span> <span class="n">BranchPythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;check_if_products_exist&quot;</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="k">lambda</span> <span class="n">ti</span><span class="p">:</span> <span class="s2">&quot;retrieve_rag_context_task&quot;</span> <span class="k">if</span> <span class="n">ti</span><span class="o">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;end_pipeline&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">retrieve_context_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">generate_summaries_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cache_results_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">end_pipeline</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;end_pipeline&quot;</span><span class="p">)</span>

    <span class="n">get_products_task</span> <span class="o">&gt;&gt;</span> <span class="n">check_if_products_exist</span>
    <span class="n">check_if_products_exist</span> <span class="o">&gt;&gt;</span> <span class="p">[</span><span class="n">retrieve_context_task</span><span class="p">,</span> <span class="n">end_pipeline</span><span class="p">]</span>
    <span class="n">retrieve_context_task</span> <span class="o">&gt;&gt;</span> <span class="n">generate_summaries_task</span> <span class="o">&gt;&gt;</span> <span class="n">cache_results_task</span>
</pre></div>
</div>
</section>
<section id="id30">
<h4>Infrastructure as Code (Terraform)<a class="headerlink" href="#id30" title="Permalink to this heading">¶</a></h4>
<p><strong>infra/dynamodb.tf</strong></p>
<div class="highlight-hcl notranslate"><div class="highlight"><pre><span></span><span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_dynamodb_table&quot;</span><span class="w"> </span><span class="nv">&quot;summary_cache&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;ProductSummaryCache&quot;</span>
<span class="w">  </span><span class="na">billing_mode</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;PAY_PER_REQUEST&quot;</span><span class="c1"> # Best for spiky, infrequent workloads</span>
<span class="w">  </span><span class="na">hash_key</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;product_id&quot;</span>

<span class="w">  </span><span class="nb">attribute</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;product_id&quot;</span>
<span class="w">    </span><span class="na">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;S&quot;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="nb">ttl</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">attribute_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;ttl&quot;</span>
<span class="w">    </span><span class="na">enabled</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="no">true</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="nb">tags</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">Project</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;ReviewSummarization&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id31">
<h4>Integration Test<a class="headerlink" href="#id31" title="Permalink to this heading">¶</a></h4>
<p><strong>tests/integration/test_inference_pipeline.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># --- Test Configuration ---</span>
<span class="n">STAGING_DYNAMODB_TABLE</span> <span class="o">=</span> <span class="s2">&quot;StagingProductSummaryCache&quot;</span>
<span class="n">TEST_PRODUCT_ID</span> <span class="o">=</span> <span class="s2">&quot;product_integration_test_001&quot;</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;module&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dynamodb_client</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;dynamodb&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_inference_pipeline_caches_summary</span><span class="p">(</span><span class="n">dynamodb_client</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Verifies that after the batch inference DAG runs, a summary for the</span>
<span class="sd">    test product exists in the staging DynamoDB cache.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Arrange (Setup would have populated source DBs and run the DAG)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># Give a moment for potential eventual consistency</span>

    <span class="c1"># Act</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">dynamodb_client</span><span class="o">.</span><span class="n">get_item</span><span class="p">(</span>
            <span class="n">TableName</span><span class="o">=</span><span class="n">STAGING_DYNAMODB_TABLE</span><span class="p">,</span>
            <span class="n">Key</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;product_id&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;S&#39;</span><span class="p">:</span> <span class="n">TEST_PRODUCT_ID</span><span class="p">}}</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="n">ClientError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to query DynamoDB: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Assert</span>
    <span class="k">assert</span> <span class="s2">&quot;Item&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;No summary found in cache for product </span><span class="si">{</span><span class="n">TEST_PRODUCT_ID</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="n">item</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;Item&quot;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="s2">&quot;summary_json&quot;</span> <span class="ow">in</span> <span class="n">item</span><span class="p">,</span> <span class="s2">&quot;Cached item is missing the &#39;summary_json&#39; attribute.&quot;</span>
    
    <span class="c1"># Check if the summary is valid JSON</span>
    <span class="n">summary</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;summary_json&quot;</span><span class="p">][</span><span class="s2">&quot;S&quot;</span><span class="p">])</span>
    <span class="k">assert</span> <span class="s2">&quot;pros&quot;</span> <span class="ow">in</span> <span class="n">summary</span>
    <span class="k">assert</span> <span class="s2">&quot;cons&quot;</span> <span class="ow">in</span> <span class="n">summary</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Integration test passed: Found a valid cached summary for product </span><span class="si">{</span><span class="n">TEST_PRODUCT_ID</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id32">
<h4>CI/CD Workflow (Github Actions)<a class="headerlink" href="#id32" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Batch Inference Pipeline</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/pipelines/inference/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;dags/batch_inference_dag.py&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">test-and-deploy</span><span class="p">:</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install -r requirements-dev.txt</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run unit tests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest tests/pipelines/inference/</span>
<span class="w">      </span>
<span class="w">      </span><span class="c1"># Integration Test Steps</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Staging Credentials</span>
<span class="w">        </span><span class="c1"># ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Integration Test Setup</span>
<span class="w">        </span><span class="c1"># ... (calls setup_inference_test.py)</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Trigger Staging DAG Run</span>
<span class="w">        </span><span class="c1"># ... (calls scripts/trigger_airflow_dag.py)</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Integration Test Verification</span>
<span class="w">        </span><span class="c1"># ... (calls pytest tests/integration/test_inference_pipeline.py)</span>
<span class="w">      </span>
<span class="w">      </span><span class="c1"># Deploy to Production</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Production Credentials</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">success()</span>
<span class="w">        </span><span class="c1"># ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Sync DAG to Production MWAA Bucket</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">success()</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws s3 sync ./dags s3://${{ secrets.MWAA_PROD_DAGS_BUCKET }}/dags --delete</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="implementation-monitoring-and-alerting">
<h3>Implementation: Monitoring and Alerting<a class="headerlink" href="#implementation-monitoring-and-alerting" title="Permalink to this heading">¶</a></h3>
<section id="id33">
<h4>Architecture Diagram<a class="headerlink" href="#id33" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_summarisation/pipeline_monitoring.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="monitoring-quality">
<h4>Monitoring Quality<a class="headerlink" href="#monitoring-quality" title="Permalink to this heading">¶</a></h4>
<p><strong>src/monitoring/quality_monitor.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="c1"># Assume Ragas and OpenAI are installed and configured</span>
<span class="c1"># from ragas import evaluate</span>
<span class="c1"># from ragas.metrics import faithfulness, context_precision</span>
<span class="c1"># from openai import OpenAI</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># --- Configuration ---</span>
<span class="n">DYNAMODB_TABLE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;SUMMARY_CACHE_TABLE&quot;</span><span class="p">]</span>
<span class="n">CLOUDWATCH_NAMESPACE</span> <span class="o">=</span> <span class="s2">&quot;LLMReviewSummarizer&quot;</span>

<span class="k">def</span> <span class="nf">get_recent_summaries</span><span class="p">(</span><span class="n">table_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">hours</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fetches recently generated summaries from the DynamoDB cache.&quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fetching summaries from the last </span><span class="si">{</span><span class="n">hours</span><span class="si">}</span><span class="s2"> hours from table </span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">dynamodb</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">resource</span><span class="p">(</span><span class="s1">&#39;dynamodb&#39;</span><span class="p">)</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">dynamodb</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="n">table_name</span><span class="p">)</span>
    
    <span class="c1"># In a real system, you&#39;d scan with a filter. For simplicity, we&#39;ll assume a GSI.</span>
    <span class="c1"># For now, we return a mock DataFrame.</span>
    <span class="n">mock_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;prod_123&quot;</span><span class="p">,</span> <span class="s2">&quot;prod_456&quot;</span><span class="p">],</span>
        <span class="s2">&quot;summary_json&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s1">&#39;{&quot;pros&quot;: &quot;Very fast.&quot;, &quot;cons&quot;: &quot;Gets hot.&quot;}&#39;</span><span class="p">,</span>
            <span class="s1">&#39;{&quot;pros&quot;: &quot;Great design.&quot;, &quot;cons&quot;: &quot;Battery is weak.&quot;}&#39;</span>
        <span class="p">],</span>
        <span class="c1"># In a real system, we&#39;d also fetch the review context used for generation.</span>
        <span class="s2">&quot;review_context&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;The laptop is incredibly fast for all my tasks.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;The battery life is a major issue, lasts only 2 hours.&quot;</span>
        <span class="p">]</span>
    <span class="p">}</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Returning mock summaries for demonstration.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">mock_data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate_summaries</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates summaries using Ragas and LLM-as-a-judge (mocked).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="s2"> summaries.&quot;</span><span class="p">)</span>
    <span class="c1"># In a real implementation:</span>
    <span class="c1"># 1. Format data for Ragas (question, answer, contexts, ground_truth)</span>
    <span class="c1"># 2. Call `evaluate(dataset, metrics=[faithfulness, ...])`</span>
    <span class="c1"># 3. Call OpenAI API for LLM-as-a-judge coherence score</span>
    
    <span class="c1"># Mocked results</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;faithfulness_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.98</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">]</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;coherence_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">]</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;toxicity_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">]</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Evaluation complete.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>

<span class="k">def</span> <span class="nf">publish_metrics_to_cloudwatch</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculates aggregate scores and publishes them as CloudWatch Custom Metrics.&quot;&quot;&quot;</span>
    <span class="n">cloudwatch</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s1">&#39;cloudwatch&#39;</span><span class="p">)</span>
    
    <span class="n">avg_faithfulness</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;faithfulness_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">avg_coherence</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;coherence_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Publishing metrics to CloudWatch: Faithfulness=</span><span class="si">{</span><span class="n">avg_faithfulness</span><span class="si">}</span><span class="s2">, Coherence=</span><span class="si">{</span><span class="n">avg_coherence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="n">metric_data</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s1">&#39;MetricName&#39;</span><span class="p">:</span> <span class="s1">&#39;AverageFaithfulness&#39;</span><span class="p">,</span>
            <span class="s1">&#39;Value&#39;</span><span class="p">:</span> <span class="n">avg_faithfulness</span><span class="p">,</span>
            <span class="s1">&#39;Unit&#39;</span><span class="p">:</span> <span class="s1">&#39;None&#39;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s1">&#39;MetricName&#39;</span><span class="p">:</span> <span class="s1">&#39;AverageCoherence&#39;</span><span class="p">,</span>
            <span class="s1">&#39;Value&#39;</span><span class="p">:</span> <span class="n">avg_coherence</span><span class="p">,</span>
            <span class="s1">&#39;Unit&#39;</span><span class="p">:</span> <span class="s1">&#39;None&#39;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="n">cloudwatch</span><span class="o">.</span><span class="n">put_metric_data</span><span class="p">(</span>
        <span class="n">Namespace</span><span class="o">=</span><span class="n">CLOUDWATCH_NAMESPACE</span><span class="p">,</span>
        <span class="n">MetricData</span><span class="o">=</span><span class="n">metric_data</span>
    <span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Metrics successfully published.&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Main function to run the monitoring pipeline.&quot;&quot;&quot;</span>
    <span class="n">summaries_df</span> <span class="o">=</span> <span class="n">get_recent_summaries</span><span class="p">(</span><span class="n">DYNAMODB_TABLE</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">summaries_df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="n">evaluated_df</span> <span class="o">=</span> <span class="n">evaluate_summaries</span><span class="p">(</span><span class="n">summaries_df</span><span class="p">)</span>
        <span class="n">publish_metrics_to_cloudwatch</span><span class="p">(</span><span class="n">evaluated_df</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No new summaries found to monitor.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id34">
<h4>Unit Test<a class="headerlink" href="#id34" title="Permalink to this heading">¶</a></h4>
<p><strong>tests/monitoring/test_quality_monitor.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">patch</span><span class="p">,</span> <span class="n">MagicMock</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">src.monitoring</span> <span class="kn">import</span> <span class="n">quality_monitor</span>

<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;boto3.client&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_publish_metrics_to_cloudwatch</span><span class="p">(</span><span class="n">mock_boto_client</span><span class="p">):</span>
    <span class="c1"># Arrange</span>
    <span class="n">mock_cloudwatch</span> <span class="o">=</span> <span class="n">MagicMock</span><span class="p">()</span>
    <span class="n">mock_boto_client</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="n">mock_cloudwatch</span>
    
    <span class="n">test_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;faithfulness_score&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="c1"># Avg = 0.95</span>
        <span class="s1">&#39;coherence_score&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span>     <span class="c1"># Avg = 4.5</span>
    <span class="p">}</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
    
    <span class="c1"># Act</span>
    <span class="n">quality_monitor</span><span class="o">.</span><span class="n">publish_metrics_to_cloudwatch</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span>
    
    <span class="c1"># Assert</span>
    <span class="n">mock_cloudwatch</span><span class="o">.</span><span class="n">put_metric_data</span><span class="o">.</span><span class="n">assert_called_once</span><span class="p">()</span>
    
    <span class="c1"># Get the arguments passed to the mock</span>
    <span class="n">call_args</span> <span class="o">=</span> <span class="n">mock_cloudwatch</span><span class="o">.</span><span class="n">put_metric_data</span><span class="o">.</span><span class="n">call_args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">assert</span> <span class="n">call_args</span><span class="p">[</span><span class="s1">&#39;Namespace&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;LLMReviewSummarizer&quot;</span>
    
    <span class="n">metric_data</span> <span class="o">=</span> <span class="n">call_args</span><span class="p">[</span><span class="s1">&#39;MetricData&#39;</span><span class="p">]</span>
    <span class="n">faithfulness_metric</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">metric_data</span> <span class="k">if</span> <span class="n">m</span><span class="p">[</span><span class="s1">&#39;MetricName&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;AverageFaithfulness&#39;</span><span class="p">)</span>
    <span class="n">coherence_metric</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">metric_data</span> <span class="k">if</span> <span class="n">m</span><span class="p">[</span><span class="s1">&#39;MetricName&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;AverageCoherence&#39;</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">faithfulness_metric</span><span class="p">[</span><span class="s1">&#39;Value&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">0.95</span>
    <span class="k">assert</span> <span class="n">coherence_metric</span><span class="p">[</span><span class="s1">&#39;Value&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">4.5</span>
</pre></div>
</div>
</section>
<section id="id35">
<h4>Pipeline Code (Airflow DAG)<a class="headerlink" href="#id35" title="Permalink to this heading">¶</a></h4>
<p><strong>dags/model_quality_monitoring_dag.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.models.dag</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.docker_operator</span> <span class="kn">import</span> <span class="n">DockerOperator</span>

<span class="c1"># Assumes the monitoring script is containerized in an image in ECR</span>
<span class="n">ECR_IMAGE</span> <span class="o">=</span> <span class="s2">&quot;123456789012.dkr.ecr.eu-west-1.amazonaws.com/quality-monitor:latest&quot;</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s2">&quot;model_quality_monitoring&quot;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2024</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="s2">&quot;UTC&quot;</span><span class="p">),</span>
    <span class="n">schedule</span><span class="o">=</span><span class="s2">&quot;0 3 * * *&quot;</span><span class="p">,</span>  <span class="c1"># Run daily at 3 AM UTC</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;monitoring&quot;</span><span class="p">,</span> <span class="s2">&quot;quality&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">run_quality_monitor</span> <span class="o">=</span> <span class="n">DockerOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;run_quality_monitor&quot;</span><span class="p">,</span>
        <span class="n">image</span><span class="o">=</span><span class="n">ECR_IMAGE</span><span class="p">,</span>
        <span class="n">api_version</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">auto_remove</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="c1"># Pass environment variables needed by the script</span>
        <span class="n">environment</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;SUMMARY_CACHE_TABLE&quot;</span><span class="p">:</span> <span class="s2">&quot;ProductionProductSummaryCache&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AWS_ACCESS_KEY_ID&quot;</span><span class="p">:</span> <span class="s2">&quot;{{ conn.aws_default.login }}&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AWS_SECRET_ACCESS_KEY&quot;</span><span class="p">:</span> <span class="s2">&quot;{{ conn.aws_default.password }}&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AWS_SESSION_TOKEN&quot;</span><span class="p">:</span> <span class="s2">&quot;{{ conn.aws_default.extra_dejson.aws_session_token }}&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AWS_REGION&quot;</span><span class="p">:</span> <span class="s2">&quot;eu-west-1&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">command</span><span class="o">=</span><span class="s2">&quot;/usr/bin/python3 quality_monitor.py&quot;</span><span class="p">,</span>
        <span class="n">docker_url</span><span class="o">=</span><span class="s2">&quot;unix://var/run/docker.sock&quot;</span><span class="p">,</span>
        <span class="n">network_mode</span><span class="o">=</span><span class="s2">&quot;bridge&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id36">
<h4>Infrastructure as Code (Terraform)<a class="headerlink" href="#id36" title="Permalink to this heading">¶</a></h4>
<p><strong>infra/monitoring.tf</strong></p>
<div class="highlight-hcl notranslate"><div class="highlight"><pre><span></span><span class="kr">variable</span><span class="w"> </span><span class="nv">&quot;faithfulness_threshold&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">description</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;The minimum acceptable faithfulness score before triggering an alert.&quot;</span>
<span class="w">  </span><span class="na">type</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="kt">number</span>
<span class="w">  </span><span class="na">default</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="nv">0.95</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sns_topic&quot;</span><span class="w"> </span><span class="nv">&quot;alerts_topic&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;LLM-Summarizer-Alerts-Topic&quot;</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sns_topic_subscription&quot;</span><span class="w"> </span><span class="nv">&quot;email_subscription&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">topic_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_sns_topic.alerts_topic.arn</span>
<span class="w">  </span><span class="na">protocol</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;email&quot;</span>
<span class="w">  </span><span class="na">endpoint</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;oncall-ml-team@example.com&quot;</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_cloudwatch_metric_alarm&quot;</span><span class="w"> </span><span class="nv">&quot;faithfulness_alarm&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">alarm_name</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;High-Hallucination-Rate-Alarm&quot;</span>
<span class="w">  </span><span class="na">comparison_operator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;LessThanThreshold&quot;</span>
<span class="w">  </span><span class="na">evaluation_periods</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;1&quot;</span>
<span class="w">  </span><span class="na">metric_name</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;AverageFaithfulness&quot;</span>
<span class="w">  </span><span class="na">namespace</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;LLMReviewSummarizer&quot;</span>
<span class="w">  </span><span class="na">period</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;86400&quot;</span><span class="c1"> # 24 hours, matching the DAG schedule</span>
<span class="w">  </span><span class="na">statistic</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Average&quot;</span>
<span class="w">  </span><span class="na">threshold</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="nv">var.faithfulness_threshold</span>
<span class="w">  </span><span class="na">alarm_description</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;This alarm triggers if the average summary faithfulness score drops below the acceptable threshold.&quot;</span>
<span class="w">  </span>
<span class="w">  </span><span class="na">alarm_actions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="nv">aws_sns_topic.alerts_topic.arn</span><span class="p">]</span>
<span class="w">  </span><span class="na">ok_actions</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="nv">aws_sns_topic.alerts_topic.arn</span><span class="p">]</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_cloudwatch_dashboard&quot;</span><span class="w"> </span><span class="nv">&quot;summarizer_dashboard&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">dashboard_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;LLM-Review-Summarizer-Dashboard&quot;</span>

<span class="w">  </span><span class="na">dashboard_body</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">jsonencode</span><span class="p">({</span>
<span class="w">    </span><span class="na">widgets</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="na">type</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;metric&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="na">x</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span>
<span class="w">        </span><span class="na">y</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span>
<span class="w">        </span><span class="na">width</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="m">12</span><span class="p">,</span>
<span class="w">        </span><span class="na">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">,</span>
<span class="w">        </span><span class="nb">properties</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="na">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">            </span><span class="p">[</span><span class="s2">&quot;LLMReviewSummarizer&quot;, &quot;AverageFaithfulness&quot;</span><span class="p">]</span>
<span class="w">          </span><span class="p">],</span>
<span class="w">          </span><span class="na">period</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">300</span><span class="p">,</span>
<span class="w">          </span><span class="na">stat</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Average&quot;</span><span class="p">,</span>
<span class="w">          </span><span class="na">region</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;eu-west-1&quot;</span><span class="p">,</span>
<span class="w">          </span><span class="na">title</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Summary Faithfulness (Daily Average)&quot;</span>
<span class="c1">          # Add horizontal annotation for the alarm threshold</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">},</span>
<span class="c1">      # ... other widgets for coherence, EKS GPU utilization, etc.</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">  </span><span class="p">})</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="ci-cd-github-actions-workflow">
<h4>CI/CD Github Actions Workflow<a class="headerlink" href="#ci-cd-github-actions-workflow" title="Permalink to this heading">¶</a></h4>
<p><strong>.github/workflows/deploy_monitoring.yml</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Monitoring System</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/monitoring/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;dags/model_quality_monitoring_dag.py&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;infra/monitoring.tf&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">test-and-build</span><span class="p">:</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies and run unit tests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pip install -r requirements-dev.txt</span>
<span class="w">          </span><span class="no">pytest tests/monitoring/</span>
<span class="w">      </span><span class="c1"># ... steps to build and push the quality-monitor docker image to ECR</span>

<span class="w">  </span><span class="nt">deploy-infra</span><span class="p">:</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test-and-build</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hashicorp/setup-terraform@v2</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="c1"># ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Terraform Apply for Monitoring</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">terraform -chdir=infra init</span>
<span class="w">          </span><span class="no">terraform -chdir=infra apply -auto-approve -target=aws_sns_topic.alerts_topic -target=aws_cloudwatch_metric_alarm.faithfulness_alarm -target=aws_cloudwatch_dashboard.summarizer_dashboard</span>

<span class="w">  </span><span class="nt">deploy-dag</span><span class="p">:</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deploy-infra</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="c1"># ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Sync Monitoring DAG to Production</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws s3 sync ./dags s3://${{ secrets.MWAA_PROD_DAGS_BUCKET }}/dags --delete</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="ecom_rag.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">RAG-Based Product Discovery</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="ecom_propensity.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Real-Time Purchase Intent Scoring</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, Deepak Karkala
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Reviews Summarisation</a><ul>
<li><a class="reference internal" href="#id1"></a><ul>
<li><a class="reference internal" href="#tldr-end-to-end-llm-powered-review-summarization"><strong>TLDR: End-to-End LLM-Powered Review Summarization</strong></a></li>
<li><a class="reference internal" href="#the-business-imperative-from-information-overload-to-actionable-intelligence">1. The Business Imperative: From Information Overload to Actionable Intelligence</a><ul>
<li><a class="reference internal" href="#project-objectives-and-goals"><strong>Project Objectives and Goals</strong></a></li>
<li><a class="reference internal" href="#measuring-success-key-performance-indicators-kpis"><strong>Measuring Success: Key Performance Indicators (KPIs)</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#ml-problem-framing-from-business-need-to-technical-blueprint">2. ML Problem Framing: From Business Need to Technical Blueprint</a><ul>
<li><a class="reference internal" href="#setting-the-business-objectives">2.1 Setting the Business Objectives</a></li>
<li><a class="reference internal" href="#is-machine-learning-the-right-approach">2.2 Is Machine Learning the Right Approach?</a></li>
<li><a class="reference internal" href="#defining-the-ml-problem">2.3 Defining the ML Problem</a></li>
<li><a class="reference internal" href="#assessing-feasibility-risks">2.4 Assessing Feasibility &amp; Risks</a></li>
<li><a class="reference internal" href="#defining-success-metrics">2.5 Defining Success Metrics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#genai-application-end-to-end-planning">3. GenAI Application: End to end planning</a><ul>
<li><a class="reference internal" href="#llmops-tech-stack">3.1 LLMOps Tech Stack</a></li>
<li><a class="reference internal" href="#key-pipelines-and-workflows">3.2 Key Pipelines and Workflows</a></li>
<li><a class="reference internal" href="#why-rag-for-reviews-summarization">3.3 Why RAG for Reviews Summarization ?</a><ul>
<li><a class="reference internal" href="#approach-1-recursive-summarization">Approach 1: Recursive Summarization</a></li>
<li><a class="reference internal" href="#approach-2-rag-for-batch-summarization-the-recommended-architecture">Approach 2: RAG for Batch Summarization (The Recommended Architecture)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#project-management-and-stages">3.3 Project Management and Stages</a></li>
<li><a class="reference internal" href="#cross-functional-team-roles">3.4 Cross-Functional Team &amp; Roles</a></li>
<li><a class="reference internal" href="#versioning-and-governance-strategy">3.5 Versioning and Governance Strategy</a></li>
<li><a class="reference internal" href="#comprehensive-evaluation-strategy">3.6 Comprehensive Evaluation Strategy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#data-engineering-pipelines-the-foundation-for-summarization">5. Data Engineering &amp; Pipelines: The Foundation for Summarization</a><ul>
<li><a class="reference internal" href="#pipeline-1-daily-data-ingestion-cleaning"><strong>Pipeline 1: Daily Data Ingestion &amp; Cleaning</strong></a><ul>
<li><a class="reference internal" href="#architecture-diagram">Architecture Diagram</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pipeline-2-embedding-generation"><strong>Pipeline 2: Embedding Generation</strong></a><ul>
<li><a class="reference internal" href="#id2">Architecture Diagram</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#feature-engineering-from-hand-crafted-features-to-semantic-vectors">6. Feature Engineering: From Hand-Crafted Features to Semantic Vectors</a><ul>
<li><a class="reference internal" href="#the-features-for-our-rag-system">6.1 The “Features” for our RAG System</a></li>
<li><a class="reference internal" href="#the-vector-database-the-new-feature-store">6.2 The Vector Database: The New Feature Store</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ml-training-pipeline-planning-the-continuous-fine-tuning-workflow">8. ML Training Pipeline: Planning the Continuous Fine-Tuning Workflow</a><ul>
<li><a class="reference internal" href="#python-scripts-pipeline-components"><strong>8.1 Python Scripts (Pipeline Components)</strong></a></li>
<li><a class="reference internal" href="#unit-tests-pytest"><strong>8.2 Unit Tests (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong></a></li>
<li><a class="reference internal" href="#pipeline-code-airflow-dag"><strong>8.3 Pipeline Code (Airflow DAG)</strong></a></li>
<li><a class="reference internal" href="#infrastructure-as-code-terraform"><strong>8.4 Infrastructure as Code (Terraform)</strong></a></li>
<li><a class="reference internal" href="#integration-test"><strong>8.5 Integration Test</strong></a></li>
<li><a class="reference internal" href="#id3"><strong>8.6 Architecture Diagram</strong></a></li>
<li><a class="reference internal" href="#ci-cd-workflow-github-actions"><strong>8.7 CI/CD Workflow (GitHub Actions)</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#batch-inference-pipeline-planning-the-production-summarization-workflow">9. Batch Inference Pipeline: Planning the Production Summarization Workflow</a><ul>
<li><a class="reference internal" href="#id4"><strong>9.1 Python Scripts (Pipeline Components)</strong></a></li>
<li><a class="reference internal" href="#id5"><strong>9.2 Unit Tests (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong></a></li>
<li><a class="reference internal" href="#id6"><strong>9.3 Pipeline Code (Airflow DAG)</strong></a></li>
<li><a class="reference internal" href="#id7"><strong>9.4 Infrastructure as Code (Terraform)</strong></a></li>
<li><a class="reference internal" href="#id8"><strong>9.5 Integration Test</strong></a></li>
<li><a class="reference internal" href="#id9"><strong>9.6 Architecture Diagram</strong></a></li>
<li><a class="reference internal" href="#id10"><strong>9.7 CI/CD Workflow (GitHub Actions)</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-llm-serving-endpoint-deployment-pipeline">Model (LLM Serving Endpoint) Deployment Pipeline</a><ul>
<li><a class="reference internal" href="#why-a-shared-reusable-load-test">Why a shared, reusable Load Test ?</a></li>
<li><a class="reference internal" href="#ci-cd-for-the-llm-serving-endpoint-with-the-shared-load-test">CI/CD for the LLM Serving Endpoint (with the shared Load Test)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#monitoring-and-observability-ensuring-production-health-and-quality">10. Monitoring and Observability: Ensuring Production Health and Quality</a><ul>
<li><a class="reference internal" href="#monitoring-and-alerting-plan"><strong>Monitoring and Alerting Plan</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#closing-the-loop-continual-learning-production-testing">11. Closing the Loop: Continual Learning &amp; Production Testing</a><ul>
<li><a class="reference internal" href="#continual-learning-retraining-strategy">11.1 Continual Learning &amp; Retraining Strategy</a></li>
<li><a class="reference internal" href="#mitigating-catastrophic-forgetting-a-core-llm-challenge">11.2 Mitigating Catastrophic Forgetting: A Core LLM Challenge</a></li>
<li><a class="reference internal" href="#phased-production-testing-from-safety-to-business-impact">11.3 Phased Production Testing: From Safety to Business Impact</a></li>
<li><a class="reference internal" href="#a-b-testing-framework-for-a-batch-system">11.4 A/B Testing Framework for a Batch System</a></li>
</ul>
</li>
<li><a class="reference internal" href="#governance-ethics-the-human-element">12. Governance, Ethics &amp; The Human Element</a><ul>
<li><a class="reference internal" href="#comprehensive-model-governance">12.1 Comprehensive Model Governance</a></li>
<li><a class="reference internal" href="#responsible-ai-rai-principles-in-practice">12.2 Responsible AI (RAI) Principles in Practice</a></li>
<li><a class="reference internal" href="#holistic-testing-production-readiness-ml-test-score">12.3 Holistic Testing &amp; Production Readiness (ML Test Score)</a></li>
<li><a class="reference internal" href="#the-human-element-team-user-experience">12.4 The Human Element: Team &amp; User Experience</a></li>
</ul>
</li>
<li><a class="reference internal" href="#overall-system-architecture">13. Overall System Architecture</a><ul>
<li><a class="reference internal" href="#aws-system-architecture-diagram">13.1 AWS System Architecture Diagram</a></li>
<li><a class="reference internal" href="#sequence-diagram-batch-inference-workflow">13.2 Sequence Diagram: Batch Inference Workflow</a></li>
<li><a class="reference internal" href="#potential-bottlenecks-and-performance-optimizations">13.3 Potential Bottlenecks and Performance Optimizations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-data-ingestion-pipeline">Implementation: Data Ingestion Pipeline</a><ul>
<li><a class="reference internal" href="#id11">Architecture Diagram</a></li>
<li><a class="reference internal" href="#python-scripts">Python Scripts</a></li>
<li><a class="reference internal" href="#unit-tests">Unit Tests</a></li>
<li><a class="reference internal" href="#pipeline-airflow-dag">Pipeline (Airflow DAG)</a></li>
<li><a class="reference internal" href="#id12">Integration Test</a></li>
<li><a class="reference internal" href="#id13">CI/CD Workflow (Github Actions)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-embeddings-generation-pipeline">Implementation: Embeddings Generation Pipeline</a><ul>
<li><a class="reference internal" href="#id14">Architecture Diagram</a></li>
<li><a class="reference internal" href="#id15">Python Scripts</a></li>
<li><a class="reference internal" href="#unit-test">Unit Test</a></li>
<li><a class="reference internal" href="#id16">Pipeline Code (Airflow DAG)</a></li>
<li><a class="reference internal" href="#id17">Integration Test</a><ul>
<li><a class="reference internal" href="#setup-scripts-data"><strong>1. Setup Scripts &amp; Data</strong></a></li>
<li><a class="reference internal" href="#verification-script-pytest"><strong>2. Verification Script (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#id18">CI/CD Workflow (GitHub Actions)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-llm-fine-tuning-pipeline">Implementation: LLM Fine-tuning Pipeline</a><ul>
<li><a class="reference internal" href="#id19">Architecture Diagram</a></li>
<li><a class="reference internal" href="#id20">Python Scripts</a></li>
<li><a class="reference internal" href="#id21">Unit Tests</a></li>
<li><a class="reference internal" href="#id22">Pipeline (Airflow DAG)</a></li>
<li><a class="reference internal" href="#id23">Infrastructure as Code (Terraform)</a></li>
<li><a class="reference internal" href="#id24">Integration Test</a></li>
<li><a class="reference internal" href="#id25">CI/CD Workflow (Github Actions)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-batch-inference-pipeline">Implementation: Batch Inference Pipeline</a><ul>
<li><a class="reference internal" href="#id26">Architecture Diagram</a></li>
<li><a class="reference internal" href="#id27">Python Scripts</a></li>
<li><a class="reference internal" href="#id28">Unit Tests</a></li>
<li><a class="reference internal" href="#id29">Pipeline (Airflow DAG)</a></li>
<li><a class="reference internal" href="#id30">Infrastructure as Code (Terraform)</a></li>
<li><a class="reference internal" href="#id31">Integration Test</a></li>
<li><a class="reference internal" href="#id32">CI/CD Workflow (Github Actions)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-monitoring-and-alerting">Implementation: Monitoring and Alerting</a><ul>
<li><a class="reference internal" href="#id33">Architecture Diagram</a></li>
<li><a class="reference internal" href="#monitoring-quality">Monitoring Quality</a></li>
<li><a class="reference internal" href="#id34">Unit Test</a></li>
<li><a class="reference internal" href="#id35">Pipeline Code (Airflow DAG)</a></li>
<li><a class="reference internal" href="#id36">Infrastructure as Code (Terraform)</a></li>
<li><a class="reference internal" href="#ci-cd-github-actions-workflow">CI/CD Github Actions Workflow</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/scripts/furo.js?v=4e2eecee"></script>
    </body>
</html>