<!doctype html>
<html class="no-js" lang="en" data-content_root="">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Projects" href="../projects/index.html" /><link rel="prev" title="Reviews Summarisation" href="ecom_summarisation.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><!-- Generated with Sphinx 7.1.2 and Furo 2024.05.06 -->
        <title>RAG-Based Product Discovery - Home</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=387cc868" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/style.css?v=8a7ff5ee" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" /
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Home</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">Home</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Past Experiences</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Past Experiences</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="iot_anomaly.html">Anomaly Detection in Time Series IoT Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="iot_forecasting.html">Energy Demand Forecasting in Time Series IoT Data</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="adas_engine/index.html">ADAS: Data Engine</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of ADAS: Data Engine</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch0_business_challenge.html">Business Challenge and Goals</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch1_ml_problem_framing.html">ML Problem Framing</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch2_operational_strategy.html">Planning, Operational Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch3_pipelines_workflows.html">Workflows, Team, Roles</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch4_testing_strategy.html">Testing Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch6_data_ingestion_workflows.html">Data Ingestion Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch7_scene_understanding_data_mining.html">Scene Understanding &amp; Data Mining</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch8_model_training.html">Model Training &amp; Experimentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch9_packaging_promotion.html">Packaging, Evaluation &amp; Promotion Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch10_deployment_serving.html">Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch11_monitoring_continual_learning.html">Monitoring &amp; Continual Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch12_cost_lifecycle_compliance.html">Cost, Lifecycle, Compliance</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch13_reliability_capacity_maps.html">Reliability, Capacity, Maps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ecom_cltv.html">Customer Lifetime Value</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_propensity.html">Real-Time Purchase Intent Scoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_summarisation.html">Reviews Summarisation</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">RAG-Based Product Discovery</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../projects/index.html">Projects</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Projects</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/nlp/index.html">Natural Language Processing</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Natural Language Processing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/airbnb_alternate_search/about/index.html">Airbnb Listing description based Semantic Search</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/cv/index.html">Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Computer Vision</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/ecommerce_image_segmentation/about/index.html">Image Segmentation for Ecommerce Products</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/ml/index.html">Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Machine Learning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/airbnb_price_modeling/about/index.html">Predictive Price Modeling for Airbnb listings</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications/index.html">Patents, Papers, Thesis</a></li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../agents/index.html">AI Agents: A Lead Engineer’s Handbook</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of AI Agents: A Lead Engineer’s Handbook</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch1_intro.html">Agent Fundamentals: What, Why, and When?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch2_patterns.html">Agentic Patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch5_context_engineering.html">Context Engineering for AI Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch6_case_studies.html">The State of the Industry: Insights from the Field</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch7_conclusion.html"><strong>Conclusion: The Lead Engineer’s Mental Model for Building Agents</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_cost.html">Cost Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_data.html">Data Management and Knowledge Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_deploy.html">Deployment and Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_guardrails.html">Guardrails</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_hitl.html">Human-in-the-Loop (HITL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_latency.html">Latency Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_llm.html">LLM – Prompts, Goals, and Persona</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_memory.html">Managing Agent Memory (Short-Term and Long-Term)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_monitor.html">Monitoring and Observability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_orchestration.html">Orchestration and Task Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_prod.html">Production Challenges and Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_security.html">Securing AI Agents and Preventing Abuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_tool.html">Tool Use and Integration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_trust.html">Building Trustworthy and Ethical AI Agents</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">MLOps</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of MLOps</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch1_problem_framing.html">ML Problem framing</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of ML Problem framing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/ch2_blueprint_operational_strategy.html">The MLOps Blueprint &amp; Operational Strategy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch2a_platform/index.html">ML Platforms</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of ML Platforms</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/ml_platforms.html">ML Platforms: How to</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/uber.html">Uber Michelangelo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/linkedin.html">LinkedIn DARWIN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/netflix.html">Netflix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/shopify.html">Shopify Merlin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/zomato.html">Zomato: Real-time ML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/coveo.html">Coveo: MLOPs at reasonable scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/monzo.html">Monzo ML Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/didact.html">Didact AI</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch3_project_planning/index.html">Project Planning</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of Project Planning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/prd.html">Project Requirements Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/tech_stack.html">Tech Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/config_management.html">Config Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/pipeline_design.html">Pipeline Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/environment_strategy.html">Environment Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/cicd_branching_model.html">CI/CD Strategy and Branching Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/directory_structure.html">Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/env_branchind_cicd_deployment.html">Environments, Branching, CI/CD, and Deployments Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/project_management.html">Project Management for MLOps</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch4_data_discovery/index.html">Data Sourcing, Discovery</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of Data Sourcing, Discovery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/data_sourcing_discovery.html">Data Sourcing, Discovery &amp; Understanding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/ch4_project.html">Project-Trending Now: Implementing Web Scraping, Ingestion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/industry_case_studies.html">Data Discovery Platforms: Industry Case Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/facebook_nemo.html">Facebook: Nemo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/netflix_metacat.html">Netflix Metacat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/uber_databook.html">Uber Databook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/linkedin_datahub.html">LinkedIn Datahub</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch5_data_pipelines/index.html">Data Engineering, Pipelines</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of Data Engineering, Pipelines</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/data_pipelines.html">Data Engineering for Reliable ML Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/data_engineering_pipelines.html">Data Engineering &amp; Pipelines: A Lead’s Compendium</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/streaming_pipelines.html">Real-Time &amp; Streaming Data Pipelines: Challenges, Solutions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/netflix_keystone.html">Netflix Keystone</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/doordash_riviera.html">Doordash Riviera</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch6_feature_engg/index.html">Feature Engineering, Feature Stores</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of Feature Engineering, Feature Stores</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch6_feature_engg/feature_engineering_store.html">Feature Engineering and Feature Stores</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch6_feature_engg/feature_engineering.html">Feature Engineering for MLOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch6_feature_engg/explained_feature_stores.html">Feature Stores for MLOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch6_feature_engg/point_in_time.html">Point-in-Time Correctness &amp; Time Travel in ML Data Pipelines</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/index.html">Feast Feature Store</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of Feast Feature Store</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_architecture.html">Feast Architecture: A Technical Deep Dive for MLOps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_concepts.html">Feast Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_components.html">Feast Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_usecases.html">Feast Use Cases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_aws.html">Running Feast with AWS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/run_in_prod.html">Running Feast in Production</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/validate_historical_with_gx.html">Validating Historical Features with Great Expectations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/add_reuse_tests.html">Adding or Reusing Tests in Feast</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch7_model_development/index.html">Model Development, Tuning, Selection, Ensembles, Calibration</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of Model Development, Tuning, Selection, Ensembles, Calibration</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/ch7_model_development.html">Chapter 7: Model Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/dl_training_playbook.html">How to train DL Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/development.html">Model Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/industry_lessons.html">Model Development: Lessons from production systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/ensembles.html"><strong>Model Ensembles</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/selection.html">Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/tuning_hypopt.html">Hyperparameter Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/expt_tracking.html">ML Expt tracking, Data Lineage, Model Registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/calibration.html">Model Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/ch8_ml_pipelines.html">ML Training Pipelines</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch9_ml_testing/index.html">Testing in ML Systems</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of Testing in ML Systems</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch9_ml_testing/ch9_ml_testing.html">Testing in ML Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch9_ml_testing/data_testing_validation.html">Data Testing &amp; Validation in Production</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch9_ml_testing/ml_testing.html">Testing ML Systems: Ensuring Reliability from Code to Production</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch10_deployment_serving/index.html">Model Deployment &amp; Serving</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of Model Deployment &amp; Serving</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/ch10_deployment_serving.html">Chapter 10: Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/guide_deployment_serving.html">Guide: Model Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/guide_inference_stack.html">Deep Dive: Inference Stack</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/index.html">Monitoring, Observability, Drift, Interpretability</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle navigation of Monitoring, Observability, Drift, Interpretability</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/ch11_monitor_observe_drift.html">Chapter 11: Monitoring, Observability, Drifts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_monitor_observe_drift.html">Guide: ML System Failures, Data Distribution Shifts, Monitoring, and Observability</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_interpretability_shap_lime.html">Interpretability, SHAP, LIME</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_stack.html">Prometheus + Grafana and ELK Stacks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/index.html">Continual learning, Retraining, A/B Testing</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle navigation of Continual learning, Retraining, A/B Testing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/ch12_continual_learning_prod_testing.html">Chapter 12: Continual Learning &amp; Production Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_continual_learning.html">Continual Learning &amp; Model Retraining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_ab_testing.html">A/B Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_ab_testing_industry_lessons.html">A/B Testing &amp; Experimentation: Industry lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_prod_testing_expt.html">Guide: Production Testing &amp; Experimentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/dr_prod_testing_expt.html">Deep Research: Production Testing &amp; Experimentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/ch13_governance_ethics_human.html">Governance, Ethics &amp; The Human Element</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/index.html">PyTorch</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><div class="visually-hidden">Toggle navigation of PyTorch</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/general.html">General</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/state_dict.html">state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/distributed_data_parallel.html">Distributed Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/ddp_under_the_hood.html">DDP: Under the Hood</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/dp_ddp.html">DP vs DDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/fsdp.html">FSDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/tensor_parallelism.html">Tensor parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/pipeline_parallelism.html">Pipeline Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/device_mesh.html">Device Mesh</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lld/index.html">Low Level Design</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><div class="visually-hidden">Toggle navigation of Low Level Design</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lld/parking_lot.html">Parking Lot</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../visualization/index.html">Data Visualization Projects</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/past_experiences/ecom_rag.md.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="rag-based-product-discovery">
<h1>RAG-Based Product Discovery<a class="headerlink" href="#rag-based-product-discovery" title="Permalink to this heading">¶</a></h1>
<section id="id1">
<h2><a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<hr class="docutils" />
<section id="tldr-from-clunky-search-to-conversational-commerce">
<h3><strong>TLDR: From Clunky Search to Conversational Commerce</strong><a class="headerlink" href="#tldr-from-clunky-search-to-conversational-commerce" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Challenge</strong></p>
<ul>
<li><p>For a mid-sized European e-commerce marketplace, the traditional keyword-based search was a significant source of user frustration and lost revenue. It failed to understand user intent, could not handle synonyms or multimodal queries (images), and led to high rates of “no results found,” causing customers to abandon their purchasing journey.</p></li>
</ul>
</li>
<li><p><strong>My Role &amp; Solution</strong></p>
<ul>
<li><p>As the lead <strong>ML/GenAI Engineer</strong>, I designed and implemented an end-to-end <strong>Multimodal Retrieval-Augmented Generation (RAG)</strong> system to transform the search bar into an intelligent, conversational shopping assistant. My contributions spanned the entire MLOps lifecycle: <br><br> <strong>1. Strategy &amp; Architecture:</strong> I architected a scalable, cost-effective solution on AWS, leveraging a modular, microservices-based design. <br> <strong>2. Data Ingestion &amp; Indexing:</strong> I built the automated pipelines using <strong>AWS Glue, Lambda, and Step Functions</strong> to process and index the multimodal product catalog (text, images, PDFs) into <strong>Amazon OpenSearch</strong>. <br> <strong>3. RAG Development &amp; Iteration:</strong> I drove the experimentation process, systematically progressing from a baseline RAG to implementing advanced strategies like <strong>Hybrid Search, Contextual Re-ranking, and Multimodal Retrieval</strong> using <strong>Amazon Bedrock (Claude 3, Titan Embeddings)</strong> and <strong>LangChain</strong>. <br> <strong>4. Continual Learning:</strong> I designed and built the <strong>“Data Flywheel” pipeline</strong> using <strong>Amazon SageMaker</strong> to periodically fine-tune the embedding model on real user interaction data, ensuring the system continuously improves. <br> <strong>5. Productionization &amp; Monitoring:</strong> I engineered the low-latency <strong>Inference Pipeline</strong> on <strong>AWS Fargate</strong>, established a comprehensive observability stack using <strong>LangSmith and CloudWatch</strong>, and implemented a rigorous <strong>A/B testing framework</strong> to validate business impact in production.</p></li>
</ul>
</li>
<li><p><strong>Impact</strong></p>
<ul>
<li><p>The project successfully transitioned the search experience from a liability to a strategic asset. By deploying this system, we achieved measurable improvements in key business metrics over six months of A/B testing: <br> - <strong>+12% Increase in Search-to-Purchase Conversion Rate:</strong> Users found relevant products more quickly and confidently. <br> - <strong>+7% Increase in Average Order Value (AOV):</strong> The conversational assistant effectively cross-sold and recommended relevant complementary products. <br> - <strong>-85% Reduction in “No Results Found” Rate:</strong> The system’s semantic and multimodal understanding dramatically improved query success. <br> - <strong>Maintained &lt;500ms p99 Latency:</strong> Ensured a fast and responsive user experience even at peak traffic.</p></li>
</ul>
</li>
</ul>
<p><strong>System Architecture</strong></p>
<p>The final architecture is a robust, event-driven system on AWS that balances real-time performance with offline continuous improvement. My core contributions are highlighted in green.</p>
<img src="../_static/past_experiences/ecom_rag/contributions.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<hr class="docutils" />
<section id="business-challenge-beyond-the-limitations-of-keyword-search">
<h3><strong>1. Business Challenge: Beyond the Limitations of Keyword Search</strong><a class="headerlink" href="#business-challenge-beyond-the-limitations-of-keyword-search" title="Permalink to this heading">¶</a></h3>
<p>For any e-commerce marketplace, the search bar is not merely a utility; it is the single most critical touchpoint for high-intent customers. Users who engage with search are responsible for a disproportionate share of revenue, yet the underlying technology of traditional keyword-based search is fundamentally broken. It operates on a brittle system of lexical matching, failing to comprehend the semantic nuance, context, and visual nature of user intent. This technological deficit is a primary driver of customer frustration, site abandonment, and direct revenue loss.</p>
<section id="the-limitations-of-traditional-keyword-search">
<h4><strong>The Limitations of Traditional Keyword Search</strong><a class="headerlink" href="#the-limitations-of-traditional-keyword-search" title="Permalink to this heading">¶</a></h4>
<p>Traditional search systems are plagued by several well-documented failure modes that create significant friction in the product discovery journey:</p>
<ul class="simple">
<li><p><strong>Lack of Semantic Understanding:</strong> The system cannot grasp synonyms or conceptual relationships. A search for “denim pants” will fail if products are tagged exclusively as “jeans,” and a query for “a dress for a summer wedding” is treated as a simple keyword match, returning a mix of unrelated items.</p></li>
<li><p><strong>Poor Error and Variation Tolerance:</strong> Simple misspellings or regional variations in terminology frequently lead to a “no results found” page, immediately halting the customer journey.</p></li>
<li><p><strong>Inability to Handle Complex Queries:</strong> Users often search with multiple attributes in mind (e.g., “waterproof trail running shoes for wide feet under $150”). Keyword systems struggle to handle this complexity, often returning zero results if no single product matches every specific term.</p></li>
<li><p><strong>Blindness to Visual Intent:</strong> E-commerce is an inherently visual experience. A customer may want to find a product that <em>looks like</em> one they’ve seen elsewhere. Traditional search is completely blind to this powerful, image-based intent.</p></li>
</ul>
</section>
<section id="the-tangible-business-impact">
<h4><strong>The Tangible Business Impact</strong><a class="headerlink" href="#the-tangible-business-impact" title="Permalink to this heading">¶</a></h4>
<p>These technical failings translate directly into significant negative business outcomes:</p>
<ul class="simple">
<li><p><strong>Direct Revenue Loss:</strong> Industry data reveals that a staggering 85% of e-commerce site searches fail to return the intended product. This leads to high bounce rates and abandoned carts, as frustrated users quickly leave for a competitor’s site.</p></li>
<li><p><strong>Degraded Customer Experience:</strong> A poor search experience erodes user trust and damages brand perception. When a search returns irrelevant results or a blank page, the customer loses confidence in the platform’s ability to meet their needs.</p></li>
<li><p><strong>High Operational Overhead:</strong> Maintaining a keyword-based search system is a constant, manual effort. Engineering and product teams are burdened with the endless task of managing synonym lists, redirect rules, and product tags—a slow, expensive, and ultimately scalable process that cannot keep pace with evolving product catalogs and consumer language.</p></li>
</ul>
</section>
<section id="project-goals-from-transactional-search-to-conversational-discovery">
<h4><strong>Project Goals: From Transactional Search to Conversational Discovery</strong><a class="headerlink" href="#project-goals-from-transactional-search-to-conversational-discovery" title="Permalink to this heading">¶</a></h4>
<p>The objective of this project was to address these challenges by fundamentally re-architecting the product discovery experience. The goal was to transition from a simple, transactional search box to an intelligent, conversational shopping assistant powered by Retrieval-Augmented Generation (RAG).</p>
<p>The primary goals were to:</p>
<ul class="simple">
<li><p><strong>Enable Semantic and Multimodal Understanding:</strong> Build a system that comprehends the user’s intent from natural language text and visual queries (images), retrieving products based on conceptual meaning rather than just keyword matches.</p></li>
<li><p><strong>Ensure Factual Grounding and Accuracy:</strong> Leverage a RAG architecture to ensure all generated responses and product recommendations are grounded in the factual, up-to-date information from the product catalog, eliminating hallucinations about product details.</p></li>
<li><p><strong>Reduce User Friction and Guide the Journey:</strong> Create an interactive, conversational interface that can answer specific product questions, offer personalized recommendations, and guide the user from a broad query to a confident purchase decision.</p></li>
<li><p><strong>Establish a Scalable and Automated MLOps Foundation:</strong> Build the necessary data ingestion, evaluation, and monitoring pipelines to ensure the system is reliable, cost-effective, and capable of continuous improvement.</p></li>
</ul>
</section>
<section id="measuring-success-tying-technology-to-business-value">
<h4><strong>Measuring Success: Tying Technology to Business Value</strong><a class="headerlink" href="#measuring-success-tying-technology-to-business-value" title="Permalink to this heading">¶</a></h4>
<p>To validate the project’s impact, a clear set of Key Performance Indicators (KPIs) was established. These metrics were designed to measure success not just in technical terms, but in direct relation to core business objectives.</p>
<section id="primary-business-kpis">
<h5><strong>Primary Business KPIs</strong><a class="headerlink" href="#primary-business-kpis" title="Permalink to this heading">¶</a></h5>
<p>These metrics are the ultimate measure of the project’s financial and strategic success.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>KPI</p></th>
<th class="head text-left"><p>Why It Matters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Search-to-Purchase Conversion Rate</strong></p></td>
<td class="text-left"><p>The single most important metric. It directly measures the percentage of search sessions that result in a completed purchase, proving the system’s effectiveness in driving revenue.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Average Order Value (AOV)</strong></p></td>
<td class="text-left"><p>A rising AOV indicates the conversational system is successfully cross-selling and up-selling by recommending relevant complementary products or higher-value alternatives.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Revenue Per Search User</strong></p></td>
<td class="text-left"><p>This KPI normalizes revenue by the number of users who engage with search, providing a clear measure of the feature’s direct financial contribution.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Reduction in “No Results” Rate</strong></p></td>
<td class="text-left"><p>A direct measure of the system’s improved ability to handle a wider range of user queries, indicating a significant reduction in a major point of user frustration.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="secondary-engagement-kpis">
<h5><strong>Secondary Engagement KPIs</strong><a class="headerlink" href="#secondary-engagement-kpis" title="Permalink to this heading">¶</a></h5>
<p>These leading indicators measure user interaction and satisfaction, providing an early signal of the system’s health and its impact on the customer experience.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>KPI</p></th>
<th class="head text-left"><p>Why It Matters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Search Engagement Rate</strong></p></td>
<td class="text-left"><p>The percentage of site visitors who choose to interact with the new RAG-based search. An increase indicates that the feature is discoverable, intuitive, and perceived as valuable.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Reduced Query Reformulation Rate</strong></p></td>
<td class="text-left"><p>Measures how often users need to rephrase or alter their search query. A lower rate is a strong signal that the system is understanding user intent correctly on the first attempt.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Explicit User Feedback Score</strong></p></td>
<td class="text-left"><p>The ratio of “thumbs up” to “thumbs down” on generated responses. This provides a direct, real-time pulse on user-perceived answer quality and satisfaction.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Click-Through Rate (CTR) on Cited Sources</strong></p></td>
<td class="text-left"><p>The percentage of responses where a user clicks on a linked product. This acts as a proxy for retrieval relevance and user trust in the provided information.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="problem-framing-from-business-need-to-a-measurable-ml-vision">
<h3><strong>2. Problem Framing: From Business Need to a Measurable ML Vision</strong><a class="headerlink" href="#problem-framing-from-business-need-to-a-measurable-ml-vision" title="Permalink to this heading">¶</a></h3>
<p>Building a successful Generative AI application does not begin with algorithms or infrastructure; it begins with a rigorous process of <strong>Problem Framing</strong>. This foundational stage translates a high-level business objective into a well-defined, feasible, and measurable machine learning task. A flawed framing can lead a project to deliver a technically impressive system that provides no real business value. This section outlines the structured process used to define the vision for the RAG-based search system.</p>
<section id="a-setting-the-business-objectives">
<h4><strong>A. Setting the Business Objectives</strong><a class="headerlink" href="#a-setting-the-business-objectives" title="Permalink to this heading">¶</a></h4>
<p>The project originated from a core business need: to transform the underperforming, keyword-based search bar into a revenue-driving, customer-centric product discovery engine. After aligning with key stakeholders—including Product Management, Sales, and Engineering—the primary business objectives were defined:</p>
<ul class="simple">
<li><p><strong>Increase Revenue:</strong> Directly tie the improved search experience to top-line growth by increasing conversion rates and average order value.</p></li>
<li><p><strong>Enhance Customer Satisfaction:</strong> Reduce the friction and frustration inherent in traditional search, leading to higher user engagement, longer session durations, and improved brand loyalty.</p></li>
<li><p><strong>Reduce Operational Overhead:</strong> Automate the process of understanding product relationships and user intent, eliminating the costly and scalable manual effort required to maintain keyword lists and synonym dictionaries.</p></li>
</ul>
</section>
<section id="b-is-rag-the-right-approach-genai-use-case-evaluation">
<h4><strong>B. Is RAG the Right Approach? (GenAI Use Case Evaluation)</strong><a class="headerlink" href="#b-is-rag-the-right-approach-genai-use-case-evaluation" title="Permalink to this heading">¶</a></h4>
<p>Before committing to a complex RAG architecture, a critical evaluation was performed to determine if Generative AI was the appropriate solution.</p>
<p><strong>When to Use RAG for E-commerce Search:</strong></p>
<ul class="simple">
<li><p><strong>Complex Patterns:</strong> User queries are expressed in natural language, full of nuance, context, and ambiguity that simple rule-based systems cannot handle. RAG is designed to interpret these complex linguistic patterns.</p></li>
<li><p><strong>Existing Knowledge Base:</strong> A rich, diverse knowledge base already exists in the form of product catalogs, detailed descriptions, customer reviews, and images. RAG excels at activating this latent knowledge.</p></li>
<li><p><strong>Need for Grounded Generation:</strong> The system must provide factually accurate information about products. RAG’s ability to ground LLM responses in retrieved, authoritative data is critical for preventing hallucinations and building user trust.</p></li>
<li><p><strong>Scalability and Evolving Data:</strong> The product catalog is constantly changing. A RAG system, supported by automated ingestion pipelines, can adapt to this evolving knowledge base far more effectively than a manually curated system.</p></li>
</ul>
<p><strong>When a Simpler Solution Might Suffice:</strong></p>
<ul class="simple">
<li><p>If the product catalog were small, static, and had a very simple query space, a simpler solution involving keyword search with a well-maintained synonym list might be adequate. However, given the scale, diversity, and dynamic nature of a modern e-commerce marketplace, this approach was deemed insufficient and unsustainable.</p></li>
</ul>
<p><strong>The Value Proposition of RAG-Powered Search</strong></p>
<p>The analysis concluded that RAG was the optimal approach. It enables a paradigm shift from a simple search box to a conversational shopping assistant that understands intent, answers specific questions, and guides users through their discovery journey. This creates a powerful <strong>data flywheel</strong>: as more users engage with the improved system, they generate richer interaction data (clicks, conversions, feedback), which can then be used to continuously fine-tune the retrieval and ranking models, creating a virtuous cycle of improvement.</p>
</section>
<section id="c-defining-the-ml-problem">
<h4><strong>C. Defining the ML Problem</strong><a class="headerlink" href="#c-defining-the-ml-problem" title="Permalink to this heading">¶</a></h4>
<p>The broad business objective was translated into a specific, technical machine learning problem.</p>
<ul class="simple">
<li><p><strong>Ideal Outcome:</strong> A user finds the product that best meets their functional needs and stylistic preferences, resulting in a confident purchase.</p></li>
<li><p><strong>System’s Goal:</strong> Given a multimodal user query (text and/or image), the system must:</p>
<ol class="arabic simple">
<li><p><strong>Retrieve</strong> a ranked list of the most relevant product information (descriptions, specifications, images, reviews) from the knowledge base.</p></li>
<li><p><strong>Generate</strong> a helpful, concise, and factually accurate natural language response that answers the user’s question and presents the most relevant products, citing its sources.</p></li>
</ol>
</li>
<li><p><strong>ML Task Formulation:</strong> This is a composite system that relies on several ML tasks:</p>
<ul>
<li><p><strong>Information Retrieval:</strong> At its core, this is a retrieval problem. The primary task is to rank documents based on their semantic relevance to a query.</p></li>
<li><p><strong>Text Generation:</strong> The final step involves conditional text generation by an LLM</p></li>
</ul>
</li>
<li><p><strong>The “Relevance” Proxy:</strong> The ideal outcome (“user satisfaction”) is not directly measurable as a label. Therefore, <strong>relevance</strong> is used as a proxy. This relevance is further approximated by measurable user behaviors, which serve as the implicit labels for training and evaluation:</p>
<ul>
<li><p><strong>Good Proxies:</strong> <code class="docutils literal notranslate"><span class="pre">Purchase</span></code>, <code class="docutils literal notranslate"><span class="pre">Add-to-Cart</span></code>. These are strong signals of relevance but are relatively sparse.</p></li>
<li><p><strong>Weaker Proxies:</strong> <code class="docutils literal notranslate"><span class="pre">Click</span></code>, <code class="docutils literal notranslate"><span class="pre">Dwell</span> <span class="pre">Time</span></code>. These are more abundant but can be noisy (e.g., a user might click on an irrelevant but eye-catching product).</p></li>
</ul>
</li>
</ul>
</section>
<section id="d-assessing-feasibility-risks">
<h4><strong>D. Assessing Feasibility &amp; Risks</strong><a class="headerlink" href="#d-assessing-feasibility-risks" title="Permalink to this heading">¶</a></h4>
<p>A thorough feasibility assessment was conducted to identify potential risks and ensure the project was achievable within the given constraints.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Category</p></th>
<th class="head text-left"><p>Assessment</p></th>
<th class="head text-left"><p>Notes &amp; Mitigation Strategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Data</strong></p></td>
<td class="text-left"><p><strong>Medium Risk</strong></p></td>
<td class="text-left"><p><strong>Risk:</strong> Inconsistent quality and structure across product descriptions and reviews. <strong>Mitigation:</strong> Invest heavily in a robust data cleaning and pre-processing pipeline. Start with a single, clean product category for the PoC.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Problem Difficulty</strong></p></td>
<td class="text-left"><p><strong>High Risk</strong></p></td>
<td class="text-left"><p><strong>Risk:</strong> Multimodal and multilingual RAG are technically complex. Achieving low latency at scale is a significant engineering challenge. <strong>Mitigation:</strong> Adopt a phased approach. Begin with text-only RAG, then incrementally add multimodal and multilingual capabilities after the core system is validated.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Prediction Quality</strong></p></td>
<td class="text-left"><p><strong>High Risk</strong></p></td>
<td class="text-left"><p><strong>Risk:</strong> The cost of errors is high. An irrelevant result is a lost sale. A hallucinated product feature erodes trust and can lead to returns. <strong>Mitigation:</strong> Prioritize groundedness and factual accuracy in model evaluation. Implement rigorous prompt engineering and output guardrails.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Technical Requirements</strong></p></td>
<td class="text-left"><p><strong>High Risk</strong></p></td>
<td class="text-left"><p><strong>Risk:</strong> Sub-second p99 latency is required for a good user experience. LLM API and vector database costs can scale unpredictably. <strong>Mitigation:</strong> Architect for performance from day one with aggressive caching. Implement strict cost monitoring and alerting. Use smaller, faster models where possible.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Ethics &amp; Fairness</strong></p></td>
<td class="text-left"><p><strong>Medium Risk</strong></p></td>
<td class="text-left"><p><strong>Risk:</strong> Potential for the model to amplify biases present in historical data (e.g., in product reviews or sales trends). <strong>Mitigation:</strong> Implement bias detection tools during evaluation. Ensure fairness is a key metric in re-ranker training. Provide transparency to users.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="e-defining-success-metrics">
<h4><strong>E. Defining Success Metrics</strong><a class="headerlink" href="#e-defining-success-metrics" title="Permalink to this heading">¶</a></h4>
<p>A multi-layered set of metrics was defined to provide a holistic view of the system’s performance, from its operational health to its direct impact on business goals.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Metric Type</p></th>
<th class="head text-left"><p>Metric Name</p></th>
<th class="head text-left"><p>How It’s Measured</p></th>
<th class="head text-left"><p>Why It Matters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Business (Primary)</strong></p></td>
<td class="text-left"><p><strong>Search-to-Purchase Conversion Rate</strong></p></td>
<td class="text-left"><p>A/B testing: (Purchases from Search) / (Total Search Sessions).</p></td>
<td class="text-left"><p>The ultimate measure of the system’s ability to drive revenue.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Business (Primary)</strong></p></td>
<td class="text-left"><p><strong>Average Order Value (AOV)</strong></p></td>
<td class="text-left"><p>A/B testing: (Total Revenue from Search) / (Number of Orders from Search).</p></td>
<td class="text-left"><p>Measures the effectiveness of cross-selling and up-selling within the conversational experience.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Model Evaluation (Retrieval)</strong></p></td>
<td class="text-left"><p><strong>Mean Reciprocal Rank (MRR)</strong></p></td>
<td class="text-left"><p>Offline evaluation against a golden dataset of (query, relevant_product) pairs.</p></td>
<td class="text-left"><p>Measures the quality of the ranking, ensuring the <em>best</em> result appears near the top.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Evaluation (Generation)</strong></p></td>
<td class="text-left"><p><strong>Groundedness / Faithfulness</strong></p></td>
<td class="text-left"><p>Offline evaluation using an LLM-as-a-judge to score if the generated answer is factually consistent with the retrieved context.</p></td>
<td class="text-left"><p>The most critical AI quality metric. Directly measures the system’s resistance to hallucination and builds user trust.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Operational (Online)</strong></p></td>
<td class="text-left"><p><strong>p99 End-to-End Latency</strong></p></td>
<td class="text-left"><p>Real-time monitoring of the API endpoint.</p></td>
<td class="text-left"><p>Ensures a fast and responsive user experience, which is critical for maintaining engagement and conversions.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Operational (Online)</strong></p></td>
<td class="text-left"><p><strong>Cost-per-Query</strong></p></td>
<td class="text-left"><p>Aggregating costs from all services (Vector DB, LLM API, Compute) involved in a single request.</p></td>
<td class="text-left"><p>Provides financial visibility and ensures the system remains economically viable at scale.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>User Engagement (Secondary)</strong></p></td>
<td class="text-left"><p><strong>User Feedback Ratio</strong></p></td>
<td class="text-left"><p>Tracking the ratio of “thumbs up” to “thumbs down” clicks on generated responses.</p></td>
<td class="text-left"><p>A direct, real-time signal of user-perceived quality and satisfaction.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="the-end-to-end-project-and-operational-blueprint">
<h3><strong>3. The End-to-End Project and Operational Blueprint</strong><a class="headerlink" href="#the-end-to-end-project-and-operational-blueprint" title="Permalink to this heading">¶</a></h3>
<section id="a-the-llmops-tech-stack-an-architectural-blueprint">
<h4>A. The LLMOps Tech Stack: An Architectural Blueprint<a class="headerlink" href="#a-the-llmops-tech-stack-an-architectural-blueprint" title="Permalink to this heading">¶</a></h4>
<p>Building a production-grade RAG system requires a deliberate and holistic approach to technology selection. The MLOps Stack Canvas provides a structured framework for architecting the end-to-end system, ensuring that every component—from data ingestion to production monitoring—is chosen to meet the specific demands of a Generative AI application.</p>
<p>The following stack was selected to prioritize managed services for scalability and reduced operational overhead, while leveraging specialized frameworks like LangChain for rapid development and LangSmith for essential LLM-specific observability.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Canvas Block</p></th>
<th class="head text-left"><p>Chosen Stack &amp; Rationale</p></th>
<th class="head text-left"><p>Key GenAI/RAG Considerations</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Value Proposition</strong></p></td>
<td class="text-left"><p><strong>Project Charter &amp; Design Docs</strong></p></td>
<td class="text-left"><p><strong>Goal Alignment:</strong> This block is the strategic “why.” It ensures all technical decisions directly align with the business objective: creating a conversational, multimodal product discovery engine to drive revenue and customer satisfaction.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Knowledge Base &amp; Data Versioning</strong></p></td>
<td class="text-left"><p><strong>Vector DB:</strong> Amazon OpenSearch (k-NN) <br> <strong>Data Lake:</strong> AWS S3 <br> <strong>Versioning:</strong> DVC</p></td>
<td class="text-left"><p><strong>The Heart of RAG:</strong> The Vector DB is the indexed, searchable knowledge base. OpenSearch is chosen for its scalability and hybrid search capabilities. S3 serves as the raw data lake for documents and images before they are processed and indexed. DVC versions the curated datasets used for model fine-tuning.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Experiment Management</strong></p></td>
<td class="text-left"><p><strong>Amazon SageMaker Studio</strong> (Notebooks) &amp; <strong>LangSmith</strong> (Tracing/Evaluation)</p></td>
<td class="text-left"><p><strong>Shift from Model to Pipeline:</strong> For RAG, the core “experiment” is less about model architecture and more about prompt engineering and pipeline configuration (e.g., chunking, retrieval). LangSmith is purpose-built for tracing and evaluating these complex LLM chains, making it superior to traditional tools for this use case.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Feature Store for Re-ranking</strong></p></td>
<td class="text-left"><p><strong>Amazon SageMaker Feature Store</strong> &amp; <strong>AWS Step Functions</strong> (Orchestration)</p></td>
<td class="text-left"><p><strong>Enrichment for Precision:</strong> The Feature Store serves structured, real-time features (e.g., <code class="docutils literal notranslate"><span class="pre">popularity</span></code>, <code class="docutils literal notranslate"><span class="pre">inventory_level</span></code>) for the <strong>re-ranking model</strong>. This is a classic ML feature serving pattern used to inject business logic into the final ranking, distinct from the unstructured retrieval of the Vector DB.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Foundations (DevOps &amp; Code)</strong></p></td>
<td class="text-left"><p><strong>Git (GitHub)</strong>, <strong>Terraform (IaC)</strong>, &amp; <strong>GitHub Actions (CI/CD)</strong></p></td>
<td class="text-left"><p><strong>Prompts as Code:</strong> A standard, best-practice DevOps foundation. A key adaptation for GenAI is treating <strong>prompt templates</strong> as critical, version-controlled code artifacts. Terraform ensures the entire cloud infrastructure is reproducible and auditable.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>6. Continuous Training &amp; Indexing</strong></p></td>
<td class="text-left"><p><strong>GitHub Actions</strong> triggering <strong>AWS Step Functions</strong></p></td>
<td class="text-left"><p><strong>Focus on Re-Indexing &amp; Fine-Tuning:</strong> The “training” pipeline here is twofold: 1) The frequent re-indexing of the Vector DB as the product catalog changes, and 2) The periodic fine-tuning of the embedding or re-ranking models. These are orchestrated, automated workflows.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>7. Model Registry &amp; Versioning</strong></p></td>
<td class="text-left"><p><strong>Amazon SageMaker Model Registry</strong></p></td>
<td class="text-left"><p><strong>Managing Custom Models:</strong> The registry is used to version and store the custom-trained models that we own—specifically, the fine-tuned embedding model and the contextual re-ranker. The foundational LLM is a managed API endpoint and is not stored here.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>8. Deployment &amp; Serving</strong></p></td>
<td class="text-left"><p><strong>Amazon Bedrock</strong> (LLM), <strong>SageMaker Endpoints</strong> (Custom Models), <strong>API Gateway</strong> + <strong>AWS Lambda</strong> (Inference Service)</p></td>
<td class="text-left"><p><strong>A Distributed System:</strong> The RAG serving layer is a distributed application. The Lambda function acts as the orchestrator, making real-time calls to SageMaker (re-ranking), <strong>OpenSearch (retrieval)</strong>, and Amazon Bedrock (generation). This modularity allows independent scaling.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>9. System &amp; Model Monitoring</strong></p></td>
<td class="text-left"><p><strong>LangSmith</strong> &amp; <strong>Amazon CloudWatch</strong></p></td>
<td class="text-left"><p><strong>LLM-Specific Observability:</strong> This is a critical adaptation from traditional MLOps. While CloudWatch monitors infrastructure health (latency, errors), <strong>LangSmith</strong> provides essential, LLM-specific observability by tracing queries, monitoring for hallucinations, tracking token usage, and collecting user feedback.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>10. Metadata &amp; Artifacts</strong></p></td>
<td class="text-left"><p><strong>LangSmith</strong>, <strong>OpenSearch Metadata</strong>, <strong>DynamoDB</strong></p></td>
<td class="text-left"><p><strong>Rich Metadata is Key:</strong> Metadata is mission-critical. It links vector chunks back to their source product pages for citation. It enables filtering in OpenSearch. It stores traces and feedback in LangSmith. A dedicated table in DynamoDB can track the lineage of datasets and model versions.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>11. Build vs. Buy</strong></p></td>
<td class="text-left"><p><strong>Hybrid: Buy Core, Build Differentiators</strong></p></td>
<td class="text-left"><p><strong>Buy:</strong> Leverage managed services for foundational components to accelerate development (Amazon Bedrock for LLMs, Amazon OpenSearch for vector search). <strong>Build:</strong> Focus engineering effort on the components that create a competitive advantage—the custom re-ranking model and the domain-specific ingestion logic.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>12. Platform &amp; Skills</strong></p></td>
<td class="text-left"><p><strong>Integrated AWS Platform</strong></p></td>
<td class="text-left"><p><strong>Unified Ecosystem:</strong> Choosing an integrated platform like AWS simplifies security, networking, and billing. It requires a cross-functional team with skills in data engineering, ML engineering, and data science, augmented by prompt engineering expertise.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="b-the-four-core-pipelines-an-operational-blueprint">
<h4>B. The Four Core Pipelines: An Operational Blueprint<a class="headerlink" href="#b-the-four-core-pipelines-an-operational-blueprint" title="Permalink to this heading">¶</a></h4>
<p>The end-to-end RAG system is powered by four distinct but interconnected pipelines. Two are offline processes focused on building and improving the system’s intelligence, while two are online, real-time processes that deliver the user experience and monitor its health.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Pipeline &amp; Cadence</p></th>
<th class="head text-left"><p>Trigger</p></th>
<th class="head text-left"><p>Inputs</p></th>
<th class="head text-left"><p>Key Steps</p></th>
<th class="head text-left"><p>Outputs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Data Ingestion &amp; Indexing Pipeline</strong> <br> <em>(Batch/Streaming)</em></p></td>
<td class="text-left"><p>- <strong>Scheduled:</strong> Nightly full-catalog refresh (via Amazon EventBridge). <br> - <strong>Event-Driven:</strong> Real-time updates for single product changes (e.g., price, inventory) triggered by database events.</p></td>
<td class="text-left"><p>- Raw product data (from PIM/DB). <br> - Unstructured text (descriptions, reviews). <br> - Product images.</p></td>
<td class="text-left"><p>1.  <strong>Extract:</strong> Pull data from all source systems. <br> 2.  <strong>Transform &amp; Clean:</strong> Sanitize text, handle missing values, and structure data into a unified JSON format per product. <br> 3.  <strong>Chunk:</strong> Break down long text documents into semantically coherent chunks. <br> 4.  <strong>Generate Embeddings:</strong> Create text embeddings (e.g., with Amazon Titan) for text chunks and image embeddings for product images. <br> 5.  <strong>Index Data:</strong> Load the embeddings and rich metadata (product_id, category, brand) into the Amazon OpenSearch vector index.</p></td>
<td class="text-left"><p>- A fully populated and up-to-date vector index in Amazon OpenSearch. <br> - Execution logs and data quality reports.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Inference Pipeline</strong> <br> <em>(Real-time)</em></p></td>
<td class="text-left"><p>- User query via the application’s API Gateway.</p></td>
<td class="text-left"><p>- User query (text and/or image). <br> - User context (e.g., user ID for personalization signals).</p></td>
<td class="text-left"><p>1.  <strong>Query Transformation:</strong> Apply techniques like HyDE to enhance the query for better retrieval. <br> 2.  <strong>Hybrid Retrieval:</strong> Perform parallel semantic (vector) and keyword search to get top-K candidate documents. <br> 3.  <strong>Contextual Re-ranking:</strong> Re-rank the candidates using a model that incorporates business logic (e.g., popularity, stock levels) and personalization. <br> 4.  <strong>Prompt Construction:</strong> Build a final, optimized prompt with the top-ranked context and system instructions. <br> 5.  <strong>LLM Generation:</strong> Call Amazon Bedrock to generate a response, streaming the result token-by-token. <br> 6.  <strong>Post-processing &amp; Guardrails:</strong> Apply safety filters and format the response with source citations.</p></td>
<td class="text-left"><p>- A streamed, factually grounded, and safe response delivered to the user. <br> - A detailed trace of the entire request (latency, token counts, retrieved docs) sent to the observability platform (LangSmith/CloudWatch).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Monitoring &amp; Observability Pipeline</strong> <br> <em>(Continuous)</em></p></td>
<td class="text-left"><p>- Continuous ingestion of data streams from all production services.</p></td>
<td class="text-left"><p>- System logs, metrics (latency, error rates), and traces from all services. <br> - LLM interaction data from LangSmith (prompts, responses, token usage, feedback). <br> - User feedback events (‘thumbs up/down’).</p></td>
<td class="text-left"><p>1.  <strong>Collect &amp; Aggregate:</strong> Gather data from CloudWatch, LangSmith, and application logs. <br> 2.  <strong>Process &amp; Analyze:</strong> Calculate key metrics (p99 latency, cost-per-query, groundedness scores). <br> 3.  <strong>Drift &amp; Anomaly Detection:</strong> Monitor for data drift in inputs and concept drift in model outputs. <br> 4.  <strong>Alerting:</strong> Trigger alerts (e.g., via PagerDuty/Slack) when KPIs or operational metrics breach pre-defined thresholds.</p></td>
<td class="text-left"><p>- Real-time dashboards (in CloudWatch/Grafana) showing system health, cost, and AI quality. <br> - Actionable alerts for the on-call engineering team. <br> - A curated dataset of logs and feedback to fuel the Continual Learning Pipeline.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Continual Learning / Data Flywheel Pipeline</strong> <br> <em>(Periodic)</em></p></td>
<td class="text-left"><p>- <strong>Scheduled:</strong> Weekly or bi-weekly execution. <br> - <strong>Manual:</strong> Triggered by an ML engineer after a significant product catalog update or observed performance degradation.</p></td>
<td class="text-left"><p>- Aggregated user interaction logs (queries, clicks, conversions) from the observability system. <br> - User-flagged poor responses and explicit feedback data.</p></td>
<td class="text-left"><p>1.  <strong>Dataset Creation:</strong> Process interaction logs to create new, high-quality training datasets of (query, relevant_product) pairs. <br> 2.  <strong>Model Retraining/Fine-tuning:</strong> Use the new dataset to fine-tune the embedding model or retrain the contextual re-ranking model. <br> 3.  <strong>Offline Evaluation:</strong> Rigorously evaluate the new candidate model against the current production model on offline metrics (MRR, NDCG). <br> 4.  <strong>Staging &amp; A/B Test Setup:</strong> If the candidate model is superior, register it and prepare a new A/B testing configuration for production deployment.</p></td>
<td class="text-left"><p>- A new, versioned, and evaluated candidate model in the Amazon SageMaker Model Registry. <br> - Evaluation reports comparing the candidate model to the production “champion.” <br> - A deployment configuration ready for a live A/B test.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="c-project-management-and-operational-strategy">
<h4><strong>C. Project Management and Operational Strategy</strong><a class="headerlink" href="#c-project-management-and-operational-strategy" title="Permalink to this heading">¶</a></h4>
<p>A successful GenAI project requires more than just a strong technical architecture; it demands a well-defined plan, a clear team structure, and a robust governance strategy. This blueprint outlines the phased approach to development, the roles and responsibilities of the cross-functional team, and the versioning strategy that ensures reproducibility and quality from experimentation to production.</p>
<section id="project-stages-an-iterative-path-to-production">
<h5><strong>1. Project Stages: An Iterative Path to Production</strong><a class="headerlink" href="#project-stages-an-iterative-path-to-production" title="Permalink to this heading">¶</a></h5>
<p>We adopted an agile, iterative methodology designed to deliver value quickly, manage risk, and incorporate learnings at each stage. The project is broken down into five distinct, overlapping phases.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Timeline</p></th>
<th class="head text-left"><p>Key Activities &amp; Objectives</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Ideation &amp; Planning</strong></p></td>
<td class="text-left"><p>Weeks 1-4</p></td>
<td class="text-left"><p>- <strong>Finalize Business Objectives:</strong> Solidify the KPIs and secure stakeholder alignment on the project’s goals. <br> - <strong>Define MVP Scope:</strong> Narrow the initial focus to a text-only, single-language RAG for one product category to ensure rapid validation. <br> - <strong>Architectural Blueprint:</strong> Finalize the core tech stack choices and create a high-level design for the MVP.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Experiment Management</strong></p></td>
<td class="text-left"><p>Weeks 5-12</p></td>
<td class="text-left"><p>- <strong>Establish Baseline:</strong> Build and rigorously evaluate a “naive” text-only RAG pipeline to establish performance benchmarks. <br> - <strong>Test Advanced RAG:</strong> Systematically experiment with and quantify the impact of Hybrid Search, Contextual Re-ranking, and Query Transformations (HyDE). <br> - <strong>Explore Multimodality:</strong> Develop a PoC for image-based retrieval using multimodal embeddings, updating ingestion and inference pipelines to handle visual queries. <br> - <strong>Validate Multilingual Support:</strong> Test multilingual embedding models and language-specific text processing to assess the feasibility and complexity of international expansion.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Pipelines Development</strong></p></td>
<td class="text-left"><p>Weeks 9-16</p></td>
<td class="text-left"><p>- <strong>Build Data Ingestion Pipeline:</strong> Engineer the robust, automated ETL pipeline in AWS, orchestrating it with AWS Step Functions. <br> - <strong>Engineer Inference Pipeline:</strong> Develop the production-grade, low-latency inference service using API Gateway and AWS Lambda. <br> - <strong>Implement MLOps Pipelines:</strong> Build the CI/CD workflows in GitHub Actions for automated testing and deployment, and script the periodic Continual Learning pipeline.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Deployment &amp; Serving</strong></p></td>
<td class="text-left"><p>Weeks 17-20</p></td>
<td class="text-left"><p>- <strong>Provision Production Infrastructure:</strong> Use Terraform to deploy the full, scalable production environment. <br> - <strong>Conduct Pre-flight Checks:</strong> Perform end-to-end integration tests and rigorous load testing against the production environment. <br> - <strong>MVP Rollout (A/B Test):</strong> Deploy the MVP to a small segment of live traffic (e.g., 5%) using an A/B testing framework to measure its real-world impact on primary business KPIs.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Monitoring &amp; Continual Learning</strong></p></td>
<td class="text-left"><p>Ongoing</p></td>
<td class="text-left"><p>- <strong>Activate Observability:</strong> Continuously monitor system health, cost, and AI quality dashboards (CloudWatch &amp; LangSmith). <br> - <strong>Establish On-Call Rotation:</strong> Respond to automated alerts for latency spikes, error rate increases, or significant model drift. <br> - <strong>Turn the Data Flywheel:</strong> Execute the Continual Learning pipeline on a regular schedule (e.g., bi-weekly) to retrain models with new user interaction data. <br> - <strong>Iterate and Enhance:</strong> Analyze user feedback and monitoring data to inform the roadmap for the next set of features and improvements.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="cross-functional-team-roles">
<h5><strong>2. Cross-Functional Team &amp; Roles</strong><a class="headerlink" href="#cross-functional-team-roles" title="Permalink to this heading">¶</a></h5>
<p>This project’s success hinges on the tight collaboration of a dedicated, cross-functional team.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Role</p></th>
<th class="head text-left"><p>Key Responsibilities</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Product Manager</strong></p></td>
<td class="text-left"><p>Owns the product vision, defines the roadmap, prioritizes features, and is ultimately responsible for the business KPIs.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data Engineer</strong></p></td>
<td class="text-left"><p>Designs, builds, and maintains the scalable Data Ingestion and Indexing pipeline. Ensures data quality, freshness, and reliability.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>ML/GenAI Engineer (My Role)</strong></p></td>
<td class="text-left"><p><strong>Leads the end-to-end technical implementation.</strong> Designs and builds the core RAG inference pipeline, implements advanced retrieval and re-ranking strategies, and engineers the MLOps workflows for CI/CD, monitoring, and continual learning.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data Scientist (My Role)</strong></p></td>
<td class="text-left"><p>Drives the experimentation and evaluation process. Selects, fine-tunes, and analyzes the performance of embedding and re-ranking models. Designs and interprets A/B tests.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Backend Engineer</strong></p></td>
<td class="text-left"><p>Develops the public-facing API endpoints, handles user authentication, and integrates the RAG service with the broader e-commerce platform.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Frontend Engineer</strong></p></td>
<td class="text-left"><p>Builds, tests, and iterates on the conversational user interface, focusing on creating an intuitive and responsive experience.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="versioning-and-governance-strategy">
<h5><strong>3. Versioning and Governance Strategy</strong><a class="headerlink" href="#versioning-and-governance-strategy" title="Permalink to this heading">¶</a></h5>
<p>A strict versioning and governance strategy is essential for ensuring reproducibility, quality, and maintainability.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Artifact</p></th>
<th class="head text-left"><p>Versioning Tool</p></th>
<th class="head text-left"><p>Governance Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Code</strong> (Application, Pipelines, Tests)</p></td>
<td class="text-left"><p><strong>Git (GitHub)</strong></p></td>
<td class="text-left"><p>All code changes must be submitted via a pull request (PR) and require at least one peer review and passing all CI checks before being merged. We follow a GitFlow branching model to manage features, releases, and hotfixes.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Infrastructure</strong></p></td>
<td class="text-left"><p><strong>Terraform &amp; Git</strong></p></td>
<td class="text-left"><p>The state of all cloud infrastructure is defined declaratively in Terraform. All infrastructure changes must be reviewed and applied through the CI/CD pipeline. No manual changes to the production environment are permitted.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Data</strong> (for Model Training)</p></td>
<td class="text-left"><p><strong>DVC (Data Version Control)</strong></p></td>
<td class="text-left"><p>The specific datasets used to fine-tune the embedding model or retrain the re-ranker are versioned with DVC. The DVC hash is logged with the model version, ensuring perfect reproducibility.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Prompts</strong></p></td>
<td class="text-left"><p><strong>Git</strong></p></td>
<td class="text-left"><p>All system prompt templates are treated as mission-critical code. They are stored in a dedicated repository, versioned in Git, and deployed through the CI/CD pipeline.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Models</strong> (Custom-trained)</p></td>
<td class="text-left"><p><strong>Amazon SageMaker Model Registry</strong></p></td>
<td class="text-left"><p>Every custom-trained model (embedding, re-ranker) is versioned and registered. The registry entry links the model artifact to its source code, training data version, and its offline evaluation metrics, providing a complete audit trail. A model cannot be deployed unless its evaluation metrics meet a pre-defined quality bar.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="d-comprehensive-evaluation-strategy-the-quality-gauntlet">
<h4><strong>D. Comprehensive Evaluation Strategy: The Quality Gauntlet</strong><a class="headerlink" href="#d-comprehensive-evaluation-strategy-the-quality-gauntlet" title="Permalink to this heading">¶</a></h4>
<p>A production-grade RAG system requires a multi-layered evaluation strategy that goes far beyond simple accuracy. Our approach is to treat testing not as a single stage, but as a continuous process embedded throughout the entire lifecycle. We will evaluate individual components (retrieval, generation), the integrated pipelines, and the live system’s impact on business KPIs.</p>
<section id="offline-evaluation-component-wise-and-pipeline-testing">
<h5>1. Offline Evaluation: Component-Wise and Pipeline Testing<a class="headerlink" href="#offline-evaluation-component-wise-and-pipeline-testing" title="Permalink to this heading">¶</a></h5>
<p>This is the “dress rehearsal” stage, conducted in development and staging environments before any code or model is exposed to users. Its purpose is to ensure each component works correctly and the integrated system is robust.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Test Category</p></th>
<th class="head text-left"><p>What We’re Testing</p></th>
<th class="head text-left"><p>Tools &amp; Methods</p></th>
<th class="head text-left"><p>Key Metrics &amp; Success Criteria</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Data &amp; Knowledge Base Validation</strong></p></td>
<td class="text-left"><p><strong>Quality of “Ingredients”:</strong> Ensuring the product data fed into the system is clean, complete, and correctly structured.</p></td>
<td class="text-left"><p>- <strong>Great Expectations:</strong> To define and validate schemas and data quality rules for product catalogs. <br> - <strong>Custom <code class="docutils literal notranslate"><span class="pre">pytest</span></code> checks:</strong> For validating the output of the PDF/URL parsers and text chunking logic.</p></td>
<td class="text-left"><p>- <strong>Schema Adherence:</strong> 100% compliance with the expected product data schema. <br> - <strong>Data Integrity:</strong> &lt;1% null values in critical fields like <code class="docutils literal notranslate"><span class="pre">product_title</span></code> and <code class="docutils literal notranslate"><span class="pre">description</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Retrieval Evaluation</strong></p></td>
<td class="text-left"><p><strong>Finding the Right Information:</strong> Does the system retrieve the most relevant product chunks for a given query?</p></td>
<td class="text-left"><p>- <strong>“Golden Dataset”:</strong> A manually curated set of 100-200 (query, relevant_product_id) pairs representing common and difficult search scenarios. <br> - <strong>Offline Metric Calculation:</strong> Using the golden dataset to test different retrieval strategies (Vector vs. Hybrid vs. HyDE).</p></td>
<td class="text-left"><p>- <strong>Hit Rate &#64; K=5 &gt; 95%:</strong> At least one correct product is found in the top 5 results. <br> - <strong>MRR &gt; 0.90:</strong> The first correct result is, on average, very close to the top position. <br> - <strong>NDCG&#64;10 &gt; 0.85:</strong> The overall ranking of the top 10 results is highly relevant.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Generation Evaluation</strong></p></td>
<td class="text-left"><p><strong>Generating a Trustworthy Answer:</strong> Is the LLM’s final response factually correct, relevant, and helpful?</p></td>
<td class="text-left"><p>- <strong>LLM-as-a-Judge:</strong> Using a powerful model (e.g., GPT-4o, Claude 3 Opus) to score the generated answers from our application’s LLM (e.g., Claude 3 Sonnet).</p></td>
<td class="text-left"><p>- <strong>Groundedness/Faithfulness &gt; 99%:</strong> The answer must be factually consistent with the retrieved context. This is non-negotiable. <br> - <strong>Answer Relevance &gt; 95%:</strong> The answer directly addresses the user’s query.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. End-to-End Pipeline Tests</strong></p></td>
<td class="text-left"><p><strong>Integration &amp; Robustness:</strong> Does the entire pipeline—from query ingestion to response generation—function correctly on a staging environment?</p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">pytest</span></code> Integration Tests:</strong> Simulate API calls to a staging endpoint and assert the correctness of the response structure. <br> - <strong>Locust Load Tests:</strong> Simulate realistic traffic patterns against the staging environment.</p></td>
<td class="text-left"><p>- <strong>100% Pass Rate</strong> on integration test suite. <br> - <strong>p99 Latency &lt; 1500ms</strong> under simulated peak load. <br> - <strong>Error Rate &lt; 0.1%</strong> during load test.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Guardrail &amp; Safety Tests</strong></p></td>
<td class="text-left"><p><strong>System Resilience &amp; Responsibility:</strong> Does the system handle inappropriate inputs and avoid generating harmful outputs?</p></td>
<td class="text-left"><p>- <strong>Adversarial Test Suite:</strong> A curated set of prompts designed to test for prompt injections, topical refusals, and toxic content generation.</p></td>
<td class="text-left"><p>- <strong>Refusal Rate &gt; 99%</strong> for out-of-scope topics (e.g., medical advice). <br> - <strong>0 Harmful Outputs:</strong> System must not generate toxic, biased, or unsafe content.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="online-evaluation-testing-in-production">
<h5>2. Online Evaluation: Testing in Production<a class="headerlink" href="#online-evaluation-testing-in-production" title="Permalink to this heading">¶</a></h5>
<p>The ultimate test of the system’s value is its performance with real users and its impact on the business. This is measured through carefully controlled online experiments.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Test Category</p></th>
<th class="head text-left"><p>What We’re Testing</p></th>
<th class="head text-left"><p>Tools &amp; Methods</p></th>
<th class="head text-left"><p>Key Metrics &amp; Success Criteria</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Business Impact (A/B Testing)</strong></p></td>
<td class="text-left"><p><strong>Driving Real Value:</strong> Does the new RAG system outperform the legacy keyword search on core business metrics?</p></td>
<td class="text-left"><p>- <strong>A/B Testing Framework:</strong> Randomly assigning users to either the legacy search (Control) or the new RAG search (Variant) and comparing their behavior over time.</p></td>
<td class="text-left"><p>- <strong>Primary KPI:</strong> Statistically significant <strong>increase in Search-to-Purchase Conversion Rate</strong> for the Variant group. <br> - <strong>Secondary KPIs:</strong> Statistically significant increases in AOV and Revenue Per Search User.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. User Satisfaction &amp; Engagement</strong></p></td>
<td class="text-left"><p><strong>Is the Experience Better?</strong> Are users engaging more with the new system and do they perceive it as higher quality?</p></td>
<td class="text-left"><p>- <strong>A/B Testing Framework</strong> <br> - <strong>Implicit Signals:</strong> Tracking user actions like query reformulation, clicks on product links, and session duration. <br> - <strong>Explicit Feedback:</strong> Analyzing the “thumbs up/down” ratio for generated responses.</p></td>
<td class="text-left"><p>- <strong>Reduced Query Reformulation Rate:</strong> Users find what they need on the first try. <br> - <strong>Higher Click-Through Rate (CTR)</strong> on recommended products. <br> - <strong>Positive Feedback Ratio &gt; 85%</strong>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Model Performance Monitoring (Shadow Testing)</strong></p></td>
<td class="text-left"><p><strong>Safe Model Rollouts:</strong> How does a new candidate model (e.g., a fine-tuned embedding model) perform on live traffic without impacting users?</p></td>
<td class="text-left"><p>- <strong>Shadow Deployment:</strong> Route a copy of live production traffic to the new candidate model in the background. The user sees the response from the current production model, but the candidate model’s response is logged for analysis.</p></td>
<td class="text-left"><p>- <strong>Offline Metric Comparison:</strong> Compare the candidate’s retrieval metrics (Hit Rate, MRR) against the production model on the same set of live queries. <br> - <strong>Latency Check:</strong> Ensure the candidate model meets latency SLOs before considering a promotion to an A/B test.</p></td>
</tr>
</tbody>
</table>
</div>
<p>This comprehensive, two-pronged evaluation strategy ensures that we are not only building a system that is technically correct and robust (Offline Evaluation) but also one that is demonstrably effective and valuable in the real world (Online Evaluation). It provides a clear, data-driven framework for making decisions about which models to promote, which features to prioritize, and how to continuously improve the system over time.</p>
<hr class="docutils" />
<!--
### **4. Data Sourcing, Discovery, and Analysis**

The foundation of any high-performing RAG system is a deep understanding of its data ecosystem. This involves not only identifying the available data sources but also characterizing their volume, velocity, and inherent complexities. This analysis directly informs the architectural decisions for the ingestion pipelines, the choice of models, and the strategies required for governance and compliance.

#### **A. Data Characteristics**

For a mid-sized European e-commerce marketplace with approximately 75,000 active SKUs and 250,000 daily active users, the data landscape is diverse and dynamic. The following table characterizes the key data sources that fuel the RAG system and the broader MLOps lifecycle.

| Data Source | Description | Volume, Velocity & Profile | Role in RAG & Governance Notes |
| :--- | :--- | :--- | :--- |
| **Product Catalog** | Structured data from the Product Information Management (PIM) system, including SKU, category, brand, price, and stock levels. | **Volume:** ~75,000 active SKUs. <br> **Velocity:** Low. ~500-1,000 new products added per month. <br> **Profile:** Highly structured, single source of truth. | **Role:** Provides the essential **metadata for filtering** in retrieval and for linking all other data back to a specific product. <br> **Governance:** Data consistency is paramount. |
| **Product Descriptions & Documents** | Unstructured and semi-structured text, including detailed marketing copy, specifications, and usage guides, often sourced from PDFs or vendor portals. | **Volume:** ~75,000+ documents. ~1-5 GB of raw text. <br> **Velocity:** Low. Updated when products are added or descriptions are revised. <br> **Profile:** Multilingual (primarily English, German, French). Varies from structured specs to evocative prose. | **Role:** The **primary textual knowledge base** for the RAG system. This content is chunked, embedded, and indexed for semantic retrieval. <br> **Governance:** Requires a robust parsing and cleaning pipeline. |
| **Product Images** | High-resolution product photos from various angles and in lifestyle contexts. | **Volume:** ~300,000+ images (avg. 4 per SKU). ~100-200 GB of image data. <br> **Velocity:** Low. Added with new products. <br> **Profile:** High-quality JPEGs/PNGs. | **Role:** The **core visual knowledge base** for multimodal search. Enables image-based queries and visual similarity search. <br> **Governance:** Must be correctly mapped to the corresponding SKU. |
| **Customer Reviews** | User-generated unstructured text providing real-world feedback, questions, and usage context. | **Volume:** ~4 million total reviews. <br> **Velocity:** Medium. ~1,000-2,000 new reviews per day. <br> **Profile:** Multilingual, grammatically diverse. Highly skewed sentiment distribution (~60% 5-star). | **Role:** A **secondary textual knowledge base**. Enriches the RAG system with authentic user language and long-tail information not found in official descriptions. <br> **Governance:** Requires moderation for toxic content and PII. |
| **Behavioral (Clickstream) Data** | High-velocity event streams tracking all user interactions on the platform. | **Volume:** High. ~5-10 million events/day. ~4-8 GB/day. <br> **Velocity:** High-velocity event streams ingested in near real-time via **AWS Kinesis**. <br> **Profile:** Semi-structured JSON events (`page_view`, `search`, `add_to_cart`). | **Role:** **The fuel for the Data Flywheel.** This data is *not* directly searched by RAG but is used to train the **contextual re-ranking model** to optimize for business outcomes. <br> **Governance:** Requires explicit user consent under GDPR. |
| **Transactional & CRM Data** | Order history, payment status, and customer profile information (e.g., purchase history, country). | **Volume:** Medium. ~4k-6k orders/day. <br> **Velocity:** Near real-time events, ingested in daily batches via **Airflow**. <br> **Profile:** Highly structured and sensitive, containing PII. | **Role:** Provides critical features for the **re-ranking model** (e.g., `is_verified_purchaser`, `lifetime_value`). <br> **Governance:** Strict GDPR rules apply. PII must be masked. Must support the "right to be forgotten." |

-->
</section>
</section>
</section>
<section id="data-ingestion-and-indexing-pipeline-building-the-knowledge-base">
<h3><strong>4. Data Ingestion and Indexing Pipeline: Building the Knowledge Base</strong><a class="headerlink" href="#data-ingestion-and-indexing-pipeline-building-the-knowledge-base" title="Permalink to this heading">¶</a></h3>
<p>The intelligence of our RAG system is entirely dependent on the quality, freshness, and structure of its knowledge base. The Data Ingestion and Indexing pipeline is the factory that constructs this knowledge base. It is an automated, multi-stage workflow designed to transform raw, heterogeneous product data into a highly searchable, semantically rich index.</p>
<p>The following table details each stage of this foundational pipeline, outlining the specific operations, the chosen tools, and the rationale behind each decision, with a special focus on the requirements of a Generative AI system.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Operation Details</p></th>
<th class="head text-left"><p>Tools</p></th>
<th class="head text-left"><p>Rationale &amp; LLM-Specific Focus</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Source Extraction</strong></p></td>
<td class="text-left"><p><strong>Extract Heterogeneous Data:</strong> Pull data from all source systems. This includes batch extraction for nightly full-catalog refreshes and event-driven extraction for real-time updates (e.g., price changes, new reviews).</p></td>
<td class="text-left"><p>- <strong>AWS Glue</strong> (for batch) <br> - <strong>AWS Lambda</strong> + <strong>Amazon SQS</strong> (for real-time events)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> Glue is optimized for large-scale, scheduled ETL jobs. The Lambda/SQS pattern provides a robust, serverless architecture for low-latency, incremental updates, ensuring the knowledge base remains fresh.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Multimodal Pre-processing</strong></p></td>
<td class="text-left"><p><strong>Clean &amp; Standardize:</strong> <br> - <strong>Text:</strong> Sanitize by removing HTML tags and special characters. Normalize text (lowercase, handle unicode). <br> - <strong>Tables (from PDFs):</strong> Extract structured data from tables within product manuals and spec sheets. <br> - <strong>Images:</strong> Standardize format (JPEG), resize for consistency, and perform initial quality checks.</p></td>
<td class="text-left"><p>- <strong>Unstructured.io / PyMuPDF</strong> <br> - <strong>BeautifulSoup</strong> <br> - Python libraries (<code class="docutils literal notranslate"><span class="pre">re</span></code>, Pillow)</p></td>
<td class="text-left"><p><strong>LLM-Specific Focus:</strong> The quality of the final generated answer is directly capped by the quality of the input text. This “Garbage In, Garbage Out” principle is paramount. Clean, well-parsed text leads to more meaningful and accurate embeddings.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Content Structuring &amp; Chunking</strong></p></td>
<td class="text-left"><p><strong>Prepare Data for Embedding:</strong> <br> - <strong>Unify:</strong> Consolidate all data for a single product into a unified JSON structure. <br> - <strong>Chunk Text:</strong> Split long descriptions and reviews into smaller, semantically coherent chunks with overlap. <br> - <strong>Serialize Tables:</strong> Convert extracted tables into a Markdown format that is both human-readable and easily understood by an LLM. <br> - <strong>Generate Image Captions:</strong> Use a VLM to create descriptive text captions for each product image to enable text-based retrieval of visual information.</p></td>
<td class="text-left"><p>- <strong>LangChain</strong> (<code class="docutils literal notranslate"><span class="pre">RecursiveCharacterTextSplitter</span></code>) <br> - <strong>Amazon Bedrock</strong> (Claude 3 for Image Captioning)</p></td>
<td class="text-left"><p><strong>LLM-Specific Focus:</strong> Chunking is a critical step to fit content within the LLM’s context window and improve retrieval precision. Image captioning is a core strategy for enabling multimodal search by “flattening” visual information into a text-based index.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Embedding Generation</strong></p></td>
<td class="text-left"><p><strong>Convert Content to Vectors:</strong> <br> - <strong>Text Embeddings:</strong> Generate dense vector representations for all text chunks and serialized table descriptions. <br> - <strong>Image Embeddings:</strong> Generate dense vector representations for all product images to enable visual similarity search.</p></td>
<td class="text-left"><p>- <strong>Amazon Bedrock</strong> (Titan Text &amp; Titan Multimodal Embedding Models)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> This is the core step that enables semantic search. Using managed models like Amazon Titan simplifies operations and provides state-of-the-art performance. The use of both text and multimodal models is key to our hybrid, multimodal strategy.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Validation &amp; Indexing</strong></p></td>
<td class="text-left"><p><strong>Quality Gate &amp; Loading:</strong> <br> - <strong>Validate Data:</strong> Before indexing, run automated quality checks on the structured, chunked, and embedded data to ensure integrity. <br> - <strong>Index Embeddings &amp; Metadata:</strong> Load the vector embeddings and their rich metadata (product_id, category, brand, image_url) into the vector database.</p></td>
<td class="text-left"><p>- <strong>Great Expectations</strong> <br> - <strong>Amazon OpenSearch Service</strong> (with k-NN index)</p></td>
<td class="text-left"><p><strong>Rationale:</strong> The validation step acts as a critical quality gate, preventing corrupt data from polluting our production knowledge base. OpenSearch is chosen for its scalability, managed nature, and robust support for hybrid (vector + keyword) search.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>6. Orchestration &amp; Governance</strong></p></td>
<td class="text-left"><p><strong>Manage the End-to-End Flow:</strong> <br> - <strong>Orchestrate:</strong> Manage dependencies, retries, and error handling for the entire multi-step pipeline. <br> - <strong>Version Artifacts:</strong> Version the source data (via DVC) and the pipeline definition code (via Git) to ensure reproducibility and enable rollbacks.</p></td>
<td class="text-left"><p>- <strong>AWS Step Functions</strong> <br> - <strong>Git &amp; DVC</strong></p></td>
<td class="text-left"><p><strong>LLM-Specific Focus:</strong> A complex, multi-step pipeline like this requires a robust orchestrator to be reliable. Step Functions provides excellent visibility and error handling. Versioning is critical for debugging issues like performance degradation that may arise from subtle changes in the data or processing logic.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="architecture-diagram-data-ingestion-and-indexing-pipeline">
<h4><strong>Architecture Diagram: Data Ingestion and Indexing Pipeline</strong><a class="headerlink" href="#architecture-diagram-data-ingestion-and-indexing-pipeline" title="Permalink to this heading">¶</a></h4>
<p>The following diagram illustrates the AWS architecture for the orchestrated batch ingestion pipeline.</p>
<img src="../_static/past_experiences/ecom_rag/pipeline_ingestion.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
</section>
<hr class="docutils" />
<section id="experiment-management-iteration-the-path-to-precision">
<h3><strong>5. Experiment Management &amp; Iteration: The Path to Precision</strong><a class="headerlink" href="#experiment-management-iteration-the-path-to-precision" title="Permalink to this heading">¶</a></h3>
<p>Building a production-grade RAG system is an iterative science. The journey from a basic keyword search to a sophisticated, multimodal, and multilingual conversational engine was guided by a systematic series of experiments. Each stage was designed to test a specific hypothesis, measure its impact against a consistent set of evaluation metrics, and build upon the learnings of the previous stage.</p>
<section id="a-the-evaluation-framework-our-north-star">
<h4><strong>A. The Evaluation Framework: Our North Star</strong><a class="headerlink" href="#a-the-evaluation-framework-our-north-star" title="Permalink to this heading">¶</a></h4>
<p>Before the first experiment, we established a rigorous, multi-faceted evaluation framework to ensure we were measuring what truly matters.</p>
<ul class="simple">
<li><p><strong>Retrieval Quality (Automated):</strong> Measured on our internal “golden dataset” of query-product pairs.</p>
<ul>
<li><p><strong>Tools:</strong> The open-source <code class="docutils literal notranslate"><span class="pre">Ragas</span></code> framework, integrated into our evaluation pipeline.</p></li>
<li><p><strong>Primary Metrics:</strong> <code class="docutils literal notranslate"><span class="pre">Context</span> <span class="pre">Precision</span></code> (is retrieved information relevant?) and <code class="docutils literal notranslate"><span class="pre">Context</span> <span class="pre">Recall</span></code> (is <em>all</em> relevant information retrieved?). <strong>Mean Reciprocal Rank (MRR)</strong> was our single primary metric for ranking quality.</p></li>
</ul>
</li>
<li><p><strong>Generation Quality (Automated + Human):</strong></p>
<ul>
<li><p><strong>Tools:</strong> <code class="docutils literal notranslate"><span class="pre">Ragas</span></code> and an <strong>LLM-as-a-Judge</strong> pattern using Claude 3 Opus for its superior reasoning.</p></li>
<li><p><strong>Primary Metrics:</strong> <code class="docutils literal notranslate"><span class="pre">Faithfulness</span></code> (is the answer grounded in the context?) was a non-negotiable, automated check. Qualitative metrics like <code class="docutils literal notranslate"><span class="pre">Conciseness</span></code> and <code class="docutils literal notranslate"><span class="pre">Helpfulness</span></code> were scored by the LLM judge.</p></li>
</ul>
</li>
<li><p><strong>Business Impact (Live):</strong></p>
<ul>
<li><p><strong>Tool:</strong> The company’s internal A/B testing framework.</p></li>
<li><p><strong>Primary Metric:</strong> <strong>Search-to-Purchase Conversion Rate</strong>. No change was promoted to production unless it demonstrated a neutral or statistically significant positive impact on this metric.</p></li>
</ul>
</li>
</ul>
</section>
<section id="b-building-the-golden-dataset-synthetic-data-generation-for-rag-evaluation">
<h4>B. Building the “Golden Dataset”: Synthetic Data Generation for RAG Evaluation<a class="headerlink" href="#b-building-the-golden-dataset-synthetic-data-generation-for-rag-evaluation" title="Permalink to this heading">¶</a></h4>
<p>To rigorously benchmark our RAG experiments and ensure our improvements were statistically meaningful, we needed a large, high-quality evaluation dataset. Manually creating tens of thousands of (query, relevant product) pairs was infeasible due to the time and cost involved. Instead, we engineered a scalable pipeline to generate a <strong>synthetic “golden dataset”</strong> using a powerful LLM, a strategy that has become a cornerstone of modern RAG evaluation.</p>
<p>This approach allowed us to create a comprehensive testbed that accurately reflected real-world user behavior while covering the breadth of our product catalog.</p>
<section id="the-challenge-the-evaluation-bottleneck">
<h5><strong>The Challenge: The Evaluation Bottleneck</strong><a class="headerlink" href="#the-challenge-the-evaluation-bottleneck" title="Permalink to this heading">¶</a></h5>
<p>A small, manually curated set of 100-200 queries is sufficient for initial spot-checking, but it’s not large enough to reliably detect small-to-medium performance regressions or validate the impact of advanced RAG strategies across a diverse product catalog. We needed a dataset with tens of thousands of examples to have high statistical confidence in our results.</p>
</section>
<section id="our-four-step-synthetic-generation-pipeline">
<h5><strong>Our Four-Step Synthetic Generation Pipeline</strong><a class="headerlink" href="#our-four-step-synthetic-generation-pipeline" title="Permalink to this heading">¶</a></h5>
<p>We designed and implemented an automated pipeline to generate this dataset, grounding the synthetic data in the reality of our actual users and products.</p>
<p><strong>Step 1: Curation of the Seed Query Set</strong></p>
<p>The quality of synthetic data is highly dependent on the quality of the initial examples. To ensure our generated queries were realistic, we did not start from scratch.</p>
<ul class="simple">
<li><p><strong>Action:</strong> We mined our production logs for a month’s worth of anonymized user search queries.</p></li>
<li><p><strong>Process:</strong> From this raw log, we curated a “seed set” of ~2,000 unique, high-quality queries. This curation involved cleaning, deduplicating, and, most importantly, <strong>categorizing</strong> the queries into distinct user intents:</p>
<ul>
<li><p><strong>Attribute-based:</strong> “waterproof hiking boots with ankle support”</p></li>
<li><p><strong>Comparative:</strong> “compare sony wh-1000xm5 vs bose qc ultra”</p></li>
<li><p><strong>Conceptual/Use-Case:</strong> “best camera for travel vlogging”</p></li>
<li><p><strong>Question-based:</strong> “is the iphone 15 compatible with a qi charger?”</p></li>
</ul>
</li>
<li><p><strong>Purpose:</strong> This categorized seed set served as a powerful “style guide” for the LLM, ensuring it generated a diverse and realistic mix of query types.</p></li>
</ul>
<p><strong>Step 2: Document Chunking for Focused Context</strong></p>
<p>To prompt the LLM effectively, we needed to provide it with focused, contextually rich pieces of product information.</p>
<ul class="simple">
<li><p><strong>Action:</strong> We processed each product description document from our catalog through the same <code class="docutils literal notranslate"><span class="pre">SemanticChunker</span></code> used in our production ingestion pipeline.</p></li>
<li><p><strong>Purpose:</strong> This ensured that the LLM was generating questions based on the exact same chunks of text that our retriever would be searching over. This alignment between the generation context and the retrieval target is critical for creating a fair and accurate evaluation set.</p></li>
</ul>
<p><strong>Step 3: The LLM-Powered Generation Engine</strong></p>
<p>This is the core of the pipeline where the synthetic queries are created. We used a powerful reasoning model (Claude 3 Opus) and a carefully engineered prompt.</p>
<ul class="simple">
<li><p><strong>Action:</strong> For each text chunk from every product, we invoked the LLM with a prompt designed to generate 5-10 realistic queries that could be answered by that specific chunk.</p></li>
<li><p><strong>The Prompt:</strong></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">You</span> <span class="n">are</span> <span class="n">an</span> <span class="n">expert</span> <span class="n">e</span><span class="o">-</span><span class="n">commerce</span> <span class="n">data</span> <span class="n">scientist</span> <span class="n">tasked</span> <span class="k">with</span> <span class="n">creating</span> <span class="n">a</span> <span class="n">high</span><span class="o">-</span><span class="n">quality</span> <span class="n">evaluation</span> <span class="n">dataset</span> <span class="k">for</span> <span class="n">a</span> <span class="n">new</span> <span class="n">semantic</span> <span class="n">search</span> <span class="n">engine</span><span class="o">.</span> <span class="n">Your</span> <span class="n">goal</span> <span class="ow">is</span> <span class="n">to</span> <span class="n">generate</span> <span class="n">realistic</span> <span class="n">user</span> <span class="n">search</span> <span class="n">queries</span> <span class="n">that</span> <span class="n">can</span> <span class="n">be</span> <span class="n">answered</span> <span class="n">by</span> <span class="n">the</span> <span class="n">provided</span> <span class="n">text</span> <span class="n">snippet</span> <span class="kn">from</span> <span class="nn">a</span> <span class="n">product</span> <span class="n">description</span><span class="o">.</span>

<span class="o">**</span><span class="n">CONTEXT</span> <span class="p">(</span><span class="n">Product</span> <span class="n">Information</span> <span class="n">Snippet</span><span class="p">):</span><span class="o">**</span>
<span class="o">---</span>
<span class="p">{</span><span class="n">chunk_of_product_text</span><span class="p">}</span>
<span class="o">---</span>

<span class="o">**</span><span class="n">INSTRUCTIONS</span><span class="p">:</span><span class="o">**</span>
<span class="mf">1.</span>  <span class="n">Read</span> <span class="n">the</span> <span class="n">context</span> <span class="n">carefully</span><span class="o">.</span>
<span class="mf">2.</span>  <span class="n">Generate</span> <span class="n">a</span> <span class="n">JSON</span> <span class="nb">list</span> <span class="n">of</span> <span class="mi">5</span> <span class="n">to</span> <span class="mi">10</span> <span class="n">diverse</span><span class="p">,</span> <span class="n">realistic</span> <span class="n">user</span> <span class="n">queries</span> <span class="n">that</span> <span class="n">a</span> <span class="n">real</span> <span class="n">shopper</span> <span class="n">might</span> <span class="nb">type</span><span class="o">.</span>
<span class="mf">3.</span>  <span class="n">The</span> <span class="n">answer</span> <span class="n">to</span> <span class="n">each</span> <span class="n">query</span> <span class="n">you</span> <span class="n">generate</span> <span class="n">MUST</span> <span class="n">be</span> <span class="n">present</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">provided</span> <span class="n">context</span><span class="o">.</span> <span class="n">Do</span> <span class="n">NOT</span> <span class="n">generate</span> <span class="n">questions</span> <span class="n">that</span> <span class="n">require</span> <span class="n">outside</span> <span class="n">knowledge</span><span class="o">.</span>
<span class="mf">4.</span>  <span class="n">Mimic</span> <span class="n">the</span> <span class="n">style</span> <span class="ow">and</span> <span class="n">intent</span> <span class="n">of</span> <span class="n">the</span> <span class="n">following</span> <span class="n">examples</span><span class="o">.</span> <span class="n">Include</span> <span class="n">a</span> <span class="n">mix</span> <span class="n">of</span> <span class="n">different</span> <span class="n">query</span> <span class="n">types</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span> <span class="n">questions</span><span class="p">,</span> <span class="n">comparisons</span><span class="p">,</span> <span class="n">feature</span> <span class="n">requests</span><span class="p">)</span><span class="o">.</span>

<span class="o">**</span><span class="n">EXAMPLES</span> <span class="n">of</span> <span class="n">QUERY</span> <span class="n">STYLES</span><span class="p">:</span><span class="o">**</span>
<span class="o">-</span> <span class="s2">&quot;compare product A vs product B&quot;</span>
<span class="o">-</span> <span class="s2">&quot;does this laptop have a backlit keyboard?&quot;</span>
<span class="o">-</span> <span class="s2">&quot;trail running shoes for rocky terrain&quot;</span>
<span class="o">-</span> <span class="s2">&quot;lightweight tent for backpacking in summer&quot;</span>

<span class="o">**</span><span class="n">OUTPUT</span> <span class="p">(</span><span class="n">JSON</span> <span class="nb">format</span> <span class="n">only</span><span class="p">):</span><span class="o">**</span>
</pre></div>
</div>
<p><strong>Step 4: Automated Validation and Final Assembly</strong></p>
<p>We did not blindly trust the LLM’s output. A final, lightweight validation step was crucial to ensure the quality and utility of the dataset.</p>
<ul class="simple">
<li><p><strong>Action:</strong></p>
<ol class="arabic simple">
<li><p>We programmatically checked that the output was valid JSON.</p></li>
<li><p>We implemented a simple heuristic check to ensure the generated query had a reasonable term overlap with the source chunk, filtering out any queries that seemed to have drifted off-topic.</p></li>
<li><p>Each validated query was stored, creating a final tuple: <code class="docutils literal notranslate"><span class="pre">(generated_query,</span> <span class="pre">source_product_id,</span> <span class="pre">source_chunk_id)</span></code>.</p></li>
</ol>
</li>
<li><p><strong>Outcome:</strong> This pipeline ran over our entire catalog and produced an evaluation dataset of <strong>over 50,000 high-quality (query, relevant_document_id) pairs</strong>.</p></li>
</ul>
</section>
<section id="impact-on-the-project">
<h5><strong>Impact on the Project</strong><a class="headerlink" href="#impact-on-the-project" title="Permalink to this heading">¶</a></h5>
<p>This investment in a high-quality synthetic dataset was a strategic force multiplier. It provided us with a scalable and reliable benchmark that underpinned all subsequent experimentation. It allowed us to:</p>
<ul class="simple">
<li><p><strong>Quantify Progress:</strong> We could confidently measure the impact of each RAG strategy on our primary retrieval metric (MRR).</p></li>
<li><p><strong>Automate Regression Testing:</strong> This dataset became a core part of our CI/CD pipeline, allowing us to automatically detect any performance regressions in our retrieval system before they reached production.</p></li>
<li><p><strong>Accelerate Iteration:</strong> It enabled rapid, data-driven decisions about which strategies to pursue and which to discard, dramatically speeding up our development cycle.</p></li>
</ul>
</section>
</section>
<section id="how-the-golden-dataset-is-used-to-calculate-mrr">
<h4><strong>How the Golden Dataset is Used to Calculate MRR</strong><a class="headerlink" href="#how-the-golden-dataset-is-used-to-calculate-mrr" title="Permalink to this heading">¶</a></h4>
<p><strong>MRR requires a ranking of products for each query.</strong> Here is the step-by-step process of how we generate that ranking and calculate the metric, which should clarify the entire workflow:</p>
<p><strong>The Evaluation Process:</strong></p>
<ol class="arabic simple">
<li><p><strong>Take One Query from the Golden Dataset:</strong></p>
<ul class="simple">
<li><p>Let’s take a query <code class="docutils literal notranslate"><span class="pre">q1</span></code> from our Golden Dataset.</p></li>
<li><p>From the dataset, we know the ground truth “correct” answer is <code class="docutils literal notranslate"><span class="pre">product_id</span> <span class="pre">=</span> <span class="pre">'prod_123'</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Run the Query Through the System Under Test:</strong></p>
<ul class="simple">
<li><p>We take the text of <code class="docutils literal notranslate"><span class="pre">q1</span></code> and feed it into our <strong>entire retrieval system</strong> (e.g., the “Hybrid Search + Re-ranker” system we are evaluating).</p></li>
<li><p>The system does its job: it generates an embedding for the query, searches the OpenSearch index, gets a list of candidates, re-ranks them, and returns a <strong>final, ranked list of product IDs</strong>.</p></li>
</ul>
</li>
<li><p><strong>Get the Model-Generated Ranking:</strong></p>
<ul class="simple">
<li><p>The output from our system might look like this ranked list of the top 10 results:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prod_789</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prod_456</span></code></p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">prod_123</span></code></strong>  &lt;– Our correct answer is here!</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prod_999</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prod_555</span></code>
… and so on.</p></li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Calculate the Reciprocal Rank for This Query:</strong></p>
<ul class="simple">
<li><p>We look for our ground truth answer (<code class="docutils literal notranslate"><span class="pre">prod_123</span></code>) in the model-generated ranking.</p></li>
<li><p>We find it at <strong>rank = 3</strong>.</p></li>
<li><p>The Reciprocal Rank (RR) for query <code class="docutils literal notranslate"><span class="pre">q1</span></code> is therefore <strong>1 / 3 = 0.33</strong>.</p></li>
<li><p>(If the correct answer had been at rank 1, the RR would be 1/1 = 1. If it was not found in the top 10, the RR would be 0 for this query).</p></li>
</ul>
</li>
<li><p><strong>Repeat for All Queries:</strong></p>
<ul class="simple">
<li><p>We repeat steps 1-4 for <strong>every single query</strong> in our 50,000+ entry Golden Dataset. Each query will get its own RR score.</p></li>
</ul>
</li>
<li><p><strong>Calculate the Mean Reciprocal Rank (MRR):</strong></p>
<ul class="simple">
<li><p>The final MRR score is simply the <strong>average of all the individual RR scores</strong> calculated in step 5.</p></li>
</ul>
</li>
</ol>
<p>This process gives us a single, powerful number that represents how well our system performs at placing the <em>most</em> relevant document near the top of the search results across thousands of realistic queries. This is how the Golden Dataset, which only contains the “correct” answers, is used to evaluate a system that produces a full ranking.</p>
<!--
#### **C. The Iteration Log: From Baseline to State-of-the-Art**

The following table details the chronological sequence of experiments, the rationale for each, and the cumulative impact on retrieval performance.

| Stage | Experiment & Rationale | Key Models & Tools | Results & Key Learnings | Cumulative MRR |
| :--- | :--- | :--- | :--- | :--- |
| **1** | **Baseline: Keyword Search** <br> **Rationale:** Establish a non-AI baseline to quantify the value of semantic search. Must outperform this to justify the project. | - **Amazon OpenSearch** (BM25 algorithm) | **Results:** The system performed well on exact product names and SKUs but failed on conceptual, synonymous, and misspelled queries. <br> **Learning:** Confirmed the severe limitations of keyword search, setting a clear lower bound for performance. | **0.65** |
| **2** | **"Naive" RAG (Text-Only)** <br> **Rationale:** Implement the simplest possible RAG to validate the core hypothesis that semantic search provides a significant lift. | - **Embedding:** Amazon Titan Text G1 <br> - **Generator:** Claude 3 Sonnet <br> - **Chunking:** Fixed-size (512 tokens) | **Results:** A massive improvement over the baseline, especially for conceptual queries ("dress for a summer party"). However, retrieval often missed context split across chunk boundaries. <br> **Learning:** Semantic search is a game-changer, but the ingestion process (chunking) is a critical weak point. | **0.82** |
| **3** | **Advanced Retrieval: Hybrid Search + Re-ranker** <br> **Rationale:** Combine the strengths of keyword and semantic search. Use a re-ranker to improve the precision of the top results presented to the LLM. | - **Search:** OpenSearch Hybrid Search (BM25 + Vector) with RRF <br> - **Re-ranker:** Cohere Rerank model | **Results:** Hybrid search significantly improved performance on queries with specific brand names or technical terms. The re-ranker was highly effective at pushing the most relevant documents to the top. <br> **Learning:** Retrieval is not a single step. A multi-stage retrieve-then-rerank process is superior. | **0.88** |
| **4** | **Advanced Indexing & Querying** <br> **Rationale:** Tackle the "lost in the middle" problem with better chunking and improve understanding of vague user queries. | - **Chunking:** LangChain `SemanticChunker` <br> - **Query Transformation:** HyDE using Claude 3 Haiku for low latency. | **Results:** Semantic chunking led to more coherent context, reducing LLM errors. HyDE provided a noticeable lift for short, ambiguous queries. <br> **Learning:** Optimizing what happens *before* and *during* retrieval is as important as the retrieval algorithm itself. | **0.91** |
| **5** | **Multimodal RAG: Image Search** <br> **Rationale:** Address the critical business need for visual product discovery. Test the two primary architectural patterns. | **A)** VLM captions (Claude 3) + Text Embeddings <br> **B)** Unified Embeddings (Amazon Titan Multimodal) | **Results:** **(A)** excelled at attribute-specific queries ("find shoes with this *style* of buckle"). **(B)** was superior for overall visual similarity ("find more dresses that *look like this*"). <br> **Learning:** Neither strategy was a silver bullet. The optimal solution is a **hybrid multi-vector approach**: index both image embeddings and VLM-generated captions, and use a router to decide which to query based on user intent. | **N/A** (Qualitative Improvement) |
| **6** | **Multilingual RAG** <br> **Rationale:** Expand the search to our European user base, testing the trade-offs between translation and native multilingual models. | **A)** Amazon Translate + Titan Text G1 <br> **B)** `multilingual-e5-large` (Native Multilingual Model) | **Results:** **(A)** was simpler to implement but brittle; translation errors on colloquial or technical terms degraded performance significantly. **(B)** was far more robust, achieving near-English performance in German and French. <br> **Learning:** For a high-quality global product, a native multilingual embedding strategy is non-negotiable. | **0.90** (on DE/FR datasets) |
| **7** | **Final Polish: Embedding Model Fine-Tuning** <br> **Rationale:** As a final step, fine-tune the embedding model to learn the specific nuances of our product catalog and user query patterns. | - **Amazon SageMaker** <br> - Dataset of (query, purchased\_product) pairs from the data flywheel. | **Results:** Provided a small but significant final lift, particularly on niche, domain-specific terminology. <br> **Learning:** Fine-tuning is a powerful but high-effort technique. It should only be applied *after* all pipeline and prompt-level optimizations have been exhausted. It is a tool for incremental gains, not for fixing fundamental problems. | **0.94** |

-->
</section>
</section>
<hr class="docutils" />
<section id="continual-learning-the-embedding-model-fine-tuning-pipeline">
<h3><strong>6. Continual Learning: The Embedding Model Fine-tuning Pipeline</strong><a class="headerlink" href="#continual-learning-the-embedding-model-fine-tuning-pipeline" title="Permalink to this heading">¶</a></h3>
<p>To ensure our RAG system continuously adapts and improves, we will build a dedicated MLOps pipeline for fine-tuning our embedding model. This pipeline operationalizes the “Data Flywheel” concept by systematically learning from real user behavior to enhance retrieval relevance over time.</p>
<p>This is an <strong>offline, periodic pipeline</strong>, designed to be run on a schedule (e.g., monthly) or triggered manually when a drop in retrieval quality is detected. Its goal is not to train a model from scratch but to take a high-performing base model and adapt it to the specific nuances of our product catalog and customer query patterns.</p>
<section id="a-artifacts-to-be-implemented">
<h4><strong>A. Artifacts to Be Implemented</strong><a class="headerlink" href="#a-artifacts-to-be-implemented" title="Permalink to this heading">¶</a></h4>
<p>The following is a blueprint for the key components that will constitute the fine-tuning pipeline.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Artifact Type</p></th>
<th class="head text-left"><p>Description &amp; Plan</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Python Scripts (Components)</strong></p></td>
<td class="text-left"><p><strong>1. <code class="docutils literal notranslate"><span class="pre">data_preparation.py</span></code>:</strong> <br> - <strong>Loads</strong> raw user interaction data (from the Monitoring Pipeline’s S3 bucket). <br> - <strong>Filters</strong> for successful user journeys (e.g., queries that led to an <code class="docutils literal notranslate"><span class="pre">add_to_cart</span></code> or <code class="docutils literal notranslate"><span class="pre">purchase</span></code>). <br> - <strong>Constructs Training Triplets <code class="docutils literal notranslate"><span class="pre">(anchor,</span> <span class="pre">positive,</span> <span class="pre">negative)</span></code>:</strong> <br>     - <code class="docutils literal notranslate"><span class="pre">anchor</span></code>: The user’s search query. <br>     - <code class="docutils literal notranslate"><span class="pre">positive</span></code>: The text chunk from the product the user purchased. <br>     - <strong><code class="docutils literal notranslate"><span class="pre">negative</span></code> (Hard Negative Mining):</strong> For the same query, retrieve the top 5 results from the <em>current production embedding model</em>. From these results, select a chunk that the user did <em>not</em> click on or purchase. This “hard negative” is crucial because it teaches the new model to better distinguish between semantically similar but ultimately incorrect items. <br> - <strong>Validates &amp; Splits</strong> the data into training and validation sets. <br> - <strong>Outputs</strong> versioned, final training files to a dedicated S3 bucket. <br><br> <strong>2. <code class="docutils literal notranslate"><span class="pre">model_training.py</span></code>:</strong> <br> - <strong>Loads</strong> the prepared triplet dataset. <br> - <strong>Pulls</strong> the latest version of our chosen base embedding model (e.g., <code class="docutils literal notranslate"><span class="pre">multilingual-e5-large</span></code>) from Hugging Face. <br> - <strong>Fine-tunes</strong> the model using the <code class="docutils literal notranslate"><span class="pre">sentence-transformers</span></code> library and a <code class="docutils literal notranslate"><span class="pre">TripletLoss</span></code> objective function. This process is executed as a self-contained <strong>Amazon SageMaker Training Job</strong> for scalability and reproducibility. <br> - <strong>Saves</strong> the fine-tuned model artifact back to S3. <br><br> <strong>3. <code class="docutils literal notranslate"><span class="pre">model_evaluation.py</span></code>:</strong> <br> - <strong>Loads</strong> the new candidate model and the current production model. <br> - <strong>Runs both models</strong> against our “Golden Dataset” (the held-out test set). <br> - <strong>Compares</strong> key retrieval metrics (MRR, NDCG&#64;10, Hit Rate&#64;5). <br> - <strong>Outputs</strong> a JSON file with a comparison report and a <code class="docutils literal notranslate"><span class="pre">pass</span></code> or <code class="docutils literal notranslate"><span class="pre">fail</span></code> status based on a pre-defined threshold (e.g., “candidate MRR must be &gt; 1.02 * production MRR”). <br><br> <strong>4. <code class="docutils literal notranslate"><span class="pre">model_registration.py</span></code>:</strong> <br> - <strong>Reads</strong> the evaluation result. <br> - <strong>If “pass”:</strong> Registers the new model artifact in the <strong>Amazon SageMaker Model Registry</strong>, tagging it with its evaluation metrics and the training data version. This officially promotes it to a “candidate” status, ready for A/B testing.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Unit Tests (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong></p></td>
<td class="text-left"><p>- <code class="docutils literal notranslate"><span class="pre">test_data_preparation.py</span></code>: Tests the hard negative mining logic on a sample DataFrame to ensure it correctly identifies and selects hard negatives. <br> - <code class="docutils literal notranslate"><span class="pre">test_model_evaluation.py</span></code>: Mocks the models and tests the metric calculation logic to ensure MRR and NDCG are computed correctly.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Pipeline Orchestration (Airflow DAG)</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">embedding_finetuning_dag.py</span></code>:</strong> <br> <strong>1. Trigger:</strong> Can be scheduled (e.g., <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">0</span> <span class="pre">1</span> <span class="pre">*</span> <span class="pre">*</span></code> for monthly) or manually triggered. <br> <strong>2. <code class="docutils literal notranslate"><span class="pre">prepare_data_task</span></code>:</strong> Executes the <code class="docutils literal notranslate"><span class="pre">data_preparation.py</span></code> script (e.g., using <code class="docutils literal notranslate"><span class="pre">PythonOperator</span></code> or <code class="docutils literal notranslate"><span class="pre">KubernetesPodOperator</span></code>). <br> <strong>3. <code class="docutils literal notranslate"><span class="pre">train_model_task</span></code>:</strong> Submits a SageMaker Training Job using the <code class="docutils literal notranslate"><span class="pre">SageMakerTrainingOperator</span></code>. <br> <strong>4. <code class="docutils literal notranslate"><span class="pre">evaluate_model_task</span></code>:</strong> Submits a SageMaker Processing Job to run the <code class="docutils literal notranslate"><span class="pre">model_evaluation.py</span></code> script. <br> <strong>5. <code class="docutils literal notranslate"><span class="pre">check_evaluation_task</span></code> (BranchPythonOperator):</strong> A conditional gate that checks the evaluation output. Routes to <code class="docutils literal notranslate"><span class="pre">register_model_task</span></code> on <code class="docutils literal notranslate"><span class="pre">pass</span></code> or <code class="docutils literal notranslate"><span class="pre">notify_failure_task</span></code> on <code class="docutils literal notranslate"><span class="pre">fail</span></code>. <br> <strong>6. <code class="docutils literal notranslate"><span class="pre">register_model_task</span></code>:</strong> Executes the <code class="docutils literal notranslate"><span class="pre">model_registration.py</span></code> script. <br> <strong>7. <code class="docutils literal notranslate"><span class="pre">notify_failure_task</span></code>:</strong> Sends a Slack/email notification if the model did not meet the quality bar.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Infrastructure as Code (Terraform)</strong></p></td>
<td class="text-left"><p>- <code class="docutils literal notranslate"><span class="pre">s3.tf</span></code>: Defines new S3 buckets for storing the training datasets and the fine-tuned model artifacts. <br> - <code class="docutils literal notranslate"><span class="pre">iam.tf</span></code>: Defines a new, specific IAM Role for the SageMaker Training Jobs, granting least-privilege access to the required S3 buckets. <br> - <code class="docutils literal notranslate"><span class="pre">sagemaker.tf</span></code>: Defines a SageMaker Model Group to logically group all versions of our fine-tuned embedding model. <br> - <code class="docutils literal notranslate"><span class="pre">airflow.tf</span></code>: Manages the deployment of the <code class="docutils literal notranslate"><span class="pre">embedding_finetuning_dag.py</span></code> file to our Airflow environment.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Integration Test</strong></p></td>
<td class="text-left"><p>- An end-to-end test script (<code class="docutils literal notranslate"><span class="pre">test_finetuning_pipeline_integration.py</span></code>). <br> - This test <strong>triggers the entire Airflow DAG</strong> using a small, self-contained dataset. <br> - It does not check the model’s accuracy but <strong>asserts that the DAG runs to completion</strong> and that a new model version successfully appears in the staging SageMaker Model Registry. This validates that all components, permissions, and integrations are working correctly.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>CI/CD Workflow (GitHub Actions)</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">deploy_finetuning_pipeline.yml</span></code>:</strong> This workflow does <strong>not</strong> run the fine-tuning process. It deploys and manages the pipeline’s definition. <br> - <strong>Trigger:</strong> On push to <code class="docutils literal notranslate"><span class="pre">main</span></code> affecting the <code class="docutils literal notranslate"><span class="pre">/pipelines/finetuning/</span></code> directory. <br> - <strong>Jobs:</strong> <br>     1. <strong>Lint &amp; Test:</strong> Runs static analysis and unit tests on the pipeline’s Python scripts. <br>     2. <strong>Deploy:</strong> Executes <code class="docutils literal notranslate"><span class="pre">terraform</span> <span class="pre">apply</span></code> to deploy changes to the Airflow DAG and related AWS infrastructure.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="b-architecture-diagram-embedding-model-fine-tuning-pipeline">
<h4><strong>B. Architecture Diagram: Embedding Model Fine-tuning Pipeline</strong><a class="headerlink" href="#b-architecture-diagram-embedding-model-fine-tuning-pipeline" title="Permalink to this heading">¶</a></h4>
<p>This diagram illustrates the orchestrated workflow, from data collection to model registration.</p>
<img src="../_static/past_experiences/ecom_rag/pipeline_finetuning.png" width="80%" style="background-color: #FCF1EF;"/>
</section>
<hr class="docutils" />
<section id="note-training-triplets-dataset-vs-golden-evaluation-dataset">
<h4>Note: <strong>Training Triplets Dataset</strong> vs <strong>Golden Evaluation Dataset</strong><a class="headerlink" href="#note-training-triplets-dataset-vs-golden-evaluation-dataset" title="Permalink to this heading">¶</a></h4>
<p>The <strong>Training Triplets Dataset</strong> and the <strong>Golden Evaluation Dataset</strong> are two <strong>distinct and separate datasets</strong>, each engineered for a specific, different purpose. They are never the same. Here is a detailed breakdown:</p>
<section id="dataset-1-the-training-triplets-dataset">
<h5><strong>Dataset 1: The Training Triplets Dataset</strong><a class="headerlink" href="#dataset-1-the-training-triplets-dataset" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><strong>Purpose:</strong> To <strong>TEACH</strong> (fine-tune) the embedding model.</p></li>
<li><p><strong>Structure:</strong> <code class="docutils literal notranslate"><span class="pre">(anchor,</span> <span class="pre">positive,</span> <span class="pre">negative)</span></code></p></li>
<li><p><strong>What it does:</strong> This dataset is the “textbook” for the model. It uses a technique called contrastive learning. For every <code class="docutils literal notranslate"><span class="pre">anchor</span></code> (a user’s query), it explicitly tells the model:</p>
<ul>
<li><p>“This <code class="docutils literal notranslate"><span class="pre">positive</span></code> example (the product they bought) is what you should consider very similar. Pull its vector representation <em>closer</em> to the anchor’s vector.”</p></li>
<li><p>“This <code class="docutils literal notranslate"><span class="pre">negative</span></code> example (a product they saw but didn’t buy) is what you should consider dissimilar. Push its vector representation <em>further away</em> from the anchor’s vector.”</p></li>
</ul>
</li>
<li><p><strong>Why it needs negatives:</strong> The model cannot learn to distinguish between good and bad results without being shown both. <strong>Hard negatives</strong> are especially powerful because they teach the model to differentiate between very similar but ultimately incorrect items, which is a much harder and more valuable task than differentiating between a shoe and a television.</p></li>
<li><p><strong>Source of Data:</strong> Real, messy, high-volume user interaction data (clicks, purchases). It reflects what users <em>actually do</em>, not what we <em>think</em> they should do.</p></li>
</ul>
</section>
<section id="dataset-2-the-golden-evaluation-dataset">
<h5><strong>Dataset 2: The “Golden” Evaluation Dataset</strong><a class="headerlink" href="#dataset-2-the-golden-evaluation-dataset" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><strong>Purpose:</strong> To <strong>GRADE</strong> (evaluate or benchmark) the performance of the <em>entire retrieval system</em>.</p></li>
<li><p><strong>Structure:</strong> <code class="docutils literal notranslate"><span class="pre">(query,</span> <span class="pre">relevant_product_id,</span> <span class="pre">relevant_chunk_id)</span></code></p></li>
<li><p><strong>What it does:</strong> This dataset is the “final exam.” It represents the ground truth. It does <strong>not</strong> teach the model anything. Instead, it acts as an answer key against which we measure the performance of any model or retrieval strategy we build.</p></li>
<li><p><strong>Why it does NOT have negatives:</strong> The “negatives” for an evaluation are simply all the other millions of documents in our knowledge base that are <em>not</em> the correct answer for that specific query. The dataset’s only job is to tell us, “For this query, this is the correct document.”</p></li>
<li><p><strong>Source of Data:</strong> Synthetically generated using a powerful LLM and curated for high quality, realism, and broad coverage. It represents the “ideal” testbed.</p></li>
</ul>
<p><strong>Crucial Point:</strong> Using your evaluation data for training is a cardinal sin in machine learning, known as <strong>data leakage</strong>. If we trained the model on our Golden Dataset, it would simply memorize the “exam answers,” and our evaluation scores would be artificially inflated and completely useless for predicting real-world performance.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="the-real-time-engine-the-inference-pipeline">
<h3><strong>7. The Real-Time Engine: The Inference Pipeline</strong><a class="headerlink" href="#the-real-time-engine-the-inference-pipeline" title="Permalink to this heading">¶</a></h3>
<p>The Inference Pipeline is the synchronous, low-latency workflow that executes every time a user interacts with the search assistant. It is a distributed system of microservices, orchestrated to perform the complex RAG process—from query understanding to final generation—in under a few seconds. Its design prioritizes performance, scalability, and observability.</p>
<section id="id2">
<h4>A. Artifacts to Be Implemented<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h4>
<p>The following is a blueprint for the key components that will constitute the Inference Pipeline.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Artifact Type</p></th>
<th class="head text-left"><p>Description &amp; Plan</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Python Scripts (Core Logic)</strong></p></td>
<td class="text-left"><p><strong>The entire pipeline will be implemented as a single, modular Python application using FastAPI, deployed as a containerized service.</strong> This approach is chosen over an orchestrator like Airflow, which is designed for asynchronous, long-running batch jobs, not real-time inference. <br><br> <strong><code class="docutils literal notranslate"><span class="pre">main.py</span></code> (FastAPI Application):</strong> <br> - Defines the main API endpoint (e.g., <code class="docutils literal notranslate"><span class="pre">/search</span></code>). <br> - Handles request validation (using Pydantic), authentication, and orchestrates the RAG workflow. <br><br> <strong><code class="docutils literal notranslate"><span class="pre">orchestrator.py</span></code> (The Core Workflow):</strong> <br> - Implements the full RAG sequence as an <code class="docutils literal notranslate"><span class="pre">async</span></code> function to maximize I/O concurrency. <br> - <strong>1. Query Pre-processing:</strong> Applies input guardrails and calls the Query Transformation module. <br> - <strong>2. Parallel Retrieval:</strong> Uses <code class="docutils literal notranslate"><span class="pre">asyncio.gather</span></code> to concurrently call the Hybrid Search and Re-ranking modules. <br> - <strong>3. Prompt Construction:</strong> Builds the final prompt for the LLM. <br> - <strong>4. Generation &amp; Streaming:</strong> Makes a streaming call to the LLM Generation module. <br> - <strong>5. Post-processing:</strong> Applies output guardrails as the response is streamed. <br> - <strong>6. Tracing:</strong> Integrates <strong>LangSmith</strong> tracing decorators (<code class="docutils literal notranslate"><span class="pre">&#64;traceable</span></code>) on all key functions to automatically capture latency, inputs, and outputs for each step. <br><br> <strong>Modules for Each RAG Stage:</strong> <br> - <strong><code class="docutils literal notranslate"><span class="pre">query_transformer.py</span></code>:</strong> A module to perform HyDE. Caches results in <strong>Redis (Amazon ElastiCache)</strong> for common queries. <br> - <strong><code class="docutils literal notranslate"><span class="pre">retriever.py</span></code>:</strong> Contains logic to perform hybrid search against <strong>Amazon OpenSearch</strong>. <br> - <strong><code class="docutils literal notranslate"><span class="pre">reranker.py</span></code>:</strong> Calls the <strong>SageMaker Endpoint</strong> for the custom re-ranking model and the <strong>SageMaker Feature Store</strong> for real-time business signals. <br> - <strong><code class="docutils literal notranslate"><span class="pre">generator.py</span></code>:</strong> A client for <strong>Amazon Bedrock</strong> that handles streaming responses. <br> - <strong><code class="docutils literal notranslate"><span class="pre">guardrails.py</span></code>:</strong> Modules for input (e.g., PII redaction) and output (e.g., toxicity filtering) safety checks.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Unit Tests (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong></p></td>
<td class="text-left"><p>- <code class="docutils literal notranslate"><span class="pre">test_orchestrator.py</span></code>: Uses <code class="docutils literal notranslate"><span class="pre">pytest-asyncio</span></code> and <code class="docutils literal notranslate"><span class="pre">mocker</span></code> to test the orchestration logic. Mocks out all external calls (OpenSearch, SageMaker, Bedrock) to test the flow, error handling, and <code class="docutils literal notranslate"><span class="pre">asyncio.gather</span></code> integration. <br> - <code class="docutils literal notranslate"><span class="pre">test_retriever.py</span></code>: Tests the construction of the complex OpenSearch hybrid search query JSON. <br> - <code class="docutils literal notranslate"><span class="pre">test_reranker.py</span></code>: Tests the logic for combining semantic scores with business features before sending them to the re-ranking model. <br> - <code class="docutils literal notranslate"><span class="pre">test_guardrails.py</span></code>: Tests the input and output safety filters with example malicious/harmful text.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Pipeline Orchestration</strong></p></td>
<td class="text-left"><p><strong>Not Applicable (FastAPI Application instead of Airflow/Step Functions).</strong> The orchestration is handled in-process by the <code class="docutils literal notranslate"><span class="pre">asyncio</span></code> event loop within the FastAPI application, which is the standard and correct pattern for real-time, low-latency services.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Infrastructure as Code (Terraform)</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">api_gateway.tf</span></code>:</strong> Defines the Amazon API Gateway endpoint, sets up throttling, and configures routing to the inference service. <br> - <strong><code class="docutils literal notranslate"><span class="pre">ecs.tf</span></code> / <code class="docutils literal notranslate"><span class="pre">lambda.tf</span></code>:</strong> Defines the compute layer. <strong>AWS Fargate (on ECS)</strong> is the primary choice for its balance of performance (no cold starts) and manageability. A Lambda-based deployment is a secondary option for lower-traffic environments. <br> - <strong><code class="docutils literal notranslate"><span class="pre">sagemaker.tf</span></code>:</strong> Defines the real-time SageMaker Endpoints for the re-ranking model. <br> - <strong><code class="docutils literal notranslate"><span class="pre">elasticache.tf</span></code>:</strong> Defines the Redis cluster for caching. <br> - <strong><code class="docutils literal notranslate"><span class="pre">iam.tf</span></code>:</strong> Defines fine-grained IAM roles for the inference service, granting it least-privilege access to OpenSearch, SageMaker, Bedrock, and ElastiCache.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Integration &amp; Load Tests</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">test_inference_integration.py</span></code>:</strong> A <code class="docutils literal notranslate"><span class="pre">pytest</span></code> script that makes live HTTP requests to a deployed staging endpoint. It validates the end-to-end flow and asserts the API contract (response schema). <br> - <strong><code class="docutils literal notranslate"><span class="pre">locustfile.py</span></code>:</strong> A <strong>Locust</strong> script to perform load testing. It simulates realistic user query patterns and measures key performance metrics (RPS, p99 latency, error rate) under load. This is critical for tuning auto-scaling policies.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>CI/CD Workflow (GitHub Actions)</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">deploy_inference_service.yml</span></code>:</strong> <br> - <strong>Trigger:</strong> On push to <code class="docutils literal notranslate"><span class="pre">main</span></code> affecting the <code class="docutils literal notranslate"><span class="pre">/inference_service/</span></code> directory. <br> - <strong>Jobs:</strong> <br>     1. <strong>Lint &amp; Unit Test:</strong> Runs static analysis and <code class="docutils literal notranslate"><span class="pre">pytest</span></code> unit tests. <br>     2. <strong>Build &amp; Push Docker Image:</strong> Builds the FastAPI application container and pushes it to <strong>Amazon ECR</strong>. <br>     3. <strong>Deploy to Staging:</strong> Runs <code class="docutils literal notranslate"><span class="pre">terraform</span> <span class="pre">apply</span></code> to deploy the new container version to the staging environment. <br>     4. <strong>Integration &amp; Load Test:</strong> Automatically runs the <code class="docutils literal notranslate"><span class="pre">pytest</span></code> integration tests and the Locust load test against the newly deployed staging endpoint. <br>     5. <strong>Manual Approval Gate:</strong> Requires a manual approval click in the GitHub Actions UI before proceeding to production. <br>     6. <strong>Deploy to Production (A/B Test):</strong> Runs <code class="docutils literal notranslate"><span class="pre">terraform</span> <span class="pre">apply</span></code> to deploy the new version to a small fraction of production traffic using a canary or blue/green deployment strategy managed by the API Gateway.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="b-architecture-diagram-real-time-inference-pipeline">
<h4>B. Architecture Diagram: Real-Time Inference Pipeline<a class="headerlink" href="#b-architecture-diagram-real-time-inference-pipeline" title="Permalink to this heading">¶</a></h4>
<p>This diagram illustrates the flow of a user request through the distributed, microservices-based inference pipeline on AWS.</p>
<img src="../_static/past_experiences/ecom_rag/pipeline_inference.png" width="80%" style="background-color: #FCF1EF;"/>
<blockquote>
<div><h4 class="rubric" id="note-ecs-vs-lambda">Note: ECS vs Lambda</h4>
<ol class="arabic simple">
<li><p><strong>Execution Model:</strong> In a standard Lambda deployment, each incoming request from the API Gateway triggers a separate, independent Lambda invocation. That single invocation would be responsible for running the entire orchestration logic (query transformation, retrieval, re-ranking, generation, etc.) for that one request.</p></li>
<li><p><strong>The Cold Start Problem:</strong> This is the critical drawback of using standard Lambda for low-latency applications. A “cold start” occurs when a request comes in and there is no “warm” execution environment ready to handle it. The Lambda service has to:</p>
<ul class="simple">
<li><p>Provision a new micro-container.</p></li>
<li><p>Download your code/container image.</p></li>
<li><p>Start the runtime (e.g., the Python interpreter).</p></li>
<li><p>Initialize your application code (e.g., import libraries, establish initial database connections).</p></li>
</ul>
</li>
</ol>
<p>This entire process can add anywhere from <strong>hundreds of milliseconds to several seconds</strong> of latency <em>before your actual inference code even begins to run</em>. For a conversational search assistant, this is often an unacceptable hit to the user experience.</p>
<h5 class="rubric" id="the-solution-for-lambda-provisioned-concurrency"><strong>The Solution for Lambda: Provisioned Concurrency</strong>&gt;</h5>
<p>So why is it an option at all? Because AWS provides a feature specifically to solve this: <strong>Provisioned Concurrency</strong>.</p>
<ul class="simple">
<li><p><strong>What it is:</strong> You tell AWS, “I want you to pre-initialize and keep <em>N</em> execution environments warm for this function at all times.”</p></li>
<li><p><strong>How it Works:</strong> When a request arrives, it is immediately routed to one of these pre-warmed environments, <strong>completely bypassing the cold start latency</strong>. The experience becomes as fast as a continuously running server.</p></li>
<li><p><strong>The Trade-off:</strong> Cost. You pay an hourly fee for keeping that concurrency provisioned, whether it’s handling requests or sitting idle.</p></li>
</ul>
<p>This is why a Lambda-based deployment is a viable secondary option for <strong>lower-traffic environments</strong>. If your traffic is very low and sporadic, the cost of keeping a few Lambda environments warm 24/7 might be less than the cost of running even the smallest Fargate task 24/7. However, as traffic becomes high and sustained, the cost model of Fargate becomes more economical.</p>
</div></blockquote>
<hr class="docutils" />
<blockquote>
<div><h4 class="rubric" id="note-the-fargate-ecs-fastapi-model-batch-processing-vs-concurrent-processing">Note: The Fargate (ECS) + FastAPI Model: Batch processing vs concurrent processing</h4>
<ol class="arabic simple">
<li><p><strong>No Batching of User Requests:</strong> In a real-time, user-facing API, you cannot “batch” requests from different users. User A sends a request and expects a response immediately. They cannot be made to wait for User B’s request to arrive so you can process them together. Batching is a strategy for <em>offline</em> data processing (like our Ingestion Pipeline), not for online inference.</p></li>
<li><p><strong>Throughput via Concurrency:</strong> High throughput in this context is achieved by handling many requests <em>at the same time</em>. This happens at two levels:</p>
<ul class="simple">
<li><p><strong>Application Level (FastAPI &amp; <code class="docutils literal notranslate"><span class="pre">asyncio</span></code>):</strong> This is where the magic of <code class="docutils literal notranslate"><span class="pre">async</span></code> comes in. A single FastAPI process running in one Fargate container can handle hundreds of concurrent connections. When it processes Request A and makes a network call to Amazon Bedrock (which is an I/O-bound operation), it doesn’t just sit and wait for the response. The <code class="docutils literal notranslate"><span class="pre">asyncio</span></code> event loop immediately switches context and starts working on Request B, Request C, and so on. When the response for Request A comes back, the event loop picks it up and continues processing. This is how a single container can efficiently juggle many requests at once.</p></li>
<li><p><strong>Infrastructure Level (Fargate &amp; Load Balancer):</strong> When the number of concurrent requests exceeds what a single container can handle, you simply run more containers (Fargate tasks). An <strong>Application Load Balancer (ALB)</strong> sits in front of your Fargate service and distributes the incoming requests across all the available containers. If you need more throughput, you just tell ECS to run more tasks (horizontal scaling).</p></li>
</ul>
</li>
</ol>
<p><strong>In summary:</strong> We don’t increase throughput by batching requests. We increase it by handling many requests concurrently within each container (thanks to FastAPI’s async nature) and by running multiple containers to handle the total traffic load. Each user’s request remains an independent, asynchronous task that is processed as quickly as possible.</p>
</div></blockquote>
</section>
</section>
<hr class="docutils" />
<section id="the-monitoring-and-observability-pipeline">
<h3><strong>8. The Monitoring and Observability Pipeline</strong><a class="headerlink" href="#the-monitoring-and-observability-pipeline" title="Permalink to this heading">¶</a></h3>
<p>Operating a production RAG system effectively requires moving beyond simple uptime checks to a sophisticated, multi-layered observability strategy. This pipeline is an always-on, distributed system responsible for collecting, processing, and visualizing data from every component of our application. Its purpose is to provide actionable insights for engineers, product managers, and the continual learning process.</p>
<section id="id3">
<h4>A. Artifacts to Be Implemented<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Artifact Type</p></th>
<th class="head text-left"><p>Description &amp; Plan</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Python Scripts (Instrumentation &amp; Processing)</strong></p></td>
<td class="text-left"><p><strong>1. <code class="docutils literal notranslate"><span class="pre">instrumentation_lib.py</span></code>:</strong> <br> - A shared Python library integrated into our <strong>Inference Pipeline (FastAPI)</strong> and our <strong>Data Ingestion Pipeline (Glue/Lambda)</strong>. <br> - <strong>Structured Logging:</strong> Configures the standard Python <code class="docutils literal notranslate"><span class="pre">logging</span></code> module to emit all logs in a structured <strong>JSON format</strong>. Each log entry will automatically include a unique <code class="docutils literal notranslate"><span class="pre">trace_id</span></code> to correlate events across services. <br> - <strong>Custom Metrics Emitter:</strong> Provides simple functions (e.g., <code class="docutils literal notranslate"><span class="pre">emit_metric()</span></code>) that use the <code class="docutils literal notranslate"><span class="pre">boto3</span></code> library to send custom metrics directly to <strong>Amazon CloudWatch</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">retrieval_latency</span></code>, <code class="docutils literal notranslate"><span class="pre">tokens_generated</span></code>, <code class="docutils literal notranslate"><span class="pre">user_feedback_received</span></code>). <br> - <strong>LangSmith Integration:</strong> Centralizes the initialization of the LangSmith SDK, ensuring all services are correctly configured to send traces. <br><br> <strong>2. <code class="docutils literal notranslate"><span class="pre">log_processing_lambda.py</span></code>:</strong> <br> - A Lambda function triggered by <strong>Amazon Kinesis Data Firehose</strong>. <br> - <strong>Parses</strong> the structured JSON logs as they arrive. <br> - <strong>Aggregates</strong> logs to calculate advanced metrics that are difficult to compute in real-time (e.g., estimating a “groundedness score” by sending a sample of responses to an LLM-as-a-judge). <br> - <strong>Enriches</strong> logs with additional context (e.g., user metadata from a DynamoDB lookup). <br> - <strong>Sinks</strong> the processed, enriched logs into a dedicated S3 bucket for long-term storage and analysis (the fuel for the Data Flywheel).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Unit Tests (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong></p></td>
<td class="text-left"><p>- <code class="docutils literal notranslate"><span class="pre">test_instrumentation_lib.py</span></code>: Tests that the logging formatter correctly structures messages and that the CloudWatch metric emitter constructs the correct API payload. <br> - <code class="docutils literal notranslate"><span class="pre">test_log_processing_lambda.py</span></code>: Tests the Lambda handler function with sample JSON log events to ensure it correctly parses, enriches, and transforms the data.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Pipeline Orchestration (Event-Driven)</strong></p></td>
<td class="text-left"><p><strong>Not a DAG-based pipeline.</strong> This is a <strong>continuous, event-driven streaming architecture</strong>. The “pipeline” is the real-time flow of data through a series of interconnected AWS services, not a sequence of batch jobs.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Infrastructure as Code (Terraform)</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">cloudwatch.tf</span></code>:</strong> <br>     - Defines <strong>CloudWatch Log Groups</strong> for all services. <br>     - Defines <strong>Custom Metrics</strong> and <strong>Metric Filters</strong> to extract data from logs (e.g., parse latency from a JSON log). <br>     - Creates <strong>CloudWatch Dashboards</strong> to visualize key operational and business metrics. <br>     - Sets up <strong>CloudWatch Alarms</strong> that trigger on critical thresholds (e.g., p99 latency &gt; 2s, Error Rate &gt; 1%, a sudden drop in Groundedness Score). <br> - <strong><code class="docutils literal notranslate"><span class="pre">sns.tf</span></code>:</strong> Defines an <strong>Amazon SNS Topic</strong> that alarms will publish to. <br> - <strong><code class="docutils literal notranslate"><span class="pre">lambda.tf</span></code>:</strong> Deploys the <code class="docutils literal notranslate"><span class="pre">log_processing_lambda.py</span></code> function and its IAM role. <br> - <strong><code class="docutils literal notranslate"><span class="pre">kinesis.tf</span></code>:</strong> Defines the <strong>Kinesis Data Firehose</strong> stream that collects logs from CloudWatch and delivers them to the processing Lambda and finally to S3. <br> - <strong><code class="docutils literal notranslate"><span class="pre">s3.tf</span></code>:</strong> Defines the S3 bucket for long-term, queryable storage of processed logs.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Integration Test</strong></p></td>
<td class="text-left"><p>- <code class="docutils literal notranslate"><span class="pre">test_monitoring_integration.py</span></code>: <br> - An integration test that simulates the end-to-end flow. <br> - <strong>1. Action:</strong> The test makes a call to the staging inference API endpoint. <br> - <strong>2. Wait &amp; Poll:</strong> The test then waits for a few seconds and polls the target <strong>S3 log bucket</strong>. <br> - <strong>3. Assert:</strong> It asserts that a corresponding log file has been created and contains the correct <code class="docutils literal notranslate"><span class="pre">trace_id</span></code> and data from the initial API call. This validates that the entire logging stream (CloudWatch -&gt; Kinesis -&gt; Lambda -&gt; S3) is functioning correctly.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>CI/CD Workflow (GitHub Actions)</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">deploy_monitoring_infra.yml</span></code>:</strong> <br> - <strong>Trigger:</strong> On push to <code class="docutils literal notranslate"><span class="pre">main</span></code> affecting the <code class="docutils literal notranslate"><span class="pre">/monitoring/</span></code> directory. <br> - <strong>Jobs:</strong> <br>     1. <strong>Lint &amp; Unit Test:</strong> Runs static analysis and <code class="docutils literal notranslate"><span class="pre">pytest</span></code> unit tests on the <code class="docutils literal notranslate"><span class="pre">log_processing_lambda.py</span></code> script. <br>     2. <strong>Deploy Infrastructure:</strong> Runs <code class="docutils literal notranslate"><span class="pre">terraform</span> <span class="pre">apply</span></code> to deploy changes to the CloudWatch dashboards, alarms, Kinesis stream, and the processing Lambda. <br>     3. <strong>Run Integration Test:</strong> Executes the <code class="docutils literal notranslate"><span class="pre">test_monitoring_integration.py</span></code> script to validate the end-to-end logging flow in the staging environment.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="b-architecture-diagram-monitoring-and-observability-pipeline">
<h4>B. Architecture Diagram: Monitoring and Observability Pipeline<a class="headerlink" href="#b-architecture-diagram-monitoring-and-observability-pipeline" title="Permalink to this heading">¶</a></h4>
<p>This diagram illustrates the continuous, real-time flow of observability data from production services to our analysis and storage layers.</p>
<img src="../_static/past_experiences/ecom_rag/pipeline_monitoring.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
</section>
<hr class="docutils" />
<section id="testing-in-production-validating-business-impact">
<h3><strong>9. Testing in Production: Validating Business Impact</strong><a class="headerlink" href="#testing-in-production-validating-business-impact" title="Permalink to this heading">¶</a></h3>
<p>Once a new model or feature has passed all offline and staging evaluations, it is ready for the ultimate test: exposure to live user traffic. Testing in production is not about finding bugs; it is about measuring <strong>business impact</strong>. The goal is to answer the question: “Does this change actually improve the key performance indicators we care about?” We will employ a combination of A/B testing and Shadow Testing to make data-driven decisions about rollouts.</p>
<section id="id4">
<h4>A. Artifacts to Be Implemented<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Artifact Type</p></th>
<th class="head text-left"><p>Description &amp; Plan</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Python Scripts (Analysis)</strong></p></td>
<td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">ab_test_analysis.py</span></code>:</strong> <br> - A Python script (or a Jupyter Notebook) that automates the statistical analysis of A/B test results. <br> - <strong>Loads</strong> experiment data from the data warehouse (e.g., Amazon Redshift). This data will contain user events tagged with the experiment <code class="docutils literal notranslate"><span class="pre">variant_id</span></code> (<code class="docutils literal notranslate"><span class="pre">control</span></code> or <code class="docutils literal notranslate"><span class="pre">challenger</span></code>). <br> - <strong>Calculates</strong> primary and secondary KPIs for each variant (e.g., Conversion Rate, AOV, CTR). <br> - <strong>Performs Statistical Significance Testing</strong> (e.g., using a Chi-squared test for conversion rates or a T-test for AOV) to determine if the observed difference between variants is statistically significant or due to random chance. <br> - <strong>Generates a Report</strong> summarizing the results, including p-values and confidence intervals, and provides a clear recommendation: <code class="docutils literal notranslate"><span class="pre">Promote</span> <span class="pre">Challenger</span></code>, <code class="docutils literal notranslate"><span class="pre">Rollback</span> <span class="pre">Challenger</span></code>, or <code class="docutils literal notranslate"><span class="pre">Continue</span> <span class="pre">Experiment</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Unit Tests (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>)</strong></p></td>
<td class="text-left"><p>- <code class="docutils literal notranslate"><span class="pre">test_ab_test_analysis.py</span></code>: Tests the statistical calculation logic within the analysis script. It uses a sample DataFrame with known statistical properties to assert that the script correctly calculates metrics like conversion rate and p-values. This ensures the analysis itself is bug-free.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Pipeline Orchestration</strong></p></td>
<td class="text-left"><p><strong>Not an automated pipeline.</strong> The A/B testing process is a <strong>business workflow</strong> orchestrated by the Product Manager and the ML team, not an automated DAG. The key “pipeline” is the flow of experiment data from the production application into the data warehouse, which is part of the existing data engineering infrastructure.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Infrastructure as Code (Terraform)</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">api_gateway.tf</span></code> (Update):</strong> <br>     - <strong>Canary Deployments:</strong> Modify the API Gateway configuration to support weighted routing. This allows us to direct a small percentage of traffic (e.g., 5%) to a new “challenger” version of our inference service while the majority continues to go to the “control” (current production) version. The traffic split will be managed via Terraform variables. <br> - <strong><code class="docutils literal notranslate"><span class="pre">ecs.tf</span></code> / <code class="docutils literal notranslate"><span class="pre">lambda.tf</span></code> (Update):</strong> <br>     - <strong>Separate Deployments:</strong> The Terraform configuration will be structured to allow for the deployment of two distinct versions of the inference service (Control and Challenger) side-by-side. Each will be a separate ECS Service or Lambda Alias. <br> - <strong><code class="docutils literal notranslate"><span class="pre">feature_flags.tf</span></code> (Optional but Recommended):</strong> <br>     - Defines configurations for a feature flagging service (e.g., AWS AppConfig, LaunchDarkly). This provides a more dynamic and safer way to manage traffic allocation and user segmentation for experiments without requiring a full infrastructure redeployment for every change.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Integration Test</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">test_ab_routing.py</span></code>:</strong> <br> - An integration test that validates the traffic splitting mechanism itself. <br> - <strong>1. Action:</strong> The script makes 1,000 requests to the production API endpoint. <br> - <strong>2. Collect:</strong> It inspects a custom header in the response that identifies which variant (<code class="docutils literal notranslate"><span class="pre">control</span></code> or <code class="docutils literal notranslate"><span class="pre">challenger</span></code>) served the request. <br> - <strong>3. Assert:</strong> It asserts that the distribution of responses is within an expected tolerance of the configured traffic split (e.g., if traffic is split 90/10, the test asserts that the challenger served between 8% and 12% of the requests). This confirms the deployment and routing infrastructure is working as intended.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>CI/CD Workflow (GitHub Actions)</strong></p></td>
<td class="text-left"><p>- <strong><code class="docutils literal notranslate"><span class="pre">deploy_inference_service.yml</span></code> (Update to the “Deploy to Production” job):</strong> <br> - The final step of the CD pipeline is modified. Instead of a “big bang” rollout, it now performs a <strong>Canary Release</strong>. <br> - <strong>1. Deploy Challenger:</strong> The new container version is deployed as a “challenger” service. <br> - <strong>2. Shift Traffic:</strong> The pipeline automatically runs a Terraform plan/apply to update the API Gateway’s weighted routing, shifting a small percentage of traffic (e.g., 5%) to the new challenger. <br> - <strong>3. Monitor:</strong> The pipeline enters a monitoring phase. It queries CloudWatch for the challenger’s key operational metrics (error rate, latency). <br> - <strong>4. Automated Rollback:</strong> If the challenger’s error rate or latency exceeds a critical threshold during this initial phase, the pipeline <strong>automatically rolls back</strong> by shifting 100% of traffic back to the control and triggers an alert. <br> - <strong>5. Manual Promotion:</strong> If the canary deployment is stable, the A/B test begins. The full promotion of the challenger to 100% traffic is a <strong>manual step</strong>, performed by the team after the A/B test has concluded and the results have been analyzed.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="b-the-a-b-testing-workflow-from-candidate-to-champion">
<h4>B. The A/B Testing Workflow: From Candidate to Champion<a class="headerlink" href="#b-the-a-b-testing-workflow-from-candidate-to-champion" title="Permalink to this heading">¶</a></h4>
<p>This workflow describes the end-to-end process for safely deploying and validating a new model.</p>
<ol class="arabic simple">
<li><p><strong>Candidate Promotion:</strong> A new model (e.g., a fine-tuned embedding model) passes all offline evaluations and is registered in the SageMaker Model Registry. This makes it a “candidate” for production.</p></li>
<li><p><strong>Canary Release (Automated via CI/CD):</strong></p>
<ul class="simple">
<li><p>The CD pipeline deploys the candidate model to production infrastructure, creating a “challenger” version of the inference service.</p></li>
<li><p>It shifts a small amount of live traffic (e.g., 5%) to this challenger.</p></li>
<li><p>It performs an immediate health check. If the challenger is unhealthy (high errors/latency), it automatically rolls back.</p></li>
</ul>
</li>
<li><p><strong>Experiment Execution (Manual Start/Stop):</strong></p>
<ul class="simple">
<li><p>If the canary is stable, the A/B test officially begins.</p></li>
<li><p>The team monitors the experiment’s health via CloudWatch and LangSmith dashboards.</p></li>
<li><p>The experiment runs for a pre-determined duration (e.g., one to two weeks) to collect enough data for statistical significance.</p></li>
</ul>
</li>
<li><p><strong>Analysis &amp; Decision:</strong></p>
<ul class="simple">
<li><p>At the end of the experiment, the <code class="docutils literal notranslate"><span class="pre">ab_test_analysis.py</span></code> script is run.</p></li>
<li><p>The team reviews the results and makes a data-driven decision:</p>
<ul>
<li><p><strong>Promote:</strong> If the challenger shows a statistically significant win on the primary business KPI, it is promoted. The CD pipeline is run again to shift 100% of traffic to the new version, which now becomes the new “control.”</p></li>
<li><p><strong>Rollback:</strong> If the challenger is neutral or performs worse, it is rolled back, and 100% of traffic is returned to the original control. The learnings from the failed experiment are documented and used to inform the next iteration.</p></li>
</ul>
</li>
</ul>
</li>
</ol>
</section>
<section id="c-architecture-diagram-a-b-testing-in-production">
<h4>C. Architecture Diagram: A/B Testing in Production<a class="headerlink" href="#c-architecture-diagram-a-b-testing-in-production" title="Permalink to this heading">¶</a></h4>
<p>This diagram illustrates how traffic is split to enable live experimentation.</p>
<img src="../_static/past_experiences/ecom_rag/pipeline_abtesting.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
</section>
<hr class="docutils" />
<section id="the-foundation-of-trust-governance-ethics-and-human-centric-design">
<h3><strong>10. The Foundation of Trust - Governance, Ethics, and Human-Centric Design</strong><a class="headerlink" href="#the-foundation-of-trust-governance-ethics-and-human-centric-design" title="Permalink to this heading">¶</a></h3>
<p>A technically proficient RAG system is only half the battle. To build a truly production-grade, trustworthy application, we must embed principles of governance, ethics, and human-centric design into every stage of the MLOps lifecycle. This is not a final checklist but a continuous commitment to ensuring our system is transparent, fair, secure, and aligned with user expectations.</p>
<section id="a-comprehensive-model-governance">
<h4>A. Comprehensive Model Governance<a class="headerlink" href="#a-comprehensive-model-governance" title="Permalink to this heading">¶</a></h4>
<p>Model governance provides the framework for control, auditability, and compliance, ensuring our RAG system operates responsibly within both internal policies and external regulations like GDPR.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Governance Component</p></th>
<th class="head text-left"><p>Implementation Strategy for RAG Search</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Reproducibility &amp; Auditability</strong></p></td>
<td class="text-left"><p>- <strong>Infrastructure:</strong> All AWS infrastructure is defined declaratively in <strong>Terraform</strong> and versioned in <strong>Git</strong>, providing a complete, auditable history of the environment. <br> - <strong>Data:</strong> Training datasets for the re-ranker and embedding models are versioned with <strong>DVC</strong>, ensuring any model can be precisely reproduced. <br> - <strong>Models:</strong> Every custom model is versioned and logged in the <strong>Amazon SageMaker Model Registry</strong>. <br> - <strong>Tracing:</strong> <strong>LangSmith</strong> provides an immutable, end-to-end trace for every single query, linking the user input, retrieved context, and final LLM output for full auditability.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Documentation &amp; Transparency</strong></p></td>
<td class="text-left"><p>- <strong>Model Cards:</strong> For our custom re-ranking model, we maintain a Model Card detailing its intended use, training data, evaluation metrics (including fairness checks), and known limitations. <br> - <strong>Prompt Versioning:</strong> All system prompts are treated as code, versioned in <strong>Git</strong>, and deployed through CI/CD, providing a clear history of how the LLM’s behavior has been guided over time.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Security &amp; Access Control</strong></p></td>
<td class="text-left"><p>- <strong>Secrets Management:</strong> All API keys (e.g., for LLM providers) are stored securely in <strong>AWS Secrets Manager</strong>. <br> - <strong>Least Privilege:</strong> Each service (Lambda, Fargate, SageMaker) runs with a fine-grained <strong>IAM Role</strong> granting only the permissions necessary for its specific task (e.g., the inference Lambda can read from OpenSearch but cannot write to it). <br> - <strong>Endpoint Security:</strong> The public-facing <strong>API Gateway</strong> is configured with authentication, rate limiting, and throttling to prevent abuse.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="b-responsible-ai-rai-principles-in-practice">
<h4>B. Responsible AI (RAI) Principles in Practice<a class="headerlink" href="#b-responsible-ai-rai-principles-in-practice" title="Permalink to this heading">¶</a></h4>
<p>RAI is about proactively identifying and mitigating potential harms, ensuring our system is fair, explainable, and respects user privacy.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>RAI Principle</p></th>
<th class="head text-left"><p>Implementation Strategy for RAG Search</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Fairness &amp; Bias Mitigation</strong></p></td>
<td class="text-left"><p>- <strong>Data Analysis:</strong> We continuously analyze user interaction data to ensure our re-ranking model is not developing biases (e.g., unfairly penalizing products from smaller vendors). We monitor performance across different product categories to identify and address performance gaps. <br> - <strong>Bias in Generation:</strong> We use <strong>output guardrails</strong> to scan for and block responses that contain biased or stereotypical language.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Explainability &amp; Interpretability (XAI)</strong></p></td>
<td class="text-left"><p>- <strong>Attribution is Key:</strong> This is the most critical form of XAI for RAG. <strong>Every generated statement is linked directly back to the source product page or review</strong>, allowing users to verify the information. This is implemented via citations in the UI. <br> - <strong>Re-ranker Explainability:</strong> For our custom re-ranking model, we use <strong>SHAP (SHapley Additive exPlanations)</strong> during offline evaluation to understand which features (e.g., semantic score, popularity, stock level) are driving its ranking decisions.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Privacy Preservation</strong></p></td>
<td class="text-left"><p>- <strong>PII Redaction:</strong> We implement an <strong>input guardrail</strong> using services like Amazon Comprehend to automatically detect and redact any Personally Identifiable Information from user queries before they are logged or processed. <br> - <strong>Right to be Forgotten:</strong> Our data architecture is designed to support GDPR requirements, with clear processes for deleting user data from all associated systems upon request.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Security (Adversarial Robustness)</strong></p></td>
<td class="text-left"><p>- <strong>Prompt Injection Defense:</strong> Our primary defense is a strong system prompt that clearly instructs the LLM on its role and constraints. We also maintain a suite of adversarial prompts in our evaluation dataset to continuously test for vulnerabilities. <br> - <strong>Data Poisoning Defense:</strong> The <strong>Great Expectations</strong> validation step in our ingestion pipeline acts as a defense against data poisoning by flagging anomalous or malformed product data before it can be indexed.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="c-the-human-element-team-structure-user-centric-design">
<h4>C. The Human Element: Team Structure &amp; User-Centric Design<a class="headerlink" href="#c-the-human-element-team-structure-user-centric-design" title="Permalink to this heading">¶</a></h4>
<p>Technology alone does not create a successful product. The human elements—how the team is structured and how the user experience is designed—are equally critical.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Human-Centric Aspect</p></th>
<th class="head text-left"><p>Implementation Strategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Team Structure &amp; Collaboration</strong></p></td>
<td class="text-left"><p>We operate as a <strong>cross-functional product pod</strong>, bringing together the Product Manager, ML/GenAI Engineer, Data Scientist, and Backend/Frontend Engineers. This structure eliminates silos and ensures tight alignment between business goals, AI development, and user experience. The ML/GenAI Engineer (my role) acts as the technical anchor, translating product requirements into a robust, end-to-end MLOps system.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>User Experience (Defensive UX)</strong></p></td>
<td class="text-left"><p>- <strong>Transparency:</strong> The UI clearly labels the conversational search as an “AI Assistant” to set user expectations. <br> - <strong>Graceful Failure:</strong> When the system has low confidence or cannot find a relevant answer, it is designed to respond transparently (e.g., “I couldn’t find a specific answer for that, but here are some products you might be interested in…”) rather than hallucinating. <br> - <strong>User Control &amp; “Escape Hatches”:</strong> The user is always in control. A clear link to the traditional keyword search is always visible, and users can easily bypass the conversational experience if they choose.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>User Feedback Loops</strong></p></td>
<td class="text-left"><p>- <strong>Explicit Feedback:</strong> Simple <strong>“thumbs up/thumbs down”</strong> icons are present on every generated response, providing a direct, low-friction signal of quality. <br> - <strong>Implicit Feedback:</strong> Our monitoring pipeline tracks user actions post-response (e.g., clicks, add-to-carts, query reformulations) as powerful implicit signals of relevance and helpfulness. This feedback is the primary fuel for our <strong>Data Flywheel</strong>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="system-architecture-performance-and-economics">
<h3><strong>11. System Architecture, Performance, and Economics</strong><a class="headerlink" href="#system-architecture-performance-and-economics" title="Permalink to this heading">¶</a></h3>
<p>The RAG-based search system is architected as a modern, cloud-native application on AWS. The design philosophy is centered around four interconnected loops, each representing a core operational pipeline:</p>
<ol class="arabic simple">
<li><p><strong>The Real-Time Inference Pipeline:</strong> The user-facing, low-latency engine that serves live queries.</p></li>
<li><p><strong>The Offline Data Ingestion Pipeline:</strong> The batch/streaming factory that builds and maintains the system’s knowledge base.</p></li>
<li><p><strong>The Continual Learning (Data Flywheel) Pipeline:</strong> The periodic, offline process that improves the system’s intelligence by learning from user behavior.</p></li>
<li><p><strong>The Continuous Monitoring &amp; Observability Pipeline:</strong> The always-on “nervous system” that provides visibility into the health, cost, and quality of the entire application.</p></li>
</ol>
<p>This modular, microservices-based architecture ensures scalability, resilience, and maintainability.</p>
<section id="a-aws-system-architecture-diagram">
<h4>A. AWS System Architecture Diagram<a class="headerlink" href="#a-aws-system-architecture-diagram" title="Permalink to this heading">¶</a></h4>
<p>The following diagram illustrates the complete, end-to-end system, showing the flow of data and the interaction between all major AWS components and services.</p>
<img src="../_static/past_experiences/ecom_rag/system_architecture.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<hr class="docutils" />
<section id="b-sequence-diagram-the-anatomy-of-a-real-time-rag-query">
<h4><strong>B. Sequence Diagram: The Anatomy of a Real-Time RAG Query</strong><a class="headerlink" href="#b-sequence-diagram-the-anatomy-of-a-real-time-rag-query" title="Permalink to this heading">¶</a></h4>
<p>The following diagram details the synchronous, end-to-end flow of the inference pipeline, from the moment a user submits a query to when they start receiving a streamed response. The latency estimates represent a target for a well-optimized system, operating under normal load.</p>
<img src="../_static/past_experiences/ecom_rag/sequence_inference.png" width="100%" style="background-color: #FCF1EF;"/>
<section id="latency-budget-breakdown">
<h5>Latency Budget Breakdown<a class="headerlink" href="#latency-budget-breakdown" title="Permalink to this heading">¶</a></h5>
<p>The total latency experienced by the user is the sum of the latencies of each step in the sequence. For a conversational application, the most critical metric is <strong>Time to First Byte (TTFB)</strong>—the time it takes for the user to start seeing the first words of the response.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Step</p></th>
<th class="head text-left"><p>Operation</p></th>
<th class="head text-left"><p>Estimated Latency (p99)</p></th>
<th class="head text-left"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1-2</p></td>
<td class="text-left"><p><strong>API Gateway &amp; Network</strong></p></td>
<td class="text-left"><p>10 - 50 ms</p></td>
<td class="text-left"><p>Initial network overhead and API Gateway processing.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>3-4</p></td>
<td class="text-left"><p><strong>Retrieval (OpenSearch)</strong></p></td>
<td class="text-left"><p>50 - 150 ms</p></td>
<td class="text-left"><p>A complex hybrid (vector + keyword) query on a distributed database. Highly dependent on cluster size and index optimization.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>5-6</p></td>
<td class="text-left"><p><strong>Feature Fetch (Feature Store)</strong></p></td>
<td class="text-left"><p>10 - 30 ms</p></td>
<td class="text-left"><p>A low-latency, key-value lookup for business features.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>7-8</p></td>
<td class="text-left"><p><strong>Re-ranking (SageMaker)</strong></p></td>
<td class="text-left"><p>50 - 100 ms</p></td>
<td class="text-left"><p>Network hop plus inference time for the re-ranking model. Faster than the generator LLM but still a significant step.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>9-10</p></td>
<td class="text-left"><p><strong>Generation (Amazon Bedrock)</strong></p></td>
<td class="text-left"><p><strong>150 - 400 ms (TTFB)</strong></p></td>
<td class="text-left"><p>The “thinking time” for the LLM before it generates the first token. This is often the largest single component of perceived latency.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>11-12</p></td>
<td class="text-left"><p><strong>Response Streaming &amp; Network</strong></p></td>
<td class="text-left"><p>10 - 50 ms</p></td>
<td class="text-left"><p>Network latency for the response stream to travel back to the user.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>TOTAL (Time to First Byte)</strong></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><strong>~280 - 780 ms</strong></p></td>
<td class="text-left"><p>The total perceived latency before the user starts reading the answer. <strong>Our primary optimization target is to keep this under 500ms.</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>TOTAL (Full Response)</strong></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p>1.5 - 3.0 seconds</p></td>
<td class="text-left"><p>This depends on the length of the answer. A 200-token response would add approximately 1-2 seconds of total streaming time after the first token.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="c-inference-pipeline-bottlenecks-performance-optimizations">
<h4><strong>C. Inference Pipeline: Bottlenecks &amp; Performance Optimizations</strong><a class="headerlink" href="#c-inference-pipeline-bottlenecks-performance-optimizations" title="Permalink to this heading">¶</a></h4>
<p>Achieving a sub-500ms Time to First Byte (TTFB) in a distributed RAG system requires a relentless focus on performance. The entire inference pipeline is a series of network I/O and computation steps, and every millisecond counts. Based on the sequence diagram, we can identify four primary potential bottlenecks and a corresponding set of optimization strategies.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Bottleneck</p></th>
<th class="head text-left"><p>Description &amp; Impact on Performance</p></th>
<th class="head text-left"><p>Optimization Strategies</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. The Retrieval Bottleneck</strong></p></td>
<td class="text-left"><p><strong>Description:</strong> The hybrid search query against Amazon OpenSearch is the most complex database operation in the pipeline. It involves both a k-NN vector search and a keyword (BM25) search, followed by the fusion of results. As the index size and query volume grow, this step can easily become the largest contributor to latency. <br><br> <strong>Impact:</strong> High retrieval latency directly delays the entire downstream process, pushing TTFB higher and degrading the user experience.</p></td>
<td class="text-left"><p><strong>1. Index Tuning (HNSW Parameters):</strong> For the vector index, meticulously tune the HNSW algorithm parameters (<code class="docutils literal notranslate"><span class="pre">M</span></code> and <code class="docutils literal notranslate"><span class="pre">ef_construction</span></code> during indexing; <code class="docutils literal notranslate"><span class="pre">ef_search</span></code> at query time). This is a direct trade-off: higher values increase accuracy (recall) but also increase latency and memory usage. Find the optimal balance through offline evaluation. <br><br> <strong>2. Sharding &amp; Replication:</strong> Horizontally scale the OpenSearch cluster by adding more nodes and distributing the index across them (sharding). This parallelizes the search, reducing latency. Add replica shards to increase query throughput. <br><br> <strong>3. Pre-filtering with Metadata:</strong> For queries that contain specific filters (e.g., “in the ‘Electronics’ category”), use OpenSearch’s powerful metadata filtering <em>before</em> the vector search. This dramatically reduces the search space, leading to a significant speedup.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. The “Cold Start” &amp; Concurrency Bottleneck</strong></p></td>
<td class="text-left"><p><strong>Description:</strong> This applies if we use AWS Lambda without proper configuration. A cold start can add seconds of latency. Even with a warm container (Lambda with Provisioned Concurrency or Fargate), if the application is not designed for concurrency, it can only handle one request at a time, leading to long wait queues under load. <br><br> <strong>Impact:</strong> Unpredictable, spiky latency (due to cold starts) and a low ceiling on throughput (queries per second) for the entire service.</p></td>
<td class="text-left"><p><strong>1. Choose Fargate for Sustained Traffic:</strong> For any significant, consistent traffic, AWS Fargate is the superior choice as it eliminates cold starts entirely. <br><br> <strong>2. Use Provisioned Concurrency for Lambda:</strong> If using Lambda, <em>always</em> enable Provisioned Concurrency for the production environment to keep a pool of warm instances ready. <br><br> <strong>3. Implement Asynchronous I/O (<code class="docutils literal notranslate"><span class="pre">asyncio</span></code>):</strong> This is the <strong>single most important application-level optimization</strong>. The FastAPI application <em>must</em> be written using <code class="docutils literal notranslate"><span class="pre">async</span></code>/<code class="docutils literal notranslate"><span class="pre">await</span></code> for all network calls (to OpenSearch, SageMaker, Bedrock). This allows a single Fargate container to handle hundreds of concurrent requests by efficiently switching between them while waiting for I/O, dramatically increasing throughput.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. The LLM Generation Bottleneck</strong></p></td>
<td class="text-left"><p><strong>Description:</strong> The LLM’s “thinking time” before it produces the first token (TTFB) is a significant and often irreducible part of the latency budget. Larger, more complex models have higher TTFB. <br><br> <strong>Impact:</strong> Directly adds to the perceived latency for the user. A slow TTFB makes the application feel sluggish, even if the retrieval was fast.</p></td>
<td class="text-left"><p><strong>1. Model Selection:</strong> Choose the smallest, fastest model that meets your quality bar. For many queries, a model like Claude 3 Haiku may be sufficient and significantly faster than Opus. Consider a “router” pattern where simple queries go to a fast model and complex ones go to a slower, more powerful model. <br><br> <strong>2. Prompt Optimization:</strong> Keep the context provided to the LLM as concise as possible without losing critical information. Shorter prompts lead to faster processing. Techniques like Contextual Compression can help here. <br><br> <strong>3. Response Streaming:</strong> <strong>Always stream the response.</strong> This is a UX optimization that has a massive impact on perceived performance. The user starts reading the beginning of the answer while the rest is still being generated, masking the total generation time.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. The “Death by a Thousand Hops” Bottleneck</strong></p></td>
<td class="text-left"><p><strong>Description:</strong> The total latency is the sum of multiple network calls: API Gateway -&gt; Fargate -&gt; OpenSearch -&gt; Feature Store -&gt; SageMaker -&gt; Bedrock. While each individual hop might be fast, they add up. Network latency between services can become a significant factor. <br><br> <strong>Impact:</strong> A slow, creeping increase in overall latency that is hard to attribute to any single component.</p></td>
<td class="text-left"><p><strong>1. Co-location &amp; VPC Endpoints:</strong> Ensure all AWS services are deployed in the <strong>same AWS Region and Availability Zone</strong>. Use <strong>VPC Endpoints</strong> for all AWS service calls (S3, SageMaker, Bedrock). This keeps traffic on the private AWS backbone network, reducing latency and improving security compared to going over the public internet. <br><br> <strong>2. Aggressive Caching (Amazon ElastiCache):</strong> Implement caching for repeatable, high-cost operations. <br>      - <strong>Retrieval Cache:</strong> Cache the results of common search queries for a short TTL (e.g., 5 minutes). <br>      - <strong>HyDE Cache:</strong> The output of the Hypothetical Document Embeddings (HyDE) step is a great candidate for caching, as the same query will always produce the same hypothetical document. <br><br> <strong>3. Connection Pooling:</strong> For services like databases, maintain a warm pool of connections within the Fargate service to avoid the overhead of establishing a new connection for every request.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="d-estimated-monthly-costs-for-the-rag-system">
<h4><strong>D. Estimated Monthly Costs for the RAG System</strong><a class="headerlink" href="#d-estimated-monthly-costs-for-the-rag-system" title="Permalink to this heading">¶</a></h4>
<p>To ensure the economic viability of the project, a detailed cost estimation was performed. The following model breaks down the monthly operational costs across the three core pipelines: Ingestion, Fine-tuning, and the real-time Inference service.</p>
<!--
**Core Assumptions (Growth Stage Scenario):**

*   **Product Catalog:** 10 Million Products (requiring ~150 GB of vector index storage).
*   **User Traffic:** 5 Million search queries per month.
*   **Data Ingestion:** One full-catalog refresh per month.
*   **Model Fine-tuning:** One fine-tuning run per month.
*   **Infrastructure:** AWS-based, using the services outlined in the architecture. Prices are based on public `us-east-1` pricing in 2024.

| Pipeline Component | AWS Service(s) | Detailed Cost Calculation & Rationale | Estimated Cost (USD) |
| :--- | :--- | :--- | :--- |
| **Data Ingestion Pipeline (Batch)** | **AWS Step Functions** <br> **AWS Glue** <br> **Amazon Bedrock** | **Step Functions:** Priced per state transition. A full catalog refresh involves millions of transitions. <br> - 10M products * 5 steps/product * $25/M transitions = **~$1,250** <br><br> **Glue:** Priced per DPU-hour. A large-scale ETL and chunking job for the entire catalog. <br> - ~200 DPU-hours = **~$8,800** <br><br> **Bedrock (Embeddings):** Priced per 1M input tokens. Assume each product yields ~2k tokens for embedding. <br> - 10M products * 2k tokens/product * ($0.0001 / 1k tokens) = **~$2,000** | **$12,000 - $15,000** <br> *(Per full run)* |
| **Continual Learning Pipeline (Fine-tuning)** | **Amazon MWAA (Airflow)** <br> **Amazon SageMaker Training** | **Airflow Infrastructure (MWAA):** The cost of the 24/7 environment to orchestrate the pipeline. <br> - Small MWAA environment = **~$130/month** <br><br> **SageMaker Training:** Priced per instance-hour. A fine-tuning job on a GPU instance. <br> - 1 run/month * 4 hours/run * `ml.g5.2xlarge` (~$2.20/hr) = **~$9**. Training is cheap; the data prep is the main effort. | **$150 - $250** <br> *(Per month)* |
| **Real-Time Inference Pipeline** | **Amazon OpenSearch** <br> **Amazon SageMaker Endpoint** <br> **Amazon Bedrock** <br> **API Gateway + Fargate** | **OpenSearch Service:** This is often the **largest single cost**. Priced per instance-hour. A cluster to handle the query load and hold the 150GB index. <br> - ~6 x `r6g.xlarge.search` instances = **~$3,500** <br><br> **SageMaker Endpoint (Re-ranker):** Priced per instance-hour. Requires a constantly running endpoint. <br> - 2 x `ml.c5.large` instances for HA = **~$250** <br><br> **Bedrock (Generation):** Priced per token. 5M queries with ~2k input and ~200 output tokens. <br> - Input: 5M * 2k * ($0.25/1M tokens) = **~$2,500** <br> - Output: 5M * 0.2k * ($0.75/1M tokens) = **~$750** <br><br> **API Gateway + Fargate:** Compute for the orchestrator and API layer. <br> - API Gateway: 5M requests * $1.00/M = **$5** <br> - Fargate: ~3 tasks of 1 vCPU = **~$90** | **$7,000 - $9,000** <br> *(Per month)* |
| **Shared Costs** | **S3 Storage** <br> **CloudWatch** | **S3:** Storage for raw data, processed logs, model artifacts. Assuming ~2 TB total. <br> - 2048 GB * $0.023/GB-month = **~$50** <br><br> **CloudWatch:** Logs and metrics from all services. This can be significant. <br> - ~500 GB log ingestion + metrics/alarms = **~$300** | **$350 - $500** <br> *(Per month)* |
| **Total Estimated Monthly Cost** | **-** | This cost is dominated by the full batch ingestion run and the 24/7 inference infrastructure. | **~$19,500 - $24,750** |

**Key Financial Learnings:**

*   **Inference Compute is King:** The 24/7 cost of the OpenSearch cluster for serving real-time queries is the most significant and consistent monthly expense. Optimizing the cluster size and using Graviton instances is critical for managing this cost.
*   **Batch Ingestion is Spiky but Substantial:** While not a continuous cost, a full re-indexing of the catalog is a computationally expensive event. This cost needs to be budgeted for and the frequency of full refreshes should be carefully considered (e.g., monthly vs. quarterly).
*   **LLM Tokens Add Up:** At scale, the per-token cost of the generator LLM becomes a major variable expense. Monitoring token usage and using the smallest effective model are key cost-control levers.
*   **Training is Relatively Inexpensive:** The actual cost of the fine-tuning compute is almost negligible compared to the infrastructure required to serve and monitor the system. The real investment in continual learning is in the engineering effort to build and maintain the data flywheel pipeline.
-->
<p><strong>Core Assumptions:</strong></p>
<ul class="simple">
<li><p><strong>Product Catalog:</strong> 100,000 Products (requiring <strong>~1.5 GB</strong> of vector index storage).</p></li>
<li><p><strong>User Traffic:</strong> 5 Million search queries per month (remains high, indicating an active user base).</p></li>
<li><p><strong>Data Ingestion:</strong> One-time batch ingestion, with small weekly incremental updates (~1,000 products/week).</p></li>
<li><p><strong>Model Fine-tuning:</strong> Two times per year.</p></li>
</ul>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Pipeline Component</p></th>
<th class="head text-left"><p>AWS Service(s)</p></th>
<th class="head text-left"><p>Detailed Cost Calculation &amp; Rationale</p></th>
<th class="head text-left"><p>Estimated Cost (USD)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Data Ingestion Pipeline (Periodic)</strong></p></td>
<td class="text-left"><p><strong>AWS Lambda</strong> <br> <strong>Amazon Bedrock</strong></p></td>
<td class="text-left"><p><strong>This is now a small, recurring cost for weekly updates.</strong> We’ll assume ~4,000 products are updated per month. <br> - <strong>Lambda Compute:</strong> Low-volume ETL jobs can run on Lambda instead of Glue. ~4,000 products/month * ~10s/product * ($0.0000167/ms) = <strong>~$25</strong> <br> - <strong>Bedrock (Embeddings):</strong> Token cost for embedding updated products. <br> - 4k products * 2k tokens/product * ($0.0001 / 1k tokens) = <strong>~$1</strong>. <br> The cost is now minimal.</p></td>
<td class="text-left"><p><strong>$30 - $50</strong> <br> <em>(Per month)</em></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Continual Learning Pipeline (Periodic)</strong></p></td>
<td class="text-left"><p><strong>Amazon MWAA (Airflow)</strong></p></td>
<td class="text-left"><p>The cost of the Airflow infrastructure to be ready for the bi-annual fine-tuning runs and potentially other orchestration tasks. The expensive SageMaker training job cost is no longer a recurring monthly expense. <br> - Small MWAA environment = <strong>~$130/month</strong></p></td>
<td class="text-left"><p><strong>$130 - $180</strong> <br> <em>(Per month)</em></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Real-Time Inference Pipeline</strong></p></td>
<td class="text-left"><p><strong>Amazon OpenSearch</strong> <br> <strong>Amazon SageMaker Endpoint</strong> <br> <strong>Amazon Bedrock</strong> <br> <strong>API Gateway + Fargate</strong></p></td>
<td class="text-left"><p><strong>OpenSearch Service:</strong> <strong>Significant cost reduction.</strong> A 1.5 GB index can be served with high availability and throughput by a much smaller cluster. <br> - 2 x <code class="docutils literal notranslate"><span class="pre">r6g.large.search</span></code> instances for HA = <strong>~$475</strong> <br><br> <strong>SageMaker Endpoint (Re-ranker):</strong> No change. Still need a 2-node HA endpoint for the re-ranking model. <br> - 2 x <code class="docutils literal notranslate"><span class="pre">ml.c5.large</span></code> instances = <strong>~$250</strong> <br><br> <strong>Bedrock (Generation):</strong> <strong>This is now the dominant cost.</strong> Driven by query volume, which remains high. <br> - 5M queries * 2.2k avg tokens/query * ($0.34/1M avg tokens) = <strong>~$3,740</strong> <br><br> <strong>API Gateway + Fargate:</strong> No change. Driven by 5M requests. <br> - API Gateway + Fargate Compute = <strong>~$95</strong></p></td>
<td class="text-left"><p><strong>$4,500 - $5,500</strong> <br> <em>(Per month)</em></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Shared Costs</strong></p></td>
<td class="text-left"><p><strong>S3 Storage</strong> <br> <strong>CloudWatch</strong></p></td>
<td class="text-left"><p><strong>S3:</strong> Storage cost is drastically reduced due to smaller catalog size. <br> - ~20 GB total storage * $0.023/GB-month = <strong>~$1</strong> (rounded to <strong>~$20</strong> for logs) <br><br> <strong>CloudWatch:</strong> Log volume is driven by high query traffic, so this cost remains similar. <br> - ~500 GB log ingestion + metrics/alarms = <strong>~$300</strong></p></td>
<td class="text-left"><p><strong>$320 - $400</strong> <br> <em>(Per month)</em></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Total Estimated Monthly Cost</strong></p></td>
<td class="text-left"><p><strong>-</strong></p></td>
<td class="text-left"><p>The recurring monthly cost is now driven primarily by the LLM API usage and the 24/7 inference compute.</p></td>
<td class="text-left"><p><strong>~$5,000 - $6,130</strong></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="non-recurring-costs">
<h4><strong>Non-Recurring Costs</strong><a class="headerlink" href="#non-recurring-costs" title="Permalink to this heading">¶</a></h4>
<p>It is critical to budget for the following periodic and one-time costs separately from the monthly operational expenses:</p>
<ul class="simple">
<li><p><strong>One-Time Full Ingestion:</strong> The initial, one-time cost to ingest the entire 100,000 product catalog would be approximately <strong>$120 - $150</strong>.</p></li>
<li><p><strong>Bi-Annual Re-indexing &amp; Fine-tuning:</strong> Twice a year, the full ingestion pipeline will need to be re-run after the embedding model is fine-tuned. This should be treated as a project-based cost of <strong>~$12,000 - $15,000</strong>, incurred twice per year.</p></li>
</ul>
</section>
<section id="key-financial-learnings">
<h4><strong>Key Financial Learnings</strong><a class="headerlink" href="#key-financial-learnings" title="Permalink to this heading">¶</a></h4>
<p>This revised, more realistic model provides critical insights for a mid-sized business:</p>
<ol class="arabic simple">
<li><p><strong>The Cost Center Shifts to the LLM:</strong> With a smaller knowledge base, the dominant operational cost is no longer the vector database compute. It is now the <strong>per-query token cost of the generator LLM (Amazon Bedrock)</strong>. This places a high premium on prompt optimization and using the most cost-effective model (e.g., Claude 3 Haiku) that meets the quality bar.</p></li>
<li><p><strong>Smart Ingestion is Key:</strong> An event-driven, incremental update strategy for the knowledge base is dramatically cheaper than frequent full refreshes.</p></li>
<li><p><strong>A/B Testing is Financially Justified:</strong> With a monthly cost of ~$5,000, even a small 1-2% improvement in conversion rate resulting from an A/B test can provide a clear and immediate positive ROI. This justifies the engineering effort required for experimentation.</p></li>
<li><p><strong>The Total Cost is Now Viable:</strong> A total recurring cost in the range of <strong>$5,000 - $6,000 per month</strong> is a much more justifiable expense for a mid-sized business, especially given the potential for significant conversion and AOV lift.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="implementation-data-ingestion-and-indexing-pipeline">
<h3>Implementation: Data Ingestion and Indexing Pipeline<a class="headerlink" href="#implementation-data-ingestion-and-indexing-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="python-scripts-core-logic">
<h4>Python Scripts (Core Logic)<a class="headerlink" href="#python-scripts-core-logic" title="Permalink to this heading">¶</a></h4>
<p>The core logic is broken down into modular scripts, each responsible for a specific stage of the pipeline. These scripts would be packaged and deployed as AWS Lambda functions or run within an AWS Glue job.</p>
<p><strong>ingestion_pipeline/src/data_loader.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_product_data</span><span class="p">(</span><span class="n">bucket</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads a single product&#39;s JSON data from S3.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">s3</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s1">&#39;s3&#39;</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">s3</span><span class="o">.</span><span class="n">get_object</span><span class="p">(</span><span class="n">Bucket</span><span class="o">=</span><span class="n">bucket</span><span class="p">,</span> <span class="n">Key</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;Body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error loading data from s3://</span><span class="si">{</span><span class="n">bucket</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>

<span class="c1"># In a real scenario, this would list all new/updated product files.</span>
<span class="c1"># For the Step Function, the input `key` will be provided in the event payload.</span>
</pre></div>
</div>
<p><strong>ingestion_pipeline/src/text_processor.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">BedrockChat</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="c1"># Assume Bedrock client is initialized globally or passed in</span>
<span class="n">bedrock_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s1">&#39;bedrock-runtime&#39;</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Basic text cleaning (e.g., remove HTML tags).&quot;&quot;&quot;</span>
    <span class="c1"># In a real implementation, use BeautifulSoup or a similar library.</span>
    <span class="c1"># For brevity, we&#39;ll use a simple placeholder.</span>
    <span class="kn">import</span> <span class="nn">re</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;&lt;[^&gt;]+&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">chunk_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Splits text into semantically coherent chunks.&quot;&quot;&quot;</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span>
        <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">chunk_overlap</span><span class="p">,</span>
        <span class="n">length_function</span><span class="o">=</span><span class="nb">len</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_image_caption</span><span class="p">(</span><span class="n">image_bytes</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">bedrock_model_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;anthropic.claude-3-sonnet-20240229-v1:0&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates a descriptive caption for an image using a VLM.&quot;&quot;&quot;</span>
    <span class="c1"># This would be a more complex implementation involving base64 encoding and</span>
    <span class="c1"># constructing the correct payload for the multimodal Bedrock model.</span>
    <span class="c1"># For brevity, this is a conceptual placeholder.</span>
    <span class="c1"># llm = BedrockChat(model_id=bedrock_model_id, client=bedrock_client)</span>
    <span class="c1"># message = HumanMessage(content=[...]) # construct multimodal message</span>
    <span class="c1"># response = llm.invoke([message])</span>
    <span class="c1"># return response.content</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating caption for image of size </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">image_bytes</span><span class="p">)</span><span class="si">}</span><span class="s2"> bytes.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot;A high-quality photo of a product, showing its key features.&quot;</span> <span class="c1"># Placeholder</span>
</pre></div>
</div>
<p><strong>ingestion_pipeline/src/embedding_generator.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">langchain_community.embeddings</span> <span class="kn">import</span> <span class="n">BedrockEmbeddings</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_text_embeddings</span><span class="p">(</span><span class="n">chunks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates embeddings for a list of text chunks.&quot;&quot;&quot;</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">BedrockEmbeddings</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;amazon.titan-embed-text-v2:0&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating text embeddings for </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span><span class="si">}</span><span class="s2"> chunks.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_image_embedding</span><span class="p">(</span><span class="n">image_bytes</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates an embedding for a single image.&quot;&quot;&quot;</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">BedrockEmbeddings</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;amazon.titan-embed-image-v1&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating image embedding for image of size </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">image_bytes</span><span class="p">)</span><span class="si">}</span><span class="s2"> bytes.&quot;</span><span class="p">)</span>
    <span class="c1"># The actual implementation would involve base64 encoding the image</span>
    <span class="k">return</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="s2">&quot;placeholder for image bytes&quot;</span><span class="p">)</span> <span class="c1"># Placeholder</span>
</pre></div>
</div>
<p><strong>ingestion_pipeline/src/opensearch_indexer.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">opensearchpy</span> <span class="kn">import</span> <span class="n">OpenSearch</span><span class="p">,</span> <span class="n">RequestsHttpConnection</span><span class="p">,</span> <span class="n">AWSV4SignerAuth</span>
<span class="kn">from</span> <span class="nn">opensearchpy.helpers</span> <span class="kn">import</span> <span class="n">bulk</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_opensearch_client</span><span class="p">(</span><span class="n">host</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">region</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initializes and returns an OpenSearch client.&quot;&quot;&quot;</span>
    <span class="n">credentials</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span><span class="o">.</span><span class="n">get_credentials</span><span class="p">()</span>
    <span class="n">auth</span> <span class="o">=</span> <span class="n">AWSV4SignerAuth</span><span class="p">(</span><span class="n">credentials</span><span class="p">,</span> <span class="n">region</span><span class="p">,</span> <span class="s1">&#39;aoss&#39;</span><span class="p">)</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenSearch</span><span class="p">(</span>
        <span class="n">hosts</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;host&#39;</span><span class="p">:</span> <span class="n">host</span><span class="p">,</span> <span class="s1">&#39;port&#39;</span><span class="p">:</span> <span class="mi">443</span><span class="p">}],</span>
        <span class="n">http_auth</span><span class="o">=</span><span class="n">auth</span><span class="p">,</span>
        <span class="n">use_ssl</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">verify_certs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">connection_class</span><span class="o">=</span><span class="n">RequestsHttpConnection</span><span class="p">,</span>
        <span class="n">pool_maxsize</span><span class="o">=</span><span class="mi">20</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">client</span>

<span class="k">def</span> <span class="nf">index_documents</span><span class="p">(</span><span class="n">client</span><span class="p">:</span> <span class="n">OpenSearch</span><span class="p">,</span> <span class="n">index_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Bulk indexes documents into OpenSearch.&quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Indexing </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents into index &#39;</span><span class="si">{</span><span class="n">index_name</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="n">success</span><span class="p">,</span> <span class="n">failed</span> <span class="o">=</span> <span class="n">bulk</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">index_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">failed</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to index </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">failed</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">success</span><span class="p">,</span> <span class="n">failed</span>
</pre></div>
</div>
</section>
<section id="unit-tests">
<h4>Unit Tests<a class="headerlink" href="#unit-tests" title="Permalink to this heading">¶</a></h4>
<p><strong>ingestion_pipeline/tests/unit/test_text_processor.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">src</span> <span class="kn">import</span> <span class="n">text_processor</span>

<span class="k">def</span> <span class="nf">test_chunk_text_splits_correctly</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Ensures text is split into multiple chunks.&quot;&quot;&quot;</span>
    <span class="n">long_text</span> <span class="o">=</span> <span class="s2">&quot;This is a sentence. &quot;</span> <span class="o">*</span> <span class="mi">200</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">text_processor</span><span class="o">.</span><span class="n">chunk_text</span><span class="p">(</span><span class="n">long_text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="c1"># Check if overlap is working</span>
    <span class="k">assert</span> <span class="n">chunks</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">50</span><span class="p">:])</span>

<span class="k">def</span> <span class="nf">test_get_image_caption_mocked</span><span class="p">(</span><span class="n">mocker</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests the image captioning function with a mocked Bedrock client.&quot;&quot;&quot;</span>
    <span class="c1"># We mock the Bedrock client to avoid making a real API call</span>
    <span class="n">mock_bedrock</span> <span class="o">=</span> <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s1">&#39;boto3.client&#39;</span><span class="p">)</span>
    <span class="c1"># conceptually, you would mock the invoke method&#39;s return value</span>
    
    <span class="n">caption</span> <span class="o">=</span> <span class="n">text_processor</span><span class="o">.</span><span class="n">get_image_caption</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;fake_image_bytes&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">caption</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">caption</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
</pre></div>
</div>
</section>
<section id="pipeline-orchestration-aws-step-functions">
<h4>Pipeline Orchestration (AWS Step Functions)<a class="headerlink" href="#pipeline-orchestration-aws-step-functions" title="Permalink to this heading">¶</a></h4>
<p><strong>ingestion_pipeline/statemachine/ingestion_statemachine.asl.json</strong></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;Comment&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;RAG Data Ingestion Pipeline for a single product.&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;StartAt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;LoadProductData&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;States&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;LoadProductData&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Task&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:states:::lambda:invoke&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;FunctionName&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;${LoadDataLambdaArn}&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;Payload.$&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;$&quot;</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;Next&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ProcessAndChunkText&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Catch&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">        </span><span class="nt">&quot;ErrorEquals&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;States.ALL&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;Next&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NotifyFailure&quot;</span>
<span class="w">      </span><span class="p">}]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;ProcessAndChunkText&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Task&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:states:::lambda:invoke&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;InputPath&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;$.Payload&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;ResultPath&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;$.ProcessedText&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Next&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;GenerateEmbeddings&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;GenerateEmbeddings&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Task&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:states:::lambda:invoke&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;InputPath&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;$.ProcessedText&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;ResultPath&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;$.Embeddings&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Next&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;IndexInOpenSearch&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;IndexInOpenSearch&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Task&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:states:::lambda:invoke&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;InputPath&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;$.Embeddings&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;End&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;NotifyFailure&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Task&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:states:::sns:publish&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;TopicArn&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;${SnsTopicArn}&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;Message&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="nt">&quot;Input.$&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;$&quot;</span><span class="p">,</span>
<span class="w">          </span><span class="nt">&quot;Message&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;RAG Ingestion Pipeline Failed!&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;End&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="infrastructure-as-code-terraform">
<h4>Infrastructure as Code (Terraform)<a class="headerlink" href="#infrastructure-as-code-terraform" title="Permalink to this heading">¶</a></h4>
<p><strong>ingestion_pipeline/infra/main.tf</strong></p>
<div class="highlight-hcl notranslate"><div class="highlight"><pre><span></span><span class="c1"># main.tf - Defines the core infrastructure for the ingestion pipeline</span>

<span class="c1"># Variable for environment (e.g., &quot;staging&quot;, &quot;prod&quot;)</span>
<span class="kr">variable</span><span class="w"> </span><span class="nv">&quot;env&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">type</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="kt">string</span>
<span class="w">  </span><span class="na">default</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;staging&quot;</span>
<span class="p">}</span>

<span class="c1"># --- S3 Buckets ---</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_s3_bucket&quot;</span><span class="w"> </span><span class="nv">&quot;raw_data&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">bucket</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-product-data-raw-${var.env}&quot;</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_s3_bucket&quot;</span><span class="w"> </span><span class="nv">&quot;processed_data&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">bucket</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-product-data-processed-${var.env}&quot;</span>
<span class="p">}</span>

<span class="c1"># --- IAM Roles ---</span>
<span class="c1"># A comprehensive IAM role for the Lambda functions and Step Function</span>
<span class="c1"># In a real setup, you would create separate, least-privilege roles for each component.</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role&quot;</span><span class="w"> </span><span class="nv">&quot;ingestion_pipeline_role&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;IngestionPipelineRole-${var.env}&quot;</span>
<span class="c1">  # Assume role policy allows lambda and states services</span>
<span class="w">  </span><span class="na">assume_role_policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;...&quot;</span><span class="c1"> # Placeholder for IAM trust policy JSON</span>
<span class="p">}</span>
<span class="c1"># ... Attach policies for S3, Bedrock, OpenSearch, CloudWatch ...</span>

<span class="c1"># --- Lambda Functions ---</span>
<span class="c1"># Placeholder for one of the Lambda functions</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_lambda_function&quot;</span><span class="w"> </span><span class="nv">&quot;load_data_lambda&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">function_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;LoadDataLambda-${var.env}&quot;</span>
<span class="w">  </span><span class="na">role</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.ingestion_pipeline_role.arn</span>
<span class="w">  </span><span class="na">handler</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;data_loader.handler&quot;</span>
<span class="w">  </span><span class="na">runtime</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;python3.11&quot;</span>
<span class="c1">  # ... code packaging configuration ...</span>
<span class="p">}</span>

<span class="c1"># ... Define other Lambda functions for each step ...</span>

<span class="c1"># --- Step Functions State Machine ---</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sfn_state_machine&quot;</span><span class="w"> </span><span class="nv">&quot;ingestion_sfn&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;RAG-Ingestion-Pipeline-${var.env}&quot;</span>
<span class="w">  </span><span class="na">role_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.ingestion_pipeline_role.arn</span>

<span class="w">  </span><span class="na">definition</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">templatefile</span><span class="p">(</span><span class="s2">&quot;${path.module}/../statemachine/ingestion_statemachine.asl.json&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">LoadDataLambdaArn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_lambda_function.load_data_lambda.arn</span><span class="p">,</span>
<span class="c1">    # ... pass other Lambda ARNs and SNS Topic ARN ...</span>
<span class="w">  </span><span class="p">})</span>
<span class="p">}</span>

<span class="c1"># --- OpenSearch Serverless ---</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_opensearchserverless_collection&quot;</span><span class="w"> </span><span class="nv">&quot;vector_db&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-vector-db-${var.env}&quot;</span>
<span class="w">  </span><span class="na">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;VECTORSEARCH&quot;</span>
<span class="p">}</span>

<span class="c1"># --- EventBridge Scheduler for Batch ---</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_cloudwatch_event_rule&quot;</span><span class="w"> </span><span class="nv">&quot;nightly_trigger&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">                </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;NightlyIngestionTrigger-${var.env}&quot;</span>
<span class="w">  </span><span class="na">schedule_expression</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;cron(0 2 * * ? *)&quot;</span><span class="c1"> # 2 AM UTC every night</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_cloudwatch_event_target&quot;</span><span class="w"> </span><span class="nv">&quot;step_function_target&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">rule</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_cloudwatch_event_rule.nightly_trigger.name</span>
<span class="w">  </span><span class="na">arn</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_sfn_state_machine.ingestion_sfn.arn</span>
<span class="w">  </span><span class="na">role_arn</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.ingestion_pipeline_role.arn</span><span class="c1"> # A specific role for EventBridge is better</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="integration-test">
<h4>Integration Test<a class="headerlink" href="#integration-test" title="Permalink to this heading">¶</a></h4>
<p><strong>ingestion_pipeline/tests/integration/test_ingestion_pipeline.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Assume env variables are set for staging resources (e.g., STATE_MACHINE_ARN)</span>
<span class="n">STATE_MACHINE_ARN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STATE_MACHINE_ARN&quot;</span><span class="p">]</span>
<span class="n">RAW_BUCKET</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RAW_BUCKET&quot;</span><span class="p">]</span>
<span class="n">OS_HOST</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OS_HOST&quot;</span><span class="p">]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_full_pipeline_run</span><span class="p">():</span>
    <span class="n">s3_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;s3&quot;</span><span class="p">)</span>
    <span class="n">sfn_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;stepfunctions&quot;</span><span class="p">)</span>
    
    <span class="c1"># 1. ARRANGE: Upload a sample product file to the raw S3 bucket</span>
    <span class="n">product_id</span> <span class="o">=</span> <span class="s2">&quot;test-product-123&quot;</span>
    <span class="n">s3_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;products/</span><span class="si">{</span><span class="n">product_id</span><span class="si">}</span><span class="s2">.json&quot;</span>
    <span class="n">sample_data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="n">product_id</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;This is a test.&quot;</span><span class="p">}</span>
    <span class="n">s3_client</span><span class="o">.</span><span class="n">put_object</span><span class="p">(</span><span class="n">Bucket</span><span class="o">=</span><span class="n">RAW_BUCKET</span><span class="p">,</span> <span class="n">Key</span><span class="o">=</span><span class="n">s3_key</span><span class="p">,</span> <span class="n">Body</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">sample_data</span><span class="p">))</span>

    <span class="c1"># 2. ACT: Trigger the Step Function and wait for completion</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">sfn_client</span><span class="o">.</span><span class="n">start_execution</span><span class="p">(</span>
        <span class="n">stateMachineArn</span><span class="o">=</span><span class="n">STATE_MACHINE_ARN</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;bucket&quot;</span><span class="p">:</span> <span class="n">RAW_BUCKET</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="n">s3_key</span><span class="p">})</span>
    <span class="p">)</span>
    <span class="n">execution_arn</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;executionArn&#39;</span><span class="p">]</span>
    
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">status_response</span> <span class="o">=</span> <span class="n">sfn_client</span><span class="o">.</span><span class="n">describe_execution</span><span class="p">(</span><span class="n">executionArn</span><span class="o">=</span><span class="n">execution_arn</span><span class="p">)</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">status_response</span><span class="p">[</span><span class="s1">&#39;status&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">status</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;SUCCEEDED&#39;</span><span class="p">,</span> <span class="s1">&#39;FAILED&#39;</span><span class="p">,</span> <span class="s1">&#39;TIMED_OUT&#39;</span><span class="p">,</span> <span class="s1">&#39;ABORTED&#39;</span><span class="p">]:</span>
            <span class="k">break</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    
    <span class="c1"># 3. ASSERT</span>
    <span class="k">assert</span> <span class="n">status</span> <span class="o">==</span> <span class="s1">&#39;SUCCEEDED&#39;</span>
    
    <span class="c1"># Assert that the data was indexed correctly in OpenSearch</span>
    <span class="c1"># In a real test, you&#39;d use the opensearch_indexer client to query the index</span>
    <span class="c1"># os_client = opensearch_indexer.get_opensearch_client(...)</span>
    <span class="c1"># indexed_doc = os_client.get(index=..., id=...)</span>
    <span class="c1"># assert indexed_doc[&#39;found&#39;] == True</span>
    
    <span class="c1"># 4. CLEANUP (Optional but recommended)</span>
    <span class="n">s3_client</span><span class="o">.</span><span class="n">delete_object</span><span class="p">(</span><span class="n">Bucket</span><span class="o">=</span><span class="n">RAW_BUCKET</span><span class="p">,</span> <span class="n">Key</span><span class="o">=</span><span class="n">s3_key</span><span class="p">)</span>
    <span class="c1"># os_client.delete(index=..., id=...)</span>
</pre></div>
</div>
</section>
<section id="ci-cd-github-actions-workflow">
<h4>CI/CD GitHub Actions Workflow<a class="headerlink" href="#ci-cd-github-actions-workflow" title="Permalink to this heading">¶</a></h4>
<p><strong>.github/workflows/deploy_ingestion_pipeline.yml</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Data Ingestion Pipeline</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;ingestion_pipeline/**&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">lint-and-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Lint &amp; Unit Test</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v4</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Set up Python</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.11&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install -r ingestion_pipeline/requirements.txt</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run unit tests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest ingestion_pipeline/tests/unit/</span>

<span class="w">  </span><span class="nt">deploy-to-staging</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy to Staging</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lint-and-test</span>
<span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">staging</span>
<span class="w">    </span><span class="nt">permissions</span><span class="p">:</span>
<span class="w">      </span><span class="nt">id-token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">write</span>
<span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">read</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v4</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">role-to-assume</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AWS_ROLE_ARN }}</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.AWS_REGION }}</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Setup Terraform</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hashicorp/setup-terraform@v2</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Terraform Apply</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd ingestion_pipeline/infra</span>
<span class="w">          </span><span class="no">terraform init</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;env=staging&quot;</span>

<span class="w">  </span><span class="nt">run-integration-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Integration Tests on Staging</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deploy-to-staging</span>
<span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">staging</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (similar setup to deploy job) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run integration tests</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STATE_MACHINE_ARN</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_SFN_ARN }}</span>
<span class="w">          </span><span class="nt">RAW_BUCKET</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_RAW_BUCKET }}</span>
<span class="w">          </span><span class="nt">OS_HOST</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_OS_HOST }}</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest ingestion_pipeline/tests/integration/</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="implementation-inference-pipeline">
<h3>Implementation: Inference Pipeline<a class="headerlink" href="#implementation-inference-pipeline" title="Permalink to this heading">¶</a></h3>
<p>Here are the complete artifacts required to build, test, deploy, and automate our low-latency, real-time RAG inference service.</p>
<section id="id5">
<h4>Python Scripts (Core Logic)<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h4>
<p>The application is structured into modular components orchestrated by main.py.</p>
<p><strong>inference_service/src/main.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">Request</span><span class="p">,</span> <span class="n">HTTPException</span>
<span class="kn">from</span> <span class="nn">fastapi.responses</span> <span class="kn">import</span> <span class="n">StreamingResponse</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">orchestrator</span>
<span class="kn">from</span> <span class="nn">.config</span> <span class="kn">import</span> <span class="n">settings</span>
<span class="kn">from</span> <span class="nn">.instrumentation</span> <span class="kn">import</span> <span class="n">configure_logging</span>

<span class="c1"># Configure logging and LangSmith tracing on startup</span>
<span class="n">configure_logging</span><span class="p">()</span>
<span class="c1"># NOTE: LangSmith tracing is configured via environment variables like</span>
<span class="c1"># LANGCHAIN_TRACING_V2, LANGCHAIN_API_KEY, etc.</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">SearchRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">query</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">user_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Add other potential fields like image_url for multimodal search</span>

<span class="nd">@app</span><span class="o">.</span><span class="n">on_event</span><span class="p">(</span><span class="s2">&quot;startup&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">startup_event</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize resources on startup.&quot;&quot;&quot;</span>
    <span class="c1"># This is a good place to initialize clients that can be reused,</span>
    <span class="c1"># like the OpenSearch and Bedrock clients, to leverage connection pooling.</span>
    <span class="n">app</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">orchestrator</span> <span class="o">=</span> <span class="k">await</span> <span class="n">orchestrator</span><span class="o">.</span><span class="n">RAGOrchestrator</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Application startup complete. RAG Orchestrator initialized.&quot;</span><span class="p">)</span>

<span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/search&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">SearchRequest</span><span class="p">,</span> <span class="n">http_request</span><span class="p">:</span> <span class="n">Request</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Main endpoint for RAG-based search.</span>
<span class="sd">    Streams a response back to the client.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">request</span><span class="o">.</span><span class="n">query</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="n">status_code</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">detail</span><span class="o">=</span><span class="s2">&quot;Query cannot be empty.&quot;</span><span class="p">)</span>
        
        <span class="n">rag_orchestrator</span> <span class="o">=</span> <span class="n">http_request</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">orchestrator</span>
        
        <span class="k">async</span> <span class="k">def</span> <span class="nf">stream_generator</span><span class="p">():</span>
            <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">rag_orchestrator</span><span class="o">.</span><span class="n">stream_rag_response</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">user_id</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">chunk</span>
        
        <span class="k">return</span> <span class="n">StreamingResponse</span><span class="p">(</span><span class="n">stream_generator</span><span class="p">(),</span> <span class="n">media_type</span><span class="o">=</span><span class="s2">&quot;text/plain&quot;</span><span class="p">)</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An error occurred during search for query: &#39;</span><span class="si">{</span><span class="n">request</span><span class="o">.</span><span class="n">query</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="n">status_code</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">detail</span><span class="o">=</span><span class="s2">&quot;An internal error occurred.&quot;</span><span class="p">)</span>

<span class="nd">@app</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;/health&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">health_check</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple health check endpoint.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;ok&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p><strong>inference_service/src/orchestrator.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">AsyncGenerator</span>
<span class="kn">from</span> <span class="nn">langsmith</span> <span class="kn">import</span> <span class="n">traceable</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">retriever</span><span class="p">,</span> <span class="n">reranker</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">guardrails</span><span class="p">,</span> <span class="n">query_transformer</span>
<span class="kn">from</span> <span class="nn">.config</span> <span class="kn">import</span> <span class="n">Settings</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">RAGOrchestrator</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Orchestrates the end-to-end RAG pipeline asynchronously.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">:</span> <span class="n">Settings</span><span class="p">,</span> <span class="n">retriever_client</span><span class="p">,</span> <span class="n">reranker_client</span><span class="p">,</span> <span class="n">generator_client</span><span class="p">,</span> <span class="n">transformer_client</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">settings</span> <span class="o">=</span> <span class="n">settings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span> <span class="o">=</span> <span class="n">retriever_client</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reranker</span> <span class="o">=</span> <span class="n">reranker_client</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator_client</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">transformer_client</span>

    <span class="nd">@classmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">create</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">settings</span><span class="p">:</span> <span class="n">Settings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously create an instance of the orchestrator.&quot;&quot;&quot;</span>
        <span class="c1"># Initialize clients for dependencies</span>
        <span class="n">retriever_client</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">HybridRetriever</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">opensearch_host</span><span class="p">)</span>
        <span class="n">reranker_client</span> <span class="o">=</span> <span class="n">reranker</span><span class="o">.</span><span class="n">SageMakerReranker</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">reranker_endpoint_name</span><span class="p">)</span>
        <span class="n">generator_client</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">BedrockGenerator</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">generator_model_id</span><span class="p">)</span>
        <span class="n">transformer_client</span> <span class="o">=</span> <span class="n">query_transformer</span><span class="o">.</span><span class="n">QueryTransformer</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">hyde_model_id</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">redis_host</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">retriever_client</span><span class="p">,</span> <span class="n">reranker_client</span><span class="p">,</span> <span class="n">generator_client</span><span class="p">,</span> <span class="n">transformer_client</span><span class="p">)</span>

    <span class="nd">@traceable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;stream_rag_response&quot;</span><span class="p">)</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">stream_rag_response</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncGenerator</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Full asynchronous RAG pipeline with streaming.&quot;&quot;&quot;</span>
        
        <span class="c1"># 1. Input Guardrails &amp; Transformation (can be run concurrently)</span>
        <span class="n">guarded_query_task</span> <span class="o">=</span> <span class="n">guardrails</span><span class="o">.</span><span class="n">apply_input_guardrails</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">transformed_query_task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">transform_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        
        <span class="n">guarded_query</span><span class="p">,</span> <span class="n">transformed_query</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="n">guarded_query_task</span><span class="p">,</span> <span class="n">transformed_query_task</span>
        <span class="p">)</span>
        
        <span class="c1"># 2. Hybrid Retrieval</span>
        <span class="n">retrieved_docs</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">transformed_query</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        
        <span class="c1"># 3. Contextual Re-ranking</span>
        <span class="n">reranked_docs</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">reranker</span><span class="o">.</span><span class="n">rerank</span><span class="p">(</span><span class="n">guarded_query</span><span class="p">,</span> <span class="n">retrieved_docs</span><span class="p">,</span> <span class="n">user_id</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        
        <span class="c1"># 4. Prompt Construction and Generation</span>
        <span class="n">final_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">construct_prompt</span><span class="p">(</span><span class="n">guarded_query</span><span class="p">,</span> <span class="n">reranked_docs</span><span class="p">)</span>
        
        <span class="c1"># 5. Streaming Generation and Output Guardrails</span>
        <span class="n">token_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">stream_response</span><span class="p">(</span><span class="n">final_prompt</span><span class="p">)</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">guardrails</span><span class="o">.</span><span class="n">apply_output_guardrails</span><span class="p">(</span><span class="n">token_stream</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">token</span>
</pre></div>
</div>
</section>
<section id="id6">
<h4>Unit Tests<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h4>
<p><strong>inference_service/tests/unit/test_orchestrator.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">AsyncMock</span>

<span class="kn">from</span> <span class="nn">src.orchestrator</span> <span class="kn">import</span> <span class="n">RAGOrchestrator</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">asyncio</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">test_orchestrator_full_flow</span><span class="p">(</span><span class="n">mocker</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests the full orchestration flow with mocked dependencies.&quot;&quot;&quot;</span>
    <span class="c1"># ARRANGE: Mock all external clients and their async methods</span>
    <span class="n">mock_retriever</span> <span class="o">=</span> <span class="n">AsyncMock</span><span class="p">()</span>
    <span class="n">mock_reranker</span> <span class="o">=</span> <span class="n">AsyncMock</span><span class="p">()</span>
    <span class="n">mock_generator</span> <span class="o">=</span> <span class="n">AsyncMock</span><span class="p">()</span>
    <span class="n">mock_transformer</span> <span class="o">=</span> <span class="n">AsyncMock</span><span class="p">()</span>

    <span class="n">mock_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;page_content&quot;</span><span class="p">:</span> <span class="s2">&quot;doc1&quot;</span><span class="p">}]</span>
    <span class="n">mock_reranker</span><span class="o">.</span><span class="n">rerank</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;page_content&quot;</span><span class="p">:</span> <span class="s2">&quot;reranked_doc1&quot;</span><span class="p">}]</span>
    <span class="n">mock_generator</span><span class="o">.</span><span class="n">stream_response</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;This&quot;</span><span class="p">,</span> <span class="s2">&quot; is&quot;</span><span class="p">,</span> <span class="s2">&quot; a&quot;</span><span class="p">,</span> <span class="s2">&quot; test.&quot;</span><span class="p">])</span> <span class="c1"># Async generator mock</span>
    <span class="n">mock_transformer</span><span class="o">.</span><span class="n">transform_query</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="s2">&quot;transformed query&quot;</span>
    
    <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s1">&#39;src.guardrails.apply_input_guardrails&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="s2">&quot;safe query&quot;</span><span class="p">)</span>
    <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s1">&#39;src.guardrails.apply_output_guardrails&#39;</span><span class="p">,</span> <span class="n">side_effect</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># Pass-through mock</span>

    <span class="n">orchestrator_instance</span> <span class="o">=</span> <span class="n">RAGOrchestrator</span><span class="p">(</span>
        <span class="n">settings</span><span class="o">=</span><span class="n">mocker</span><span class="o">.</span><span class="n">Mock</span><span class="p">(),</span>
        <span class="n">retriever_client</span><span class="o">=</span><span class="n">mock_retriever</span><span class="p">,</span>
        <span class="n">reranker_client</span><span class="o">=</span><span class="n">mock_reranker</span><span class="p">,</span>
        <span class="n">generator_client</span><span class="o">=</span><span class="n">mock_generator</span><span class="p">,</span>
        <span class="n">transformer_client</span><span class="o">=</span><span class="n">mock_transformer</span>
    <span class="p">)</span>

    <span class="c1"># ACT: Run the orchestrator</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;test query&quot;</span>
    <span class="n">result_stream</span> <span class="o">=</span> <span class="n">orchestrator_instance</span><span class="o">.</span><span class="n">stream_rag_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="s2">&quot;user123&quot;</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">token</span> <span class="k">async</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">result_stream</span><span class="p">])</span>

    <span class="c1"># ASSERT: Verify that all components were called correctly</span>
    <span class="n">mock_transformer</span><span class="o">.</span><span class="n">transform_query</span><span class="o">.</span><span class="n">assert_awaited_once_with</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">mock_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="o">.</span><span class="n">assert_awaited_once_with</span><span class="p">(</span><span class="s2">&quot;transformed query&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">mock_reranker</span><span class="o">.</span><span class="n">rerank</span><span class="o">.</span><span class="n">assert_awaited_once</span><span class="p">()</span>
    <span class="n">mock_generator</span><span class="o">.</span><span class="n">construct_prompt</span><span class="o">.</span><span class="n">assert_called_once</span><span class="p">()</span>
    <span class="n">mock_generator</span><span class="o">.</span><span class="n">stream_response</span><span class="o">.</span><span class="n">assert_called_once</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">result</span> <span class="o">==</span> <span class="s2">&quot;This is a test.&quot;</span>
</pre></div>
</div>
</section>
<section id="id7">
<h4>Infrastructure as Code (Terraform)<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h4>
<p><strong>inference_service/infra/ecs.tf</strong></p>
<div class="highlight-hcl notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defines the AWS Fargate service for our FastAPI application</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_ecs_cluster&quot;</span><span class="w"> </span><span class="nv">&quot;main&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-inference-cluster-${var.env}&quot;</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_ecs_task_definition&quot;</span><span class="w"> </span><span class="nv">&quot;api&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">family</span><span class="w">                   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-api-task&quot;</span>
<span class="w">  </span><span class="na">network_mode</span><span class="w">             </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;awsvpc&quot;</span>
<span class="w">  </span><span class="na">requires_compatibilities</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;FARGATE&quot;</span><span class="p">]</span>
<span class="w">  </span><span class="na">cpu</span><span class="w">                      </span><span class="o">=</span><span class="w"> </span><span class="m">1024</span><span class="c1"> # 1 vCPU</span>
<span class="w">  </span><span class="na">memory</span><span class="w">                   </span><span class="o">=</span><span class="w"> </span><span class="m">2048</span><span class="c1"> # 2 GB</span>

<span class="c1">  # ... (container definition pointing to the ECR image, port mappings, etc.) ...</span>
<span class="c1">  </span>
<span class="c1">  # IAM role for the task to access Bedrock, SageMaker, OpenSearch</span>
<span class="w">  </span><span class="na">task_role_arn</span><span class="w">            </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.inference_task_role.arn</span>
<span class="w">  </span><span class="na">execution_role_arn</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.ecs_execution_role.arn</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_ecs_service&quot;</span><span class="w"> </span><span class="nv">&quot;main&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">            </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-inference-service-${var.env}&quot;</span>
<span class="w">  </span><span class="na">cluster</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_ecs_cluster.main.id</span>
<span class="w">  </span><span class="na">task_definition</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_ecs_task_definition.api.arn</span>
<span class="w">  </span><span class="na">desired_count</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="c1"> # Start with 2 tasks for high availability</span>
<span class="w">  </span><span class="na">launch_type</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;FARGATE&quot;</span>
<span class="c1">  </span>
<span class="c1">  # ... (network configuration, load balancer attachment) ...</span>
<span class="p">}</span>

<span class="c1"># ... (Autoscaling configuration based on CPU or request count) ...</span>
</pre></div>
</div>
</section>
<section id="id8">
<h4>Integration Test<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h4>
<p><strong>inference_service/tests/integration/test_api.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># API_ENDPOINT is the URL of the deployed staging service</span>
<span class="n">API_ENDPOINT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_API_ENDPOINT&quot;</span><span class="p">]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_search_endpoint_returns_success</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that the deployed API returns a successful response.&quot;&quot;&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">API_ENDPOINT</span><span class="si">}</span><span class="s2">/search&quot;</span><span class="p">,</span>
        <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;what are the best hiking boots?&quot;</span><span class="p">},</span>
        <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span>
    
    <span class="c1"># Assert that we get some streamed content back</span>
    <span class="n">content</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">iter_content</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">content</span> <span class="o">+=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">content</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
</pre></div>
</div>
<p><strong>inference_service/tests/load/locustfile.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">locust</span> <span class="kn">import</span> <span class="n">HttpUser</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">between</span>

<span class="k">class</span> <span class="nc">RAGUser</span><span class="p">(</span><span class="n">HttpUser</span><span class="p">):</span>
    <span class="n">wait_time</span> <span class="o">=</span> <span class="n">between</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># Simulate user think time</span>

    <span class="nd">@task</span>
    <span class="k">def</span> <span class="nf">search_query</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">}</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;waterproof trail running shoes for wide feet&quot;</span><span class="p">,</span>
            <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;locust_user_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">runner</span><span class="o">.</span><span class="n">user_count</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/search&quot;</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;/search&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id9">
<h4>CI/CD GitHub Actions Workflow<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h4>
<p><strong>.github/workflows/deploy_inference_service.yml</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy RAG Inference Service</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;inference_service/**&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># ... (lint-and-test job as before) ...</span>
<span class="w">  </span>
<span class="w">  </span><span class="nt">build-and-push-image</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Build &amp; Push Docker Image</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lint-and-test</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (checkout code) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS Credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v4</span>
<span class="w">        </span><span class="c1"># ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Login to Amazon ECR</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/amazon-ecr-login@v2</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Build, tag, and push image to Amazon ECR</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">docker build -t ${{ secrets.ECR_REPOSITORY_URI }}:${{ github.sha }} ./inference_service</span>
<span class="w">          </span><span class="no">docker push ${{ secrets.ECR_REPOSITORY_URI }}:${{ github.sha }}</span>

<span class="w">  </span><span class="nt">deploy-to-staging</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy to Staging</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">build-and-push-image</span>
<span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">staging</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (checkout code, configure AWS creds, setup Terraform) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Terraform Apply Staging</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd inference_service/infra</span>
<span class="w">          </span><span class="no">terraform init</span>
<span class="w">          </span><span class="no"># Pass the new image tag to the deployment</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;env=staging&quot; -var=&quot;image_tag=${{ github.sha }}&quot;</span>

<span class="w">  </span><span class="nt">run-staging-tests</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Integration &amp; Load Tests</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deploy-to-staging</span>
<span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">staging</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (setup, install dependencies) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run integration tests</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_API_ENDPOINT</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_API_ENDPOINT }}</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest inference_service/tests/integration/</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run load tests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">locust -f inference_service/tests/load/locustfile.py --host ${{ secrets.STAGING_API_ENDPOINT }} --headless -u 10 -r 2 --run-time 1m</span>

<span class="w">  </span><span class="c1"># ... (Manual approval gate and deploy-to-production jobs as planned) ...</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="implementation-the-monitoring-and-observability-pipeline">
<h3>Implementation: The Monitoring and Observability Pipeline<a class="headerlink" href="#implementation-the-monitoring-and-observability-pipeline" title="Permalink to this heading">¶</a></h3>
<p>This is not a traditional batch pipeline but a continuous, event-driven streaming architecture. The artifacts below define the instrumentation, infrastructure, and automation required to collect, process, and act on observability data in real-time.</p>
<section id="id10">
<h4>Python Scripts (Core Logic)<a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h4>
<p>The core logic is broken down into modular scripts, each responsible for a specific stage of the pipeline. These scripts would be packaged and deployed as AWS Lambda functions or run within an AWS Glue job.</p>
<p><strong>monitoring/src/instrumentation_lib.py</strong></p>
<p>(This library would be packaged and included as a dependency in the Inference Service)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">from</span> <span class="nn">uuid</span> <span class="kn">import</span> <span class="n">uuid4</span>

<span class="c1"># Use a custom JSON formatter for structured logging</span>
<span class="k">class</span> <span class="nc">JsonFormatter</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">Formatter</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
        <span class="n">log_record</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;timestamp&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">formatTime</span><span class="p">(</span><span class="n">record</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">datefmt</span><span class="p">),</span>
            <span class="s2">&quot;level&quot;</span><span class="p">:</span> <span class="n">record</span><span class="o">.</span><span class="n">levelname</span><span class="p">,</span>
            <span class="s2">&quot;message&quot;</span><span class="p">:</span> <span class="n">record</span><span class="o">.</span><span class="n">getMessage</span><span class="p">(),</span>
            <span class="s2">&quot;trace_id&quot;</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">record</span><span class="p">,</span> <span class="s2">&quot;trace_id&quot;</span><span class="p">,</span> <span class="s2">&quot;N/A&quot;</span><span class="p">),</span>
            <span class="s2">&quot;service&quot;</span><span class="p">:</span> <span class="s2">&quot;RAGInferenceService&quot;</span>
        <span class="p">}</span>
        <span class="c1"># Add exception info if it exists</span>
        <span class="k">if</span> <span class="n">record</span><span class="o">.</span><span class="n">exc_info</span><span class="p">:</span>
            <span class="n">log_record</span><span class="p">[</span><span class="s1">&#39;exception&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">formatException</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">exc_info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">log_record</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">configure_logging</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configures root logger for structured JSON logging.&quot;&quot;&quot;</span>
    <span class="c1"># Remove any existing handlers</span>
    <span class="k">for</span> <span class="n">handler</span> <span class="ow">in</span> <span class="n">logging</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">handlers</span><span class="p">[:]:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">removeHandler</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
        
    <span class="n">handler</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">StreamHandler</span><span class="p">()</span>
    <span class="n">handler</span><span class="o">.</span><span class="n">setFormatter</span><span class="p">(</span><span class="n">JsonFormatter</span><span class="p">())</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">handlers</span><span class="o">=</span><span class="p">[</span><span class="n">handler</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">get_trace_id</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates a unique trace ID.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid4</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">emit_cloudwatch_metric</span><span class="p">(</span><span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">unit</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Milliseconds&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Emits a custom metric to CloudWatch.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">cloudwatch</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s1">&#39;cloudwatch&#39;</span><span class="p">)</span>
        <span class="n">cloudwatch</span><span class="o">.</span><span class="n">put_metric_data</span><span class="p">(</span>
            <span class="n">Namespace</span><span class="o">=</span><span class="s1">&#39;RAGApplication&#39;</span><span class="p">,</span>
            <span class="n">MetricData</span><span class="o">=</span><span class="p">[</span>
                <span class="p">{</span>
                    <span class="s1">&#39;MetricName&#39;</span><span class="p">:</span> <span class="n">metric_name</span><span class="p">,</span>
                    <span class="s1">&#39;Value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">,</span>
                    <span class="s1">&#39;Unit&#39;</span><span class="p">:</span> <span class="n">unit</span><span class="p">,</span>
                    <span class="s1">&#39;Dimensions&#39;</span><span class="p">:</span> <span class="p">[</span>
                        <span class="p">{</span>
                            <span class="s1">&#39;Name&#39;</span><span class="p">:</span> <span class="s1">&#39;Service&#39;</span><span class="p">,</span>
                            <span class="s1">&#39;Value&#39;</span><span class="p">:</span> <span class="s1">&#39;RAGInferenceService&#39;</span>
                        <span class="p">}</span>
                    <span class="p">]</span>
                <span class="p">},</span>
            <span class="p">]</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1"># Log error but don&#39;t fail the main application path</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to emit CloudWatch metric &#39;</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>monitoring/src/log_processing_lambda.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">handler</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processes logs from Kinesis Firehose, enriches them,</span>
<span class="sd">    and returns them for storage in S3.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_records</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">event</span><span class="p">[</span><span class="s1">&#39;records&#39;</span><span class="p">]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Decode the payload from Firehose</span>
            <span class="n">payload_decoded</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">record</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
            <span class="n">log_data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">payload_decoded</span><span class="p">)</span>

            <span class="c1"># 1. PARSE: The data is already structured JSON.</span>
            <span class="c1"># 2. ENRICH: Add additional metadata.</span>
            <span class="c1">#    (e.g., lookup user details from a DB using log_data[&#39;user_id&#39;])</span>
            <span class="n">log_data</span><span class="p">[</span><span class="s1">&#39;processed_by&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">function_name</span>
            
            <span class="c1"># 3. AGGREGATE/ANALYZE (Example):</span>
            <span class="c1"># In a more complex scenario, you could send a sample of responses</span>
            <span class="c1"># to another service (e.g., an LLM-as-a-judge) here to calculate a</span>
            <span class="c1"># groundedness score before archiving.</span>
            
            <span class="n">processed_payload</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">log_data</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
            
            <span class="n">output_records</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s1">&#39;recordId&#39;</span><span class="p">:</span> <span class="n">record</span><span class="p">[</span><span class="s1">&#39;recordId&#39;</span><span class="p">],</span>
                <span class="s1">&#39;result&#39;</span><span class="p">:</span> <span class="s1">&#39;Ok&#39;</span><span class="p">,</span>
                <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">processed_payload</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
            <span class="p">})</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to process record </span><span class="si">{</span><span class="n">record</span><span class="p">[</span><span class="s1">&#39;recordId&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">output_records</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s1">&#39;recordId&#39;</span><span class="p">:</span> <span class="n">record</span><span class="p">[</span><span class="s1">&#39;recordId&#39;</span><span class="p">],</span>
                <span class="s1">&#39;result&#39;</span><span class="p">:</span> <span class="s1">&#39;ProcessingFailed&#39;</span><span class="p">,</span>
                <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="n">record</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span> <span class="c1"># Return original data on failure</span>
            <span class="p">})</span>

    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;records&#39;</span><span class="p">:</span> <span class="n">output_records</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id11">
<h4>Unit Tests<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h4>
<p><strong>monitoring/tests/unit/test_log_processing_lambda.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">src</span> <span class="kn">import</span> <span class="n">log_processing_lambda</span>

<span class="k">def</span> <span class="nf">test_lambda_handler_processes_records</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that the Lambda handler correctly processes a Kinesis Firehose event.&quot;&quot;&quot;</span>
    <span class="c1"># ARRANGE: Create a sample Kinesis Firehose event</span>
    <span class="n">log_event</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;timestamp&quot;</span><span class="p">:</span> <span class="s2">&quot;2025-08-08T10:00:00Z&quot;</span><span class="p">,</span>
        <span class="s2">&quot;level&quot;</span><span class="p">:</span> <span class="s2">&quot;INFO&quot;</span><span class="p">,</span>
        <span class="s2">&quot;message&quot;</span><span class="p">:</span> <span class="s2">&quot;Search successful&quot;</span><span class="p">,</span>
        <span class="s2">&quot;trace_id&quot;</span><span class="p">:</span> <span class="s2">&quot;123-abc&quot;</span>
    <span class="p">}</span>
    <span class="n">event_data</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">log_event</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    
    <span class="n">kinesis_event</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;records&#39;</span><span class="p">:</span> <span class="p">[{</span>
            <span class="s1">&#39;recordId&#39;</span><span class="p">:</span> <span class="s1">&#39;4964251234&#39;</span><span class="p">,</span>
            <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="n">event_data</span>
        <span class="p">}]</span>
    <span class="p">}</span>

    <span class="c1"># ACT: Call the handler</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">log_processing_lambda</span><span class="o">.</span><span class="n">handler</span><span class="p">(</span><span class="n">kinesis_event</span><span class="p">,</span> <span class="p">{})</span>

    <span class="c1"># ASSERT: Check the result structure and content</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;records&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">record</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;records&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">record</span><span class="p">[</span><span class="s1">&#39;result&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Ok&#39;</span>
    
    <span class="c1"># Decode the processed data to verify enrichment</span>
    <span class="n">processed_data_decoded</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">record</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    <span class="n">processed_data_json</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">processed_data_decoded</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">processed_data_json</span><span class="p">[</span><span class="s1">&#39;trace_id&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;123-abc&quot;</span>
    <span class="k">assert</span> <span class="s1">&#39;processed_by&#39;</span> <span class="ow">in</span> <span class="n">processed_data_json</span>
</pre></div>
</div>
</section>
<section id="id12">
<h4>Infrastructure as Code (Terraform)<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h4>
<p><strong>monitoring/infra/main.tf</strong></p>
<div class="highlight-hcl notranslate"><div class="highlight"><pre><span></span><span class="c1"># main.tf - Defines the infrastructure for the observability pipeline</span>

<span class="c1"># --- CloudWatch Resources ---</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_cloudwatch_log_group&quot;</span><span class="w"> </span><span class="nv">&quot;inference_service_logs&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;/aws/ecs/rag-inference-service-${var.env}&quot;</span>
<span class="w">  </span><span class="na">retention_in_days</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">30</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_cloudwatch_dashboard&quot;</span><span class="w"> </span><span class="nv">&quot;rag_dashboard&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">dashboard_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;RAG-Observability-Dashboard-${var.env}&quot;</span>
<span class="w">  </span><span class="na">dashboard_body</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">jsonencode</span><span class="p">({</span>
<span class="c1">    # ... (Dashboard widget definitions for Latency, Cost, RAG Quality metrics) ...</span>
<span class="w">  </span><span class="p">})</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_cloudwatch_metric_alarm&quot;</span><span class="w"> </span><span class="nv">&quot;p99_latency_alarm&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">alarm_name</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;High-P99-Latency-Alarm-${var.env}&quot;</span>
<span class="w">  </span><span class="na">comparison_operator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;GreaterThanOrEqualToThreshold&quot;</span>
<span class="w">  </span><span class="na">evaluation_periods</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;1&quot;</span>
<span class="w">  </span><span class="na">metric_name</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;p99_latency&quot;</span><span class="c1"> # Custom metric emitted by the app</span>
<span class="w">  </span><span class="na">namespace</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;RAGApplication&quot;</span>
<span class="w">  </span><span class="na">period</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;60&quot;</span>
<span class="w">  </span><span class="na">statistic</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Average&quot;</span>
<span class="w">  </span><span class="na">threshold</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;1500&quot;</span><span class="c1"> # 1.5 seconds</span>
<span class="w">  </span><span class="na">alarm_description</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;P99 latency for the RAG inference service is too high.&quot;</span>
<span class="w">  </span><span class="na">alarm_actions</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="nv">aws_sns_topic.alarms.arn</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># --- Kinesis Firehose for Log Streaming ---</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_kinesis_firehose_delivery_stream&quot;</span><span class="w"> </span><span class="nv">&quot;log_stream&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-log-stream-${var.env}&quot;</span>
<span class="w">  </span><span class="na">destination</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;extended_s3&quot;</span>

<span class="w">  </span><span class="nb">extended_s3_configuration</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">bucket_arn</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_s3_bucket.log_archive.arn</span>
<span class="w">    </span><span class="na">role_arn</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.firehose_role.arn</span>
<span class="c1">    </span>
<span class="c1">    # Route logs through our Lambda for processing</span>
<span class="w">    </span><span class="nb">processing_configuration</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="na">enabled</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;true&quot;</span>
<span class="w">      </span><span class="nb">processors</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="na">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Lambda&quot;</span>
<span class="w">        </span><span class="nb">parameters</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="na">parameter_name</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;LambdaArn&quot;</span>
<span class="w">          </span><span class="na">parameter_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_lambda_function.log_processor.arn</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1"># Subscription to send logs from CloudWatch to Kinesis</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_cloudwatch_log_subscription_filter&quot;</span><span class="w"> </span><span class="nv">&quot;log_subscription&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">            </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;KinesisSubscriptionFilter-${var.env}&quot;</span>
<span class="w">  </span><span class="na">log_group_name</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_cloudwatch_log_group.inference_service_logs.name</span>
<span class="w">  </span><span class="na">filter_pattern</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="c1"> # Send all logs</span>
<span class="w">  </span><span class="na">destination_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_kinesis_firehose_delivery_stream.log_stream.arn</span>
<span class="w">  </span><span class="na">role_arn</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.cloudwatch_to_firehose_role.arn</span>
<span class="p">}</span>

<span class="c1"># --- Supporting Resources (S3, Lambda, IAM, SNS) ---</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_s3_bucket&quot;</span><span class="w"> </span><span class="nv">&quot;log_archive&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">bucket</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-log-archive-${var.env}&quot;</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_lambda_function&quot;</span><span class="w"> </span><span class="nv">&quot;log_processor&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">function_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;LogProcessorLambda-${var.env}&quot;</span>
<span class="c1">  # ... (configuration for the log processing lambda) ...</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sns_topic&quot;</span><span class="w"> </span><span class="nv">&quot;alarms&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;RAG-Alarms-Topic-${var.env}&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id13">
<h4>Integration Test<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h4>
<p><strong>monitoring/tests/integration/test_monitoring_pipeline.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gzip</span>

<span class="n">API_ENDPOINT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_API_ENDPOINT&quot;</span><span class="p">]</span>
<span class="n">LOG_ARCHIVE_BUCKET</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;STAGING_LOG_BUCKET&quot;</span><span class="p">]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_end_to_end_logging_flow</span><span class="p">():</span>
    <span class="n">s3_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;s3&quot;</span><span class="p">)</span>
    
    <span class="c1"># 1. ARRANGE: Generate a unique ID to find in the logs</span>
    <span class="n">unique_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;integration-test-</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="c1"># 2. ACT: Make a request to the service that will generate a log</span>
    <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">API_ENDPOINT</span><span class="si">}</span><span class="s2">/search&quot;</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">unique_id</span><span class="p">})</span>
    
    <span class="c1"># 3. ASSERT: Poll the S3 bucket until the log file appears</span>
    <span class="n">found_log</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">):</span> <span class="c1"># Poll for up to 1 minute</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="c1"># Construct prefix based on Firehose date partitioning</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y/%m/</span><span class="si">%d</span><span class="s1">/%H&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">/&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">s3_client</span><span class="o">.</span><span class="n">list_objects_v2</span><span class="p">(</span><span class="n">Bucket</span><span class="o">=</span><span class="n">LOG_ARCHIVE_BUCKET</span><span class="p">,</span> <span class="n">Prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="s1">&#39;Contents&#39;</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;Contents&#39;</span><span class="p">]:</span>
                <span class="n">log_obj</span> <span class="o">=</span> <span class="n">s3_client</span><span class="o">.</span><span class="n">get_object</span><span class="p">(</span><span class="n">Bucket</span><span class="o">=</span><span class="n">LOG_ARCHIVE_BUCKET</span><span class="p">,</span> <span class="n">Key</span><span class="o">=</span><span class="n">obj</span><span class="p">[</span><span class="s1">&#39;Key&#39;</span><span class="p">])</span>
                <span class="n">log_content</span> <span class="o">=</span> <span class="n">gzip</span><span class="o">.</span><span class="n">decompress</span><span class="p">(</span><span class="n">log_obj</span><span class="p">[</span><span class="s1">&#39;Body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">read</span><span class="p">())</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">unique_id</span> <span class="ow">in</span> <span class="n">log_content</span><span class="p">:</span>
                    <span class="n">found_log</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="c1"># Check if the log was enriched by our Lambda</span>
                    <span class="k">assert</span> <span class="s1">&#39;processed_by&#39;</span> <span class="ow">in</span> <span class="n">log_content</span>
                    <span class="k">break</span>
        <span class="k">if</span> <span class="n">found_log</span><span class="p">:</span>
            <span class="k">break</span>
            
    <span class="k">assert</span> <span class="n">found_log</span><span class="p">,</span> <span class="s2">&quot;Log file with unique ID was not found in S3 within 60 seconds.&quot;</span>
</pre></div>
</div>
</section>
<section id="id14">
<h4>CI/CD GitHub Actions Workflow<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h4>
<p><strong>.github/workflows/deploy_monitoring_infra.yml</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Monitoring Infrastructure</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;monitoring/**&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># ... (lint-and-test job for monitoring/src/*.py as before) ...</span>

<span class="w">  </span><span class="nt">deploy-to-staging</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Monitoring Infra to Staging</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lint-and-test</span>
<span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">staging</span>
<span class="w">    </span><span class="nt">permissions</span><span class="p">:</span>
<span class="w">      </span><span class="nt">id-token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">write</span>
<span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">read</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (checkout, configure AWS creds, setup Terraform) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Terraform Apply Monitoring</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd monitoring/infra</span>
<span class="w">          </span><span class="no">terraform init</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;env=staging&quot;</span>
<span class="w">  </span>
<span class="w">  </span><span class="nt">run-integration-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Monitoring Integration Test</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deploy-to-staging</span>
<span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">staging</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (setup, configure AWS creds, install dependencies) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run E2E Monitoring Test</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_API_ENDPOINT</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_API_ENDPOINT }}</span>
<span class="w">          </span><span class="nt">STAGING_LOG_BUCKET</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;rag-log-archive-staging&quot;</span><span class="w"> </span><span class="c1"># This could also be a secret or TF output</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest monitoring/tests/integration/</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="implementation-testing-in-production">
<h3>Implementation: Testing in Production<a class="headerlink" href="#implementation-testing-in-production" title="Permalink to this heading">¶</a></h3>
<section id="python-scripts-analysis">
<h4>Python Scripts (Analysis)<a class="headerlink" href="#python-scripts-analysis" title="Permalink to this heading">¶</a></h4>
<p><strong>production_testing/analysis/ab_test_analysis.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2_contingency</span><span class="p">,</span> <span class="n">ttest_ind</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> - </span><span class="si">%(levelname)s</span><span class="s1"> - </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">analyze_conversion_rate</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">control_name</span><span class="o">=</span><span class="s1">&#39;control&#39;</span><span class="p">,</span> <span class="n">challenger_name</span><span class="o">=</span><span class="s1">&#39;challenger&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Performs a Chi-squared test for conversion rates.&quot;&quot;&quot;</span>
    <span class="n">contingency_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;variant_id&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;converted&#39;</span><span class="p">])</span>
    
    <span class="k">if</span> <span class="n">control_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">contingency_table</span><span class="o">.</span><span class="n">index</span> <span class="ow">or</span> <span class="n">challenger_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">contingency_table</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Control or challenger variant not found in the data.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="n">chi2</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">chi2_contingency</span><span class="p">(</span><span class="n">contingency_table</span><span class="p">)</span>
    
    <span class="n">control_conv_rate</span> <span class="o">=</span> <span class="n">contingency_table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">control_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">]</span> <span class="o">/</span> <span class="n">contingency_table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">control_name</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">challenger_conv_rate</span> <span class="o">=</span> <span class="n">contingency_table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">challenger_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">]</span> <span class="o">/</span> <span class="n">contingency_table</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">challenger_name</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Control Conversion Rate: </span><span class="si">{</span><span class="n">control_conv_rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Challenger Conversion Rate: </span><span class="si">{</span><span class="n">challenger_conv_rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chi-squared p-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">control_conv_rate</span><span class="p">,</span> <span class="n">challenger_conv_rate</span><span class="p">,</span> <span class="n">p_value</span>

<span class="k">def</span> <span class="nf">analyze_aov</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">control_name</span><span class="o">=</span><span class="s1">&#39;control&#39;</span><span class="p">,</span> <span class="n">challenger_name</span><span class="o">=</span><span class="s1">&#39;challenger&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Performs an independent T-test for Average Order Value.&quot;&quot;&quot;</span>
    <span class="n">control_aov</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;variant_id&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">control_name</span><span class="p">][</span><span class="s1">&#39;order_value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
    <span class="n">challenger_aov</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;variant_id&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">challenger_name</span><span class="p">][</span><span class="s1">&#39;order_value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">control_aov</span><span class="o">.</span><span class="n">empty</span> <span class="ow">or</span> <span class="n">challenger_aov</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;No order data for one or both variants.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">control_aov</span><span class="p">,</span> <span class="n">challenger_aov</span><span class="p">,</span> <span class="n">equal_var</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Welch&#39;s T-test</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Control AOV: $</span><span class="si">{</span><span class="n">control_aov</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Challenger AOV: $</span><span class="si">{</span><span class="n">challenger_aov</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;T-test p-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">control_aov</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">challenger_aov</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">p_value</span>

<span class="k">def</span> <span class="nf">generate_report</span><span class="p">(</span><span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads data and generates a full A/B test report.&quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading experiment data from </span><span class="si">{</span><span class="n">data_path</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
    <span class="c1"># In a real scenario, this would connect to a data warehouse like Redshift or BigQuery.</span>
    <span class="c1"># For this example, we&#39;ll use a CSV.</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data file not found at </span><span class="si">{</span><span class="n">data_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Conversion Rate Analysis ---&quot;</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">conv_p_value</span> <span class="o">=</span> <span class="n">analyze_conversion_rate</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">conv_p_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">conv_p_value</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Result: Statistically significant difference in conversion rates found.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Result: No statistically significant difference in conversion rates.&quot;</span><span class="p">)</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Average Order Value (AOV) Analysis ---&quot;</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">aov_p_value</span> <span class="o">=</span> <span class="n">analyze_aov</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">aov_p_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">aov_p_value</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Result: Statistically significant difference in AOV found.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Result: No statistically significant difference in AOV.&quot;</span><span class="p">)</span>

    <span class="c1"># --- Final Recommendation Logic ---</span>
    <span class="c1"># (This would be more sophisticated in a real business context)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Recommendation ---&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">conv_p_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">conv_p_value</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Promote Challenger: Strong evidence of impact on conversion.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Decision: Continue experiment or conclude with no significant finding.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Example usage:</span>
    <span class="c1"># python ab_test_analysis.py data/experiment_results.csv</span>
    <span class="kn">import</span> <span class="nn">sys</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">generate_report</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Please provide the path to the experiment data CSV.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id15">
<h4>Unit Tests<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h4>
<p><strong>production_testing/tests/unit/test_ab_test_analysis.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">analysis</span> <span class="kn">import</span> <span class="n">ab_test_analysis</span>

<span class="k">def</span> <span class="nf">create_sample_data</span><span class="p">(</span><span class="n">control_users</span><span class="p">,</span> <span class="n">control_conversions</span><span class="p">,</span> <span class="n">challenger_users</span><span class="p">,</span> <span class="n">challenger_conversions</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to create a sample DataFrame for testing.&quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;variant_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;control&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">control_users</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;challenger&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">challenger_users</span><span class="p">,</span>
        <span class="s1">&#39;converted&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">*</span> <span class="n">control_conversions</span> <span class="o">+</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">control_users</span> <span class="o">-</span> <span class="n">control_conversions</span><span class="p">)</span> <span class="o">+</span> \
                     <span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">*</span> <span class="n">challenger_conversions</span> <span class="o">+</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">challenger_users</span> <span class="o">-</span> <span class="n">challenger_conversions</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_conversion_rate_significant_difference</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test case where there is a clear, statistically significant difference.&quot;&quot;&quot;</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">create_sample_data</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">600</span><span class="p">)</span> <span class="c1"># 5% vs 6% conversion</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ab_test_analysis</span><span class="o">.</span><span class="n">analyze_conversion_rate</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span>

<span class="k">def</span> <span class="nf">test_conversion_rate_no_difference</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test case where there is no significant difference.&quot;&quot;&quot;</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">create_sample_data</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">505</span><span class="p">)</span> <span class="c1"># 5.0% vs 5.05%</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ab_test_analysis</span><span class="o">.</span><span class="n">analyze_conversion_rate</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="n">p_value</span> <span class="o">&gt;</span> <span class="mf">0.05</span>
</pre></div>
</div>
</section>
<section id="id16">
<h4>Integration Test<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h4>
<p><strong>production_testing/tests/integration/test_ab_routing.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># PRODUCTION_API_ENDPOINT is the URL of the live, production service</span>
<span class="n">API_ENDPOINT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PRODUCTION_API_ENDPOINT&quot;</span><span class="p">]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">production</span>
<span class="k">def</span> <span class="nf">test_traffic_splitting_distribution</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Validates that the production traffic split is working as configured.&quot;&quot;&quot;</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">num_requests</span> <span class="o">=</span> <span class="mi">1000</span>
    
    <span class="c1"># 1. ACTION: Make 1000 requests to the production endpoint</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_requests</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># The inference service should be configured to return a header</span>
            <span class="c1"># indicating which version served the request.</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">API_ENDPOINT</span><span class="si">}</span><span class="s2">/search&quot;</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;test&quot;</span><span class="p">},</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span> <span class="ow">and</span> <span class="s1">&#39;X-Variant-Version&#39;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="p">:</span>
                <span class="n">responses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="p">[</span><span class="s1">&#39;X-Variant-Version&#39;</span><span class="p">])</span>
        <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">RequestException</span><span class="p">:</span>
            <span class="c1"># Ignore timeouts or errors for this specific test</span>
            <span class="k">pass</span>

    <span class="c1"># 2. COLLECT: Count the responses from each variant</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">responses</span><span class="p">)</span>
    <span class="n">control_count</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;control&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">challenger_count</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;challenger&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="n">total_responses</span> <span class="o">=</span> <span class="n">control_count</span> <span class="o">+</span> <span class="n">challenger_count</span>
    <span class="k">assert</span> <span class="n">total_responses</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Did not receive any valid responses from the API.&quot;</span>

    <span class="n">challenger_percentage</span> <span class="o">=</span> <span class="p">(</span><span class="n">challenger_count</span> <span class="o">/</span> <span class="n">total_responses</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
    
    <span class="c1"># 3. ASSERT: Check if the distribution is within tolerance (e.g., 5% +/- 2%)</span>
    <span class="n">expected_challenger_weight</span> <span class="o">=</span> <span class="mf">5.0</span>
    <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">2.0</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Observed challenger traffic: </span><span class="si">{</span><span class="n">challenger_percentage</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">challenger_percentage</span> <span class="o">-</span> <span class="n">expected_challenger_weight</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span>
</pre></div>
</div>
</section>
<section id="id17">
<h4>CI/CD GitHub Actions Workflow<a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h4>
<p><strong>.github/workflows/deploy_inference_service.yml (Deploy to Production)</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="nt">deploy-to-production</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Canary Deploy to Production</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">run-staging-tests</span>
<span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">production</span>
<span class="w">    </span><span class="c1"># ... (permissions, secrets, etc.) ...</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (checkout, configure AWS creds, setup Terraform) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1. Deploy Challenger Service</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd inference_service/infra</span>
<span class="w">          </span><span class="no">terraform init</span>
<span class="w">          </span><span class="no"># Deploy the new container as the &quot;challenger&quot; service</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;env=production&quot; -var=&quot;image_tag=${{ github.sha }}&quot; -target=aws_ecs_service.challenger</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2. Shift 5% Traffic to Challenger</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd inference_service/infra</span>
<span class="w">          </span><span class="no"># Update the ALB / API Gateway to send 5% of traffic</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;env=production&quot; -var=&quot;challenger_weight=5&quot;</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3. Monitor Canary Health</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no"># This script would query CloudWatch for challenger&#39;s error rate and latency</span>
<span class="w">          </span><span class="no"># If metrics exceed thresholds, it exits with a non-zero code.</span>
<span class="w">          </span><span class="no">./scripts/monitor_canary.sh challenger-service-name</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4. Manual Approval for Full Rollout</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">success()</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">trstringer/manual-approval@v1</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">secret</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ github.TOKEN }}</span>
<span class="w">          </span><span class="nt">approvers</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;engineering-lead,product-manager&#39;</span>
<span class="w">          </span><span class="nt">minimum-approvals</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">          </span><span class="nt">issue-title</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Promote</span><span class="nv"> </span><span class="s">RAG</span><span class="nv"> </span><span class="s">Challenger</span><span class="nv"> </span><span class="s">to</span><span class="nv"> </span><span class="s">100%</span><span class="nv"> </span><span class="s">Traffic?&quot;</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5. Promote Challenger (if approved)</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">success()</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd inference_service/infra</span>
<span class="w">          </span><span class="no"># Shift 100% traffic to challenger and scale down control</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;env=production&quot; -var=&quot;challenger_weight=100&quot;</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6. Automated Rollback (if canary monitor fails)</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">failure()</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">echo &quot;Canary deployment failed! Rolling back...&quot;</span>
<span class="w">          </span><span class="no">cd inference_service/infra</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;env=production&quot; -var=&quot;challenger_weight=0&quot;</span>
<span class="w">          </span><span class="no"># This step would also trigger a PagerDuty/Slack alert</span>
</pre></div>
</div>
</section>
</section>
<section id="implementation-embedding-model-fine-tuning-pipeline">
<h3>Implementation: Embedding Model Fine-tuning Pipeline<a class="headerlink" href="#implementation-embedding-model-fine-tuning-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="python-scripts-pipeline-components">
<h4>Python Scripts (Pipeline Components)<a class="headerlink" href="#python-scripts-pipeline-components" title="Permalink to this heading">¶</a></h4>
<p><strong>finetuning_pipeline/src/data_preparation.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span> <span class="nn">opensearchpy</span> <span class="kn">import</span> <span class="n">OpenSearch</span> <span class="c1"># and other clients</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># This would be initialized with proper credentials</span>
<span class="n">opensearch_client</span> <span class="o">=</span> <span class="n">OpenSearch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> 
<span class="n">production_embedding_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;intfloat/multilingual-e5-large&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_interaction_data</span><span class="p">(</span><span class="n">log_bucket</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">date_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads and merges user interaction logs from S3.&quot;&quot;&quot;</span>
    <span class="c1"># In a real scenario, this would read multiple Parquet/JSON files from S3,</span>
    <span class="c1"># potentially using AWS Data Wrangler (awswrangler).</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading interaction data from s3://</span><span class="si">{</span><span class="n">log_bucket</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">date_prefix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Placeholder for data loading logic</span>
    <span class="n">sample_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;session_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="s1">&#39;s2&#39;</span><span class="p">,</span> <span class="s1">&#39;s2&#39;</span><span class="p">,</span> <span class="s1">&#39;s2&#39;</span><span class="p">],</span>
        <span class="s1">&#39;query&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;queryA&#39;</span><span class="p">,</span> <span class="s1">&#39;queryA&#39;</span><span class="p">,</span> <span class="s1">&#39;queryB&#39;</span><span class="p">,</span> <span class="s1">&#39;queryB&#39;</span><span class="p">,</span> <span class="s1">&#39;queryB&#39;</span><span class="p">],</span>
        <span class="s1">&#39;retrieved_product_id&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;prod1&#39;</span><span class="p">,</span> <span class="s1">&#39;prod2&#39;</span><span class="p">,</span> <span class="s1">&#39;prod3&#39;</span><span class="p">,</span> <span class="s1">&#39;prod4&#39;</span><span class="p">,</span> <span class="s1">&#39;prod5&#39;</span><span class="p">],</span>
        <span class="s1">&#39;clicked&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span>
        <span class="s1">&#39;purchased&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sample_data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_product_text</span><span class="p">(</span><span class="n">product_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fetches the text content for a given product ID.&quot;&quot;&quot;</span>
    <span class="c1"># This would query a database or another S3 location</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Full text description for </span><span class="si">{</span><span class="n">product_id</span><span class="si">}</span><span class="s2">.&quot;</span> <span class="c1"># Placeholder</span>

<span class="k">def</span> <span class="nf">perform_hard_negative_mining</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">positive_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Finds a hard negative for a given query and positive example.&quot;&quot;&quot;</span>
    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">production_embedding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    
    <span class="c1"># Retrieve top 5 results from the current production index</span>
    <span class="c1"># This simulates what the user would have seen</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">opensearch_client</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="c1"># Search with query_embedding</span>
    
    <span class="n">retrieved_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">hit</span><span class="p">[</span><span class="s1">&#39;_source&#39;</span><span class="p">][</span><span class="s1">&#39;product_id&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">hit</span> <span class="ow">in</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;hits&#39;</span><span class="p">][</span><span class="s1">&#39;hits&#39;</span><span class="p">]]</span>
    
    <span class="c1"># Find the first retrieved ID that is NOT the one the user purchased</span>
    <span class="k">for</span> <span class="n">an_id</span> <span class="ow">in</span> <span class="n">retrieved_ids</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">an_id</span> <span class="o">!=</span> <span class="n">positive_id</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found hard negative &#39;</span><span class="si">{</span><span class="n">an_id</span><span class="si">}</span><span class="s2">&#39; for query &#39;</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">get_product_text</span><span class="p">(</span><span class="n">an_id</span><span class="p">)</span>
            
    <span class="k">return</span> <span class="kc">None</span> <span class="c1"># Could happen if user buys the top result</span>

<span class="k">def</span> <span class="nf">create_triplets</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constructs (anchor, positive, negative) triplets from interaction data.&quot;&quot;&quot;</span>
    <span class="n">triplets</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">successful_interactions</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;purchased&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">successful_interactions</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">anchor</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">]</span>
        <span class="n">positive_text</span> <span class="o">=</span> <span class="n">get_product_text</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;retrieved_product_id&#39;</span><span class="p">])</span>
        <span class="n">negative_text</span> <span class="o">=</span> <span class="n">perform_hard_negative_mining</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;retrieved_product_id&#39;</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">anchor</span> <span class="ow">and</span> <span class="n">positive_text</span> <span class="ow">and</span> <span class="n">negative_text</span><span class="p">:</span>
            <span class="n">triplets</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive_text</span><span class="p">,</span> <span class="n">negative_text</span><span class="p">))</span>
            
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully created </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">triplets</span><span class="p">)</span><span class="si">}</span><span class="s2"> training triplets.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">triplets</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># This script would be run by the Airflow task</span>
    <span class="n">interaction_df</span> <span class="o">=</span> <span class="n">load_interaction_data</span><span class="p">(</span><span class="s2">&quot;rag-log-archive-prod&quot;</span><span class="p">,</span> <span class="s2">&quot;2025/08/&quot;</span><span class="p">)</span>
    <span class="n">triplets</span> <span class="o">=</span> <span class="n">create_triplets</span><span class="p">(</span><span class="n">interaction_df</span><span class="p">)</span>
    
    <span class="c1"># Convert to a DataFrame and save to S3</span>
    <span class="n">triplets_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">triplets</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;anchor&#39;</span><span class="p">,</span> <span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="s1">&#39;negative&#39;</span><span class="p">])</span>
    <span class="n">train_df</span><span class="p">,</span> <span class="n">val_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">triplets_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    
    <span class="c1"># Save to S3 in a versioned folder (e.g., using the run date)</span>
    <span class="c1"># train_df.to_csv(&quot;s3://rag-finetuning-data/train_YYYY-MM-DD.csv&quot;)</span>
    <span class="c1"># val_df.to_csv(&quot;s3://rag-finetuning-data/val_YYYY-MM-DD.csv&quot;)</span>
</pre></div>
</div>
</section>
<section id="unit-tests-pytest">
<h4>Unit Tests (pytest)<a class="headerlink" href="#unit-tests-pytest" title="Permalink to this heading">¶</a></h4>
<p><strong>finetuning_pipeline/tests/unit/test_data_preparation.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">patch</span>
<span class="kn">from</span> <span class="nn">src</span> <span class="kn">import</span> <span class="n">data_preparation</span>

<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;src.data_preparation.production_embedding_model&#39;</span><span class="p">)</span>
<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;src.data_preparation.opensearch_client&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_hard_negative_mining_logic</span><span class="p">(</span><span class="n">mock_os_client</span><span class="p">,</span> <span class="n">mock_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that the hard negative mining correctly selects a non-positive document.&quot;&quot;&quot;</span>
    <span class="c1"># ARRANGE</span>
    <span class="n">test_query</span> <span class="o">=</span> <span class="s2">&quot;test query&quot;</span>
    <span class="n">positive_product_id</span> <span class="o">=</span> <span class="s2">&quot;prod-positive&quot;</span>
    
    <span class="c1"># Mock the OpenSearch response</span>
    <span class="n">mock_os_client</span><span class="o">.</span><span class="n">search</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;hits&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hits&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s1">&#39;_source&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;product_id&#39;</span><span class="p">:</span> <span class="s1">&#39;prod-hard-negative&#39;</span><span class="p">}},</span>
            <span class="p">{</span><span class="s1">&#39;_source&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;product_id&#39;</span><span class="p">:</span> <span class="s1">&#39;prod-positive&#39;</span><span class="p">}},</span>
            <span class="p">{</span><span class="s1">&#39;_source&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;product_id&#39;</span><span class="p">:</span> <span class="s1">&#39;prod-other&#39;</span><span class="p">}}</span>
        <span class="p">]}</span>
    <span class="p">}</span>
    
    <span class="c1"># ACT</span>
    <span class="n">hard_negative</span> <span class="o">=</span> <span class="n">data_preparation</span><span class="o">.</span><span class="n">perform_hard_negative_mining</span><span class="p">(</span><span class="n">test_query</span><span class="p">,</span> <span class="n">positive_product_id</span><span class="p">)</span>
    
    <span class="c1"># ASSERT</span>
    <span class="c1"># It should have picked the first result that was not the positive one.</span>
    <span class="k">assert</span> <span class="s2">&quot;prod-hard-negative&quot;</span> <span class="ow">in</span> <span class="n">hard_negative</span>
</pre></div>
</div>
</section>
<section id="pipeline-orchestration-airflow-dag">
<h4>Pipeline Orchestration (Airflow DAG)<a class="headerlink" href="#pipeline-orchestration-airflow-dag" title="Permalink to this heading">¶</a></h4>
<p><strong>dags/embedding_finetuning_dag.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.models.dag</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">BranchPythonOperator</span>
<span class="kn">from</span> <span class="nn">airflow.providers.amazon.aws.operators.sagemaker</span> <span class="kn">import</span> <span class="n">SageMakerTrainingOperator</span> <span class="c1"># and ProcessingOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.dummy</span> <span class="kn">import</span> <span class="n">DummyOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.email</span> <span class="kn">import</span> <span class="n">EmailOperator</span>

<span class="c1"># Define IAM roles, S3 paths, etc. as variables</span>

<span class="k">def</span> <span class="nf">check_evaluation_result</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">ti</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;ti&#39;</span><span class="p">]</span>
    <span class="c1"># This would pull the evaluation result from XComs, which was pushed by the evaluate_model_task</span>
    <span class="n">eval_result</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">task_ids</span><span class="o">=</span><span class="s1">&#39;evaluate_model_task&#39;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;evaluation_summary&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">eval_result</span><span class="p">[</span><span class="s1">&#39;status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;pass&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;register_model_task&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;notify_failure_task&#39;</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s2">&quot;embedding_model_finetuning&quot;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2025</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="s2">&quot;UTC&quot;</span><span class="p">),</span>
    <span class="n">schedule</span><span class="o">=</span><span class="s2">&quot;0 0 1 * *&quot;</span><span class="p">,</span> <span class="c1"># At 00:00 on day-of-month 1.</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rag&quot;</span><span class="p">,</span> <span class="s2">&quot;finetuning&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    
    <span class="n">prepare_data_task</span> <span class="o">=</span> <span class="c1"># ... PythonOperator to run data_preparation.py ...</span>
    
    <span class="n">train_model_task</span> <span class="o">=</span> <span class="n">SageMakerTrainingOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;train_model_task&quot;</span><span class="p">,</span>
        <span class="c1"># ... (config pointing to training script, instance types, IAM role) ...</span>
    <span class="p">)</span>

    <span class="n">evaluate_model_task</span> <span class="o">=</span> <span class="c1"># ... SageMakerProcessingOperator to run model_evaluation.py ...</span>
    
    <span class="n">check_evaluation_gate</span> <span class="o">=</span> <span class="n">BranchPythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;check_evaluation_gate&quot;</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">check_evaluation_result</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="n">register_model_task</span> <span class="o">=</span> <span class="c1"># ... PythonOperator to run model_registration.py ...</span>
    
    <span class="n">notify_failure_task</span> <span class="o">=</span> <span class="n">EmailOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;notify_failure_task&quot;</span><span class="p">,</span>
        <span class="n">to</span><span class="o">=</span><span class="s2">&quot;ml-team@example.com&quot;</span><span class="p">,</span>
        <span class="n">subject</span><span class="o">=</span><span class="s2">&quot;Embedding Model Fine-tuning Failed Quality Gate&quot;</span><span class="p">,</span>
        <span class="n">html_content</span><span class="o">=</span><span class="s2">&quot;The candidate model did not outperform the production model. Check the logs.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">success_task</span> <span class="o">=</span> <span class="n">DummyOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;success&quot;</span><span class="p">)</span>

    <span class="n">prepare_data_task</span> <span class="o">&gt;&gt;</span> <span class="n">train_model_task</span> <span class="o">&gt;&gt;</span> <span class="n">evaluate_model_task</span> <span class="o">&gt;&gt;</span> <span class="n">check_evaluation_gate</span>
    <span class="n">check_evaluation_gate</span> <span class="o">&gt;&gt;</span> <span class="p">[</span><span class="n">register_model_task</span><span class="p">,</span> <span class="n">notify_failure_task</span><span class="p">]</span>
    <span class="n">register_model_task</span> <span class="o">&gt;&gt;</span> <span class="n">success_task</span>
</pre></div>
</div>
</section>
<section id="id18">
<h4>Infrastructure as Code (Terraform)<a class="headerlink" href="#id18" title="Permalink to this heading">¶</a></h4>
<p><strong>finetuning_pipeline/infra/sagemaker.tf</strong></p>
<div class="highlight-hcl notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defines the SageMaker Model Group for versioning our fine-tuned models</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sagemaker_model_package_group&quot;</span><span class="w"> </span><span class="nv">&quot;embedding_models&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">model_package_group_name</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;rag-embedding-models-${var.env}&quot;</span>
<span class="w">  </span><span class="na">model_package_group_description</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;All versions of the fine-tuned RAG embedding model.&quot;</span>
<span class="p">}</span>

<span class="c1"># IAM role for the SageMaker training jobs</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role&quot;</span><span class="w"> </span><span class="nv">&quot;sagemaker_training_role&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;SageMakerTrainingRole-${var.env}&quot;</span>
<span class="w">  </span><span class="na">assume_role_policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="c1"># ... (trust policy for sagemaker.amazonaws.com) ...</span>
<span class="p">}</span>

<span class="c1"># Attach policies granting access to S3 data buckets and ECR</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role_policy_attachment&quot;</span><span class="w"> </span><span class="nv">&quot;s3_access&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">role</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.sagemaker_training_role.name</span>
<span class="w">  </span><span class="na">policy_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="c1"># ... (ARN of a policy allowing read from data bucket, write to artifact bucket) ...</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id19">
<h4>Integration Test<a class="headerlink" href="#id19" title="Permalink to this heading">¶</a></h4>
<p><strong>finetuning_pipeline/tests/integration/test_finetuning_dag.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">airflow.models.dagbag</span> <span class="kn">import</span> <span class="n">DagBag</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_dag_loads_with_no_errors</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks if the DAG file is syntactically correct and can be parsed by Airflow.&quot;&quot;&quot;</span>
    <span class="n">dagbag</span> <span class="o">=</span> <span class="n">DagBag</span><span class="p">(</span><span class="n">dag_folder</span><span class="o">=</span><span class="s1">&#39;dags/&#39;</span><span class="p">,</span> <span class="n">include_examples</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">dagbag</span><span class="o">.</span><span class="n">import_errors</span>
    <span class="k">assert</span> <span class="s1">&#39;embedding_model_finetuning&#39;</span> <span class="ow">in</span> <span class="n">dagbag</span><span class="o">.</span><span class="n">dags</span>

<span class="c1"># A more advanced integration test would use the Airflow API to trigger the DAG</span>
<span class="c1"># in a test environment and check its final state.</span>
<span class="c1"># from airflow.api.client.local_client import Client</span>
<span class="c1">#</span>
<span class="c1"># def test_dag_run_completes():</span>
<span class="c1">#     client = Client(None, None)</span>
<span class="c1">#     result = client.trigger_dag(dag_id=&#39;embedding_model_finetuning&#39;)</span>
<span class="c1">#     # ... poll for completion status ...</span>
<span class="c1">#     assert result[&#39;state&#39;] == &#39;success&#39;</span>
</pre></div>
</div>
</section>
<section id="id20">
<h4>CI/CD GitHub Actions Workflow<a class="headerlink" href="#id20" title="Permalink to this heading">¶</a></h4>
<p><strong>.github/workflows/deploy_finetuning_pipeline.yml</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Fine-tuning Pipeline</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;finetuning_pipeline/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;dags/embedding_finetuning_dag.py&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">lint-and-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Lint &amp; Unit Test</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (checkout, setup python, install dependencies) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run unit tests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest finetuning_pipeline/tests/unit/</span>

<span class="w">  </span><span class="nt">deploy-to-staging</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy to Staging Airflow</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lint-and-test</span>
<span class="w">    </span><span class="nt">environment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">staging</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># ... (checkout, configure AWS creds, setup Terraform) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Terraform Apply</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd finetuning_pipeline/infra</span>
<span class="w">          </span><span class="no">terraform init</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;env=staging&quot;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Sync DAG to Staging Airflow</span>
<span class="w">        </span><span class="c1"># This step would use aws s3 sync or a custom script to upload the</span>
<span class="w">        </span><span class="c1"># dags/embedding_finetuning_dag.py file to the Airflow S3 bucket.</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws s3 sync ./dags s3://${{ secrets.STAGING_AIRFLOW_DAGS_BUCKET }}/dags</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="guide-to-fine-tuning-re-ranker-model">
<h3>Guide to Fine-tuning Re-ranker Model<a class="headerlink" href="#guide-to-fine-tuning-re-ranker-model" title="Permalink to this heading">¶</a></h3>
<section id="the-why-the-two-stage-retrieval-process">
<h4><strong>The “Why”: The Two-Stage Retrieval Process</strong><a class="headerlink" href="#the-why-the-two-stage-retrieval-process" title="Permalink to this heading">¶</a></h4>
<p>Think of RAG system’s retrieval in two stages, like a fishing operation:</p>
<ol class="arabic simple">
<li><p><strong>The Retriever (Embedding Model / Bi-Encoder): The Wide Net.</strong> Its job is to be <strong>fast</strong> and achieve <strong>high recall</strong>. It quickly scans the entire ocean of documents and pulls in a large net of 50-100 potentially relevant candidates. It’s good at finding things that are semantically <em>in the ballpark</em>, but it might also pull in a lot of irrelevant “bycatch.” This is a <strong>bi-encoder</strong> architecture, where the query and documents are embedded independently.</p></li>
<li><p><strong>The Re-ranker (Cross-Encoder): The Expert Judge.</strong> Its job is to be <strong>accurate</strong> and achieve <strong>high precision</strong>. It takes the smaller catch from the retriever’s net (the 50-100 candidates) and individually inspects each one against the query. It’s much slower but far more discerning. This is a <strong>cross-encoder</strong> architecture, which processes the query and a document <em>together</em>, allowing for much deeper contextual understanding.</p></li>
</ol>
<p>The goal of fine-tuning the re-ranker is to make this “expert judge” exceptionally good at understanding the specific nuances of relevance for <em>your</em> products and <em>your</em> customers’ queries.</p>
</section>
<section id="the-dataset-the-crucial-difference">
<h4><strong>The Dataset: The Crucial Difference</strong><a class="headerlink" href="#the-dataset-the-crucial-difference" title="Permalink to this heading">¶</a></h4>
<p>The dataset used for fine-tuning a re-ranker is fundamentally different from the triplet dataset used for the embedding model. They are structured differently to serve different training objectives.</p>
<p>Here is a clear comparison:</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Aspect</p></th>
<th class="head text-left"><p>Embedding Model Fine-tuning Dataset</p></th>
<th class="head text-left"><p>Re-ranker Fine-tuning Dataset</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Purpose</strong></p></td>
<td class="text-left"><p>To learn a <strong>global semantic representation</strong>. Teaches the model to place similar items close together in the vector space so the “wide net” retrieval works better.</p></td>
<td class="text-left"><p>To learn <strong>relative relevance</strong>. Teaches the model to look at a query and a specific document and say “Yes, this is a highly relevant answer” (label=1) or “No, this is not a good answer” (label=0).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Structure</strong></p></td>
<td class="text-left"><p>Triplets: <code class="docutils literal notranslate"><span class="pre">(anchor,</span> <span class="pre">positive,</span> <span class="pre">negative)</span></code></p></td>
<td class="text-left"><p>Scored Pairs: <code class="docutils literal notranslate"><span class="pre">(query,</span> <span class="pre">passage,</span> <span class="pre">label)</span></code> where the label is typically <code class="docutils literal notranslate"><span class="pre">1</span></code> (relevant) or <code class="docutils literal notranslate"><span class="pre">0</span></code> (irrelevant).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Example</strong></p></td>
<td class="text-left"><p>(<code class="docutils literal notranslate"><span class="pre">&quot;running</span> <span class="pre">shoes&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Text</span> <span class="pre">about</span> <span class="pre">Nike</span> <span class="pre">Pegasus...&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Text</span> <span class="pre">about</span> <span class="pre">dress</span> <span class="pre">shoes...&quot;</span></code>)</p></td>
<td class="text-left"><p>1. (<code class="docutils literal notranslate"><span class="pre">&quot;running</span> <span class="pre">shoes&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Text</span> <span class="pre">about</span> <span class="pre">Nike</span> <span class="pre">Pegasus...&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>) <br> 2. (<code class="docutils literal notranslate"><span class="pre">&quot;running</span> <span class="pre">shoes&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Text</span> <span class="pre">about</span> <span class="pre">dress</span> <span class="pre">shoes...&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Training Objective</strong></p></td>
<td class="text-left"><p><strong>Distance-based Loss</strong> (e.g., TripletLoss). The goal is to minimize the distance between <code class="docutils literal notranslate"><span class="pre">anchor</span></code> and <code class="docutils literal notranslate"><span class="pre">positive</span></code> while maximizing the distance between <code class="docutils literal notranslate"><span class="pre">anchor</span></code> and <code class="docutils literal notranslate"><span class="pre">negative</span></code>.</p></td>
<td class="text-left"><p><strong>Classification or Ranking Loss</strong> (e.g., Binary Cross-Entropy). The model predicts a single score (a logit) representing relevance, and the loss is calculated against the binary <code class="docutils literal notranslate"><span class="pre">0/1</span></code> label.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="data-sourcing-and-creation-for-the-re-ranker">
<h4><strong>Data Sourcing and Creation for the Re-ranker</strong><a class="headerlink" href="#data-sourcing-and-creation-for-the-re-ranker" title="Permalink to this heading">¶</a></h4>
<p>The process for creating the re-ranker dataset is similar to the embedding model but results in a different final structure.</p>
<ol class="arabic simple">
<li><p><strong>Load User Interaction Data:</strong> Start with the same raw logs from the Monitoring Pipeline (queries, clicks, purchases).</p></li>
<li><p><strong>Create <code class="docutils literal notranslate"><span class="pre">(query,</span> <span class="pre">positive_passage)</span></code> Pairs:</strong> For every user journey that resulted in a purchase, create a pair: <code class="docutils literal notranslate"><span class="pre">(user_query,</span> <span class="pre">text_of_purchased_product)</span></code>. These will be our <strong>positive examples with a label of <code class="docutils literal notranslate"><span class="pre">1</span></code></strong>.</p></li>
<li><p><strong>Mine Hard Negatives:</strong> This is the most critical step for a high-performing re-ranker. For each <code class="docutils literal notranslate"><span class="pre">(query,</span> <span class="pre">positive_passage)</span></code> pair:</p>
<ul class="simple">
<li><p>Use the <strong>current production retriever (the bi-encoder)</strong> to fetch the top 10-20 results for that <code class="docutils literal notranslate"><span class="pre">query</span></code>.</p></li>
<li><p>From this list, select the documents that the user <strong>did not click on or purchase</strong>. These are your <strong>hard negatives</strong>. They are powerful training examples because they are semantically close enough to the query to be retrieved, but were ultimately deemed irrelevant by the user. This is exactly the ambiguity the re-ranker needs to learn to resolve.</p></li>
<li><p>For each hard negative, create a pair: <code class="docutils literal notranslate"><span class="pre">(user_query,</span> <span class="pre">text_of_hard_negative_product)</span></code>. These will be our <strong>negative examples with a label of <code class="docutils literal notranslate"><span class="pre">0</span></code></strong>.</p></li>
</ul>
</li>
<li><p><strong>Assemble and Balance the Dataset:</strong></p>
<ul class="simple">
<li><p>Combine all positive and negative pairs into a single dataset.</p></li>
<li><p>Ensure the dataset is balanced (roughly equal numbers of positive and negative examples) to prevent the model from becoming biased. You might need to oversample positives or undersample negatives.</p></li>
<li><p>The final dataset is a list of examples, each with a <code class="docutils literal notranslate"><span class="pre">query</span></code>, a <code class="docutils literal notranslate"><span class="pre">passage</span></code>, and a <code class="docutils literal notranslate"><span class="pre">label</span></code> (0 or 1).</p></li>
</ul>
</li>
</ol>
</section>
<section id="the-model-and-training-process">
<h4><strong>The Model and Training Process</strong><a class="headerlink" href="#the-model-and-training-process" title="Permalink to this heading">¶</a></h4>
<ol class="arabic simple">
<li><p><strong>Model Architecture (Cross-Encoder):</strong></p>
<ul class="simple">
<li><p>We use a transformer model architecture (like BERT or RoBERTa) configured as a cross-encoder.</p></li>
<li><p>Instead of embedding the query and passage separately, we feed them into the model together in a single sequence, separated by a special <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> token: <code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">query</span> <span class="pre">[SEP]</span> <span class="pre">passage</span> <span class="pre">[SEP]</span></code>.</p></li>
<li><p>The model’s output for the <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> token is then passed through a linear layer to produce a single logit (a raw score). This score represents the relevance of the <code class="docutils literal notranslate"><span class="pre">passage</span></code> to the <code class="docutils literal notranslate"><span class="pre">query</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Training Objective:</strong></p>
<ul class="simple">
<li><p>The problem is framed as a simple binary classification. The model’s single logit output is passed through a sigmoid function to get a probability between 0 and 1.</p></li>
<li><p>We use <strong>Binary Cross-Entropy Loss</strong> to train the model, comparing its predicted probability against the true <code class="docutils literal notranslate"><span class="pre">0/1</span></code> label from our dataset.</p></li>
</ul>
</li>
<li><p><strong>The Training Loop (using Hugging Face <code class="docutils literal notranslate"><span class="pre">transformers</span></code>):</strong></p>
<ul class="simple">
<li><p>Load a pre-trained cross-encoder model (e.g., <code class="docutils literal notranslate"><span class="pre">cross-encoder/ms-marco-MiniLM-L-6-v2</span></code>) as a starting point.</p></li>
<li><p>Use the Hugging Face <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> API, providing it with our prepared dataset of <code class="docutils literal notranslate"><span class="pre">(query,</span> <span class="pre">passage,</span> <span class="pre">label)</span></code> pairs.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> handles the fine-tuning loop: batching the data, feeding it to the model, calculating the loss, and updating the model’s weights.</p></li>
<li><p>This is executed as a <strong>SageMaker Training Job</strong>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="the-mlops-pipeline-for-re-ranker-fine-tuning">
<h4><strong>The MLOps Pipeline for Re-ranker Fine-tuning</strong><a class="headerlink" href="#the-mlops-pipeline-for-re-ranker-fine-tuning" title="Permalink to this heading">¶</a></h4>
<p>The pipeline is nearly identical to the embedding model pipeline, demonstrating the power of reusable MLOps patterns.</p>
<ol class="arabic simple">
<li><p><strong>Airflow DAG (<code class="docutils literal notranslate"><span class="pre">reranker_finetuning_dag.py</span></code>):</strong></p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">prepare_reranker_data_task</span></code>:</strong> Executes a script to perform the data loading and hard negative mining described above, creating the final <code class="docutils literal notranslate"><span class="pre">(query,</span> <span class="pre">passage,</span> <span class="pre">label)</span></code> dataset.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">train_reranker_task</span></code>:</strong> Submits a <strong>SageMaker Training Job</strong> that runs the Hugging Face training script on a GPU instance.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">evaluate_reranker_task</span></code>:</strong> This is different. It evaluates the new re-ranker by first running the <em>production retriever</em> on the Golden Dataset to get top-50 candidates, then using the <em>new re-ranker</em> to sort them. It then calculates MRR/NDCG on this <strong>re-ranked list</strong>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">check_evaluation_gate</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">register_model_task</span></code>:</strong> Same logic as before.</p></li>
</ul>
</li>
</ol>
</section>
<section id="architecture-diagram">
<h4><strong>Architecture Diagram</strong><a class="headerlink" href="#architecture-diagram" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_rag/finetuning_reranker.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
</section>
<hr class="docutils" />
<section id="synthetic-dataset-generation-to-evaluate-retrieval-and-ranking">
<h3>Synthetic Dataset Generation to Evaluate Retrieval and Ranking<a class="headerlink" href="#synthetic-dataset-generation-to-evaluate-retrieval-and-ranking" title="Permalink to this heading">¶</a></h3>
<p>This script operationalizes the plan to create a large-scale, high-quality evaluation dataset. It uses a powerful LLM (Claude 3 Opus) to generate realistic user queries grounded in specific chunks of our product descriptions, using a curated seed list to guide the generation style.</p>
<section id="a-python-script">
<h4><strong>A. Python Script</strong><a class="headerlink" href="#a-python-script" title="Permalink to this heading">¶</a></h4>
<p><strong><code class="docutils literal notranslate"><span class="pre">golden_dataset_pipeline/src/generate_golden_dataset.py</span></code></strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">aiohttp</span> <span class="kn">import</span> <span class="n">ClientError</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">JsonOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">BedrockChat</span>

<span class="c1"># --- Configuration ---</span>
<span class="c1"># In a real project, this would come from a config file or environment variables.</span>
<span class="n">CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;PRODUCT_CATALOG_PATH&quot;</span><span class="p">:</span> <span class="s2">&quot;data/raw/product_catalog.csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SEED_QUERIES_PATH&quot;</span><span class="p">:</span> <span class="s2">&quot;data/raw/seed_queries.txt&quot;</span><span class="p">,</span>
    <span class="s2">&quot;OUTPUT_PATH&quot;</span><span class="p">:</span> <span class="s2">&quot;data/processed/golden_evaluation_dataset.jsonl&quot;</span><span class="p">,</span>
    <span class="s2">&quot;LLM_MODEL_ID&quot;</span><span class="p">:</span> <span class="s2">&quot;anthropic.claude-3-opus-20240229-v1:0&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MAX_QUERIES_PER_CHUNK&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="s2">&quot;NUM_SEED_EXAMPLES&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;MAX_CONCURRENT_REQUESTS&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>  <span class="c1"># Important to avoid rate limiting</span>
<span class="p">}</span>

<span class="c1"># --- Logging Setup ---</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
    <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> - </span><span class="si">%(levelname)s</span><span class="s1"> - </span><span class="si">%(message)s</span><span class="s1">&#39;</span>
<span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># --- Core Functions ---</span>

<span class="k">def</span> <span class="nf">load_seed_queries</span><span class="p">(</span><span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads the seed query list from a text file.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span> <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()]</span>
    <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Seed queries file not found at: </span><span class="si">{</span><span class="n">filepath</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">load_product_documents</span><span class="p">(</span><span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads product catalog data from a CSV.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
        <span class="c1"># Ensure required columns exist</span>
        <span class="k">if</span> <span class="s1">&#39;product_id&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="ow">or</span> <span class="s1">&#39;description&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;CSV must contain &#39;product_id&#39; and &#39;description&#39; columns.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;product_id&#39;</span><span class="p">,</span> <span class="s1">&#39;description&#39;</span><span class="p">])</span>
    <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Product catalog file not found at: </span><span class="si">{</span><span class="n">filepath</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">chunk_document</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Splits a document&#39;s text into smaller chunks.&quot;&quot;&quot;</span>
    <span class="n">splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span>
        <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">chunk_overlap</span><span class="p">,</span>
        <span class="n">length_function</span><span class="o">=</span><span class="nb">len</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">generate_queries_for_chunk</span><span class="p">(</span>
    <span class="n">llm</span><span class="p">:</span> <span class="n">BedrockChat</span><span class="p">,</span>
    <span class="n">chunk_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">seed_queries</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Uses an LLM to generate a list of queries for a given text chunk.&quot;&quot;&quot;</span>
    
    <span class="c1"># Randomly sample seed queries to provide varied examples in the prompt</span>
    <span class="n">examples</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- </span><span class="se">\&quot;</span><span class="si">{</span><span class="n">q</span><span class="si">}</span><span class="se">\&quot;</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">seed_queries</span><span class="p">,</span> <span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;NUM_SEED_EXAMPLES&quot;</span><span class="p">]))</span>
    
    <span class="n">prompt_template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
        <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> 
         <span class="s2">&quot;You are a data scientist creating a high-quality evaluation dataset for an e-commerce semantic search engine. &quot;</span>
         <span class="s2">&quot;Your task is to generate realistic user search queries that can be answered by the provided text snippet from a product description. &quot;</span>
         <span class="s2">&quot;The queries must be diverse, reflecting different user intents (e.g., questions, feature requests, comparisons, use-cases). &quot;</span>
         <span class="s2">&quot;The answer to each query you generate MUST be present in the provided context. Do NOT generate questions requiring outside knowledge. &quot;</span>
         <span class="s2">&quot;Output a JSON object with a single key &#39;queries&#39; containing a list of strings.&quot;</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> 
         <span class="s2">&quot;**CONTEXT (Product Information Snippet):**</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="si">{context}</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n\n</span><span class="s2">&quot;</span>
         <span class="s2">&quot;**EXAMPLES of QUERY STYLES:**</span><span class="se">\n</span><span class="si">{examples}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
         <span class="sa">f</span><span class="s2">&quot;Please generate </span><span class="si">{</span><span class="n">CONFIG</span><span class="p">[</span><span class="s1">&#39;MAX_QUERIES_PER_CHUNK&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> realistic and diverse user queries based on the context above.&quot;</span><span class="p">)</span>
    <span class="p">])</span>
    
    <span class="n">parser</span> <span class="o">=</span> <span class="n">JsonOutputParser</span><span class="p">()</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="n">prompt_template</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">chain</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">chunk_text</span><span class="p">,</span> <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="n">examples</span><span class="p">})</span>
        <span class="k">if</span> <span class="s2">&quot;queries&quot;</span> <span class="ow">in</span> <span class="n">response</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;queries&quot;</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;queries&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LLM returned malformed JSON, missing &#39;queries&#39; list. Response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[]</span>
    <span class="k">except</span> <span class="p">(</span><span class="n">ClientError</span><span class="p">,</span> <span class="n">json</span><span class="o">.</span><span class="n">JSONDecodeError</span><span class="p">,</span> <span class="ne">Exception</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error generating queries for chunk: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[]</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">process_document</span><span class="p">(</span>
    <span class="n">semaphore</span><span class="p">:</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Semaphore</span><span class="p">,</span>
    <span class="n">llm</span><span class="p">:</span> <span class="n">BedrockChat</span><span class="p">,</span>
    <span class="n">product</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span>
    <span class="n">seed_queries</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">output_file</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Chunks a document, generates queries for each chunk, and writes to the output file.&quot;&quot;&quot;</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">product_id</span> <span class="o">=</span> <span class="n">product</span><span class="p">[</span><span class="s1">&#39;product_id&#39;</span><span class="p">]</span>
            <span class="n">description</span> <span class="o">=</span> <span class="n">product</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span>
            
            <span class="n">chunks</span> <span class="o">=</span> <span class="n">chunk_document</span><span class="p">(</span><span class="n">description</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processing product </span><span class="si">{</span><span class="n">product_id</span><span class="si">}</span><span class="s2">: split into </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span><span class="si">}</span><span class="s2"> chunks.&quot;</span><span class="p">)</span>
            
            <span class="n">tasks</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
                <span class="n">tasks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generate_queries_for_chunk</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">seed_queries</span><span class="p">))</span>
            
            <span class="n">generated_queries_per_chunk</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">queries</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">generated_queries_per_chunk</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">queries</span><span class="p">:</span>
                    <span class="n">record</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span>
                        <span class="s2">&quot;relevant_product_id&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">product_id</span><span class="p">),</span>
                        <span class="s2">&quot;source_chunk_id&quot;</span><span class="p">:</span> <span class="n">i</span>
                    <span class="p">}</span>
                    <span class="n">output_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">record</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to process product </span><span class="si">{</span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;product_id&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;N/A&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Main orchestration function.&quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Starting synthetic golden dataset generation.&quot;</span><span class="p">)</span>
    
    <span class="n">seed_queries</span> <span class="o">=</span> <span class="n">load_seed_queries</span><span class="p">(</span><span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;SEED_QUERIES_PATH&quot;</span><span class="p">])</span>
    <span class="n">product_df</span> <span class="o">=</span> <span class="n">load_product_documents</span><span class="p">(</span><span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;PRODUCT_CATALOG_PATH&quot;</span><span class="p">])</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">seed_queries</span> <span class="ow">or</span> <span class="n">product_df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Cannot proceed without seed queries and product data. Exiting.&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Initialize the LLM</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="n">BedrockChat</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;LLM_MODEL_ID&quot;</span><span class="p">],</span>
        <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span> <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># Use a semaphore to limit concurrent API calls</span>
    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;MAX_CONCURRENT_REQUESTS&quot;</span><span class="p">])</span>
    
    <span class="c1"># Ensure output directory exists</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;OUTPUT_PATH&quot;</span><span class="p">]),</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;OUTPUT_PATH&quot;</span><span class="p">],</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">output_file</span><span class="p">:</span>
        <span class="n">tasks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">product</span> <span class="ow">in</span> <span class="n">product_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
            <span class="n">tasks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">process_document</span><span class="p">(</span><span class="n">semaphore</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">product</span><span class="p">,</span> <span class="n">seed_queries</span><span class="p">,</span> <span class="n">output_file</span><span class="p">))</span>
        
        <span class="c1"># Process all documents concurrently</span>
        <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset generation complete. Output saved to </span><span class="si">{</span><span class="n">CONFIG</span><span class="p">[</span><span class="s1">&#39;OUTPUT_PATH&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Ensure AWS credentials and LangSmith env variables are set before running</span>
    <span class="c1"># e.g., export LANGCHAIN_TRACING_V2=true; export LANGCHAIN_API_KEY=...</span>
    <span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="b-prerequisites-how-to-run">
<h4><strong>B. Prerequisites &amp; How to Run</strong><a class="headerlink" href="#b-prerequisites-how-to-run" title="Permalink to this heading">¶</a></h4>
<ol class="arabic">
<li><p><strong>File Structure:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>golden_dataset_pipeline/
├── src/
│   └── generate_golden_dataset.py
└── data/
    ├── raw/
    │   ├── product_catalog.csv
    │   └── seed_queries.txt
    └── processed/
        └── (output will be generated here)
</pre></div>
</div>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">product_catalog.csv</span></code>:</strong> A CSV file with at least two columns: <code class="docutils literal notranslate"><span class="pre">product_id</span></code> and <code class="docutils literal notranslate"><span class="pre">description</span></code>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">seed_queries.txt</span></code>:</strong> A plain text file where each line is a real, high-quality user query.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>waterproof hiking boots with ankle support
compare sony wh-1000xm5 vs bose qc ultra
best camera for travel vlogging
is the iphone 15 compatible with a qi charger?
...
</pre></div>
</div>
</li>
<li><p><strong>Dependencies:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">pandas</span> <span class="n">langchain</span> <span class="n">langchain</span><span class="o">-</span><span class="n">community</span><span class="o">-</span><span class="n">aws</span> <span class="n">boto3</span>
</pre></div>
</div>
</li>
<li><p><strong>Execution:</strong></p>
<ul>
<li><p>Set up your AWS credentials (<code class="docutils literal notranslate"><span class="pre">~/.aws/credentials</span></code>).</p></li>
<li><p>(Optional but recommended) Set up LangSmith environment variables for tracing.</p></li>
<li><p>Run the script from the root of the <code class="docutils literal notranslate"><span class="pre">golden_dataset_pipeline</span></code> directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>src/generate_golden_dataset.py
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</section>
<section id="c-output-golden-evaluation-dataset-jsonl">
<h4><strong>C. Output (<code class="docutils literal notranslate"><span class="pre">golden_evaluation_dataset.jsonl</span></code>)</strong><a class="headerlink" href="#c-output-golden-evaluation-dataset-jsonl" title="Permalink to this heading">¶</a></h4>
<p>The script will produce a JSON Lines file, where each line is a single, self-contained evaluation record. This format is efficient and easy to parse.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;query&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;what kind of warranty comes with the anker powerbank?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;relevant_product_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;B07Y2P1L6F&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;source_chunk_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;query&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;is this powerbank airline approved for carry on?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;relevant_product_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;B07Y2P1L6F&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;source_chunk_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;query&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;compare Anker 313 vs PowerCore Slim 10000&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;relevant_product_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;B07Y2P1L6F&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;source_chunk_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../projects/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Projects</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="ecom_summarisation.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Reviews Summarisation</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, Deepak Karkala
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">RAG-Based Product Discovery</a><ul>
<li><a class="reference internal" href="#id1"></a><ul>
<li><a class="reference internal" href="#tldr-from-clunky-search-to-conversational-commerce"><strong>TLDR: From Clunky Search to Conversational Commerce</strong></a></li>
<li><a class="reference internal" href="#business-challenge-beyond-the-limitations-of-keyword-search"><strong>1. Business Challenge: Beyond the Limitations of Keyword Search</strong></a><ul>
<li><a class="reference internal" href="#the-limitations-of-traditional-keyword-search"><strong>The Limitations of Traditional Keyword Search</strong></a></li>
<li><a class="reference internal" href="#the-tangible-business-impact"><strong>The Tangible Business Impact</strong></a></li>
<li><a class="reference internal" href="#project-goals-from-transactional-search-to-conversational-discovery"><strong>Project Goals: From Transactional Search to Conversational Discovery</strong></a></li>
<li><a class="reference internal" href="#measuring-success-tying-technology-to-business-value"><strong>Measuring Success: Tying Technology to Business Value</strong></a><ul>
<li><a class="reference internal" href="#primary-business-kpis"><strong>Primary Business KPIs</strong></a></li>
<li><a class="reference internal" href="#secondary-engagement-kpis"><strong>Secondary Engagement KPIs</strong></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#problem-framing-from-business-need-to-a-measurable-ml-vision"><strong>2. Problem Framing: From Business Need to a Measurable ML Vision</strong></a><ul>
<li><a class="reference internal" href="#a-setting-the-business-objectives"><strong>A. Setting the Business Objectives</strong></a></li>
<li><a class="reference internal" href="#b-is-rag-the-right-approach-genai-use-case-evaluation"><strong>B. Is RAG the Right Approach? (GenAI Use Case Evaluation)</strong></a></li>
<li><a class="reference internal" href="#c-defining-the-ml-problem"><strong>C. Defining the ML Problem</strong></a></li>
<li><a class="reference internal" href="#d-assessing-feasibility-risks"><strong>D. Assessing Feasibility &amp; Risks</strong></a></li>
<li><a class="reference internal" href="#e-defining-success-metrics"><strong>E. Defining Success Metrics</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-end-to-end-project-and-operational-blueprint"><strong>3. The End-to-End Project and Operational Blueprint</strong></a><ul>
<li><a class="reference internal" href="#a-the-llmops-tech-stack-an-architectural-blueprint">A. The LLMOps Tech Stack: An Architectural Blueprint</a></li>
<li><a class="reference internal" href="#b-the-four-core-pipelines-an-operational-blueprint">B. The Four Core Pipelines: An Operational Blueprint</a></li>
<li><a class="reference internal" href="#c-project-management-and-operational-strategy"><strong>C. Project Management and Operational Strategy</strong></a><ul>
<li><a class="reference internal" href="#project-stages-an-iterative-path-to-production"><strong>1. Project Stages: An Iterative Path to Production</strong></a></li>
<li><a class="reference internal" href="#cross-functional-team-roles"><strong>2. Cross-Functional Team &amp; Roles</strong></a></li>
<li><a class="reference internal" href="#versioning-and-governance-strategy"><strong>3. Versioning and Governance Strategy</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#d-comprehensive-evaluation-strategy-the-quality-gauntlet"><strong>D. Comprehensive Evaluation Strategy: The Quality Gauntlet</strong></a><ul>
<li><a class="reference internal" href="#offline-evaluation-component-wise-and-pipeline-testing">1. Offline Evaluation: Component-Wise and Pipeline Testing</a></li>
<li><a class="reference internal" href="#online-evaluation-testing-in-production">2. Online Evaluation: Testing in Production</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#data-ingestion-and-indexing-pipeline-building-the-knowledge-base"><strong>4. Data Ingestion and Indexing Pipeline: Building the Knowledge Base</strong></a><ul>
<li><a class="reference internal" href="#architecture-diagram-data-ingestion-and-indexing-pipeline"><strong>Architecture Diagram: Data Ingestion and Indexing Pipeline</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#experiment-management-iteration-the-path-to-precision"><strong>5. Experiment Management &amp; Iteration: The Path to Precision</strong></a><ul>
<li><a class="reference internal" href="#a-the-evaluation-framework-our-north-star"><strong>A. The Evaluation Framework: Our North Star</strong></a></li>
<li><a class="reference internal" href="#b-building-the-golden-dataset-synthetic-data-generation-for-rag-evaluation">B. Building the “Golden Dataset”: Synthetic Data Generation for RAG Evaluation</a><ul>
<li><a class="reference internal" href="#the-challenge-the-evaluation-bottleneck"><strong>The Challenge: The Evaluation Bottleneck</strong></a></li>
<li><a class="reference internal" href="#our-four-step-synthetic-generation-pipeline"><strong>Our Four-Step Synthetic Generation Pipeline</strong></a></li>
<li><a class="reference internal" href="#impact-on-the-project"><strong>Impact on the Project</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-the-golden-dataset-is-used-to-calculate-mrr"><strong>How the Golden Dataset is Used to Calculate MRR</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#continual-learning-the-embedding-model-fine-tuning-pipeline"><strong>6. Continual Learning: The Embedding Model Fine-tuning Pipeline</strong></a><ul>
<li><a class="reference internal" href="#a-artifacts-to-be-implemented"><strong>A. Artifacts to Be Implemented</strong></a></li>
<li><a class="reference internal" href="#b-architecture-diagram-embedding-model-fine-tuning-pipeline"><strong>B. Architecture Diagram: Embedding Model Fine-tuning Pipeline</strong></a></li>
<li><a class="reference internal" href="#note-training-triplets-dataset-vs-golden-evaluation-dataset">Note: <strong>Training Triplets Dataset</strong> vs <strong>Golden Evaluation Dataset</strong></a><ul>
<li><a class="reference internal" href="#dataset-1-the-training-triplets-dataset"><strong>Dataset 1: The Training Triplets Dataset</strong></a></li>
<li><a class="reference internal" href="#dataset-2-the-golden-evaluation-dataset"><strong>Dataset 2: The “Golden” Evaluation Dataset</strong></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#the-real-time-engine-the-inference-pipeline"><strong>7. The Real-Time Engine: The Inference Pipeline</strong></a><ul>
<li><a class="reference internal" href="#id2">A. Artifacts to Be Implemented</a></li>
<li><a class="reference internal" href="#b-architecture-diagram-real-time-inference-pipeline">B. Architecture Diagram: Real-Time Inference Pipeline</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-monitoring-and-observability-pipeline"><strong>8. The Monitoring and Observability Pipeline</strong></a><ul>
<li><a class="reference internal" href="#id3">A. Artifacts to Be Implemented</a></li>
<li><a class="reference internal" href="#b-architecture-diagram-monitoring-and-observability-pipeline">B. Architecture Diagram: Monitoring and Observability Pipeline</a></li>
</ul>
</li>
<li><a class="reference internal" href="#testing-in-production-validating-business-impact"><strong>9. Testing in Production: Validating Business Impact</strong></a><ul>
<li><a class="reference internal" href="#id4">A. Artifacts to Be Implemented</a></li>
<li><a class="reference internal" href="#b-the-a-b-testing-workflow-from-candidate-to-champion">B. The A/B Testing Workflow: From Candidate to Champion</a></li>
<li><a class="reference internal" href="#c-architecture-diagram-a-b-testing-in-production">C. Architecture Diagram: A/B Testing in Production</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-foundation-of-trust-governance-ethics-and-human-centric-design"><strong>10. The Foundation of Trust - Governance, Ethics, and Human-Centric Design</strong></a><ul>
<li><a class="reference internal" href="#a-comprehensive-model-governance">A. Comprehensive Model Governance</a></li>
<li><a class="reference internal" href="#b-responsible-ai-rai-principles-in-practice">B. Responsible AI (RAI) Principles in Practice</a></li>
<li><a class="reference internal" href="#c-the-human-element-team-structure-user-centric-design">C. The Human Element: Team Structure &amp; User-Centric Design</a></li>
</ul>
</li>
<li><a class="reference internal" href="#system-architecture-performance-and-economics"><strong>11. System Architecture, Performance, and Economics</strong></a><ul>
<li><a class="reference internal" href="#a-aws-system-architecture-diagram">A. AWS System Architecture Diagram</a></li>
<li><a class="reference internal" href="#b-sequence-diagram-the-anatomy-of-a-real-time-rag-query"><strong>B. Sequence Diagram: The Anatomy of a Real-Time RAG Query</strong></a><ul>
<li><a class="reference internal" href="#latency-budget-breakdown">Latency Budget Breakdown</a></li>
</ul>
</li>
<li><a class="reference internal" href="#c-inference-pipeline-bottlenecks-performance-optimizations"><strong>C. Inference Pipeline: Bottlenecks &amp; Performance Optimizations</strong></a></li>
<li><a class="reference internal" href="#d-estimated-monthly-costs-for-the-rag-system"><strong>D. Estimated Monthly Costs for the RAG System</strong></a></li>
<li><a class="reference internal" href="#non-recurring-costs"><strong>Non-Recurring Costs</strong></a></li>
<li><a class="reference internal" href="#key-financial-learnings"><strong>Key Financial Learnings</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-data-ingestion-and-indexing-pipeline">Implementation: Data Ingestion and Indexing Pipeline</a><ul>
<li><a class="reference internal" href="#python-scripts-core-logic">Python Scripts (Core Logic)</a></li>
<li><a class="reference internal" href="#unit-tests">Unit Tests</a></li>
<li><a class="reference internal" href="#pipeline-orchestration-aws-step-functions">Pipeline Orchestration (AWS Step Functions)</a></li>
<li><a class="reference internal" href="#infrastructure-as-code-terraform">Infrastructure as Code (Terraform)</a></li>
<li><a class="reference internal" href="#integration-test">Integration Test</a></li>
<li><a class="reference internal" href="#ci-cd-github-actions-workflow">CI/CD GitHub Actions Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-inference-pipeline">Implementation: Inference Pipeline</a><ul>
<li><a class="reference internal" href="#id5">Python Scripts (Core Logic)</a></li>
<li><a class="reference internal" href="#id6">Unit Tests</a></li>
<li><a class="reference internal" href="#id7">Infrastructure as Code (Terraform)</a></li>
<li><a class="reference internal" href="#id8">Integration Test</a></li>
<li><a class="reference internal" href="#id9">CI/CD GitHub Actions Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-the-monitoring-and-observability-pipeline">Implementation: The Monitoring and Observability Pipeline</a><ul>
<li><a class="reference internal" href="#id10">Python Scripts (Core Logic)</a></li>
<li><a class="reference internal" href="#id11">Unit Tests</a></li>
<li><a class="reference internal" href="#id12">Infrastructure as Code (Terraform)</a></li>
<li><a class="reference internal" href="#id13">Integration Test</a></li>
<li><a class="reference internal" href="#id14">CI/CD GitHub Actions Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-testing-in-production">Implementation: Testing in Production</a><ul>
<li><a class="reference internal" href="#python-scripts-analysis">Python Scripts (Analysis)</a></li>
<li><a class="reference internal" href="#id15">Unit Tests</a></li>
<li><a class="reference internal" href="#id16">Integration Test</a></li>
<li><a class="reference internal" href="#id17">CI/CD GitHub Actions Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-embedding-model-fine-tuning-pipeline">Implementation: Embedding Model Fine-tuning Pipeline</a><ul>
<li><a class="reference internal" href="#python-scripts-pipeline-components">Python Scripts (Pipeline Components)</a></li>
<li><a class="reference internal" href="#unit-tests-pytest">Unit Tests (pytest)</a></li>
<li><a class="reference internal" href="#pipeline-orchestration-airflow-dag">Pipeline Orchestration (Airflow DAG)</a></li>
<li><a class="reference internal" href="#id18">Infrastructure as Code (Terraform)</a></li>
<li><a class="reference internal" href="#id19">Integration Test</a></li>
<li><a class="reference internal" href="#id20">CI/CD GitHub Actions Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#guide-to-fine-tuning-re-ranker-model">Guide to Fine-tuning Re-ranker Model</a><ul>
<li><a class="reference internal" href="#the-why-the-two-stage-retrieval-process"><strong>The “Why”: The Two-Stage Retrieval Process</strong></a></li>
<li><a class="reference internal" href="#the-dataset-the-crucial-difference"><strong>The Dataset: The Crucial Difference</strong></a></li>
<li><a class="reference internal" href="#data-sourcing-and-creation-for-the-re-ranker"><strong>Data Sourcing and Creation for the Re-ranker</strong></a></li>
<li><a class="reference internal" href="#the-model-and-training-process"><strong>The Model and Training Process</strong></a></li>
<li><a class="reference internal" href="#the-mlops-pipeline-for-re-ranker-fine-tuning"><strong>The MLOps Pipeline for Re-ranker Fine-tuning</strong></a></li>
<li><a class="reference internal" href="#architecture-diagram"><strong>Architecture Diagram</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#synthetic-dataset-generation-to-evaluate-retrieval-and-ranking">Synthetic Dataset Generation to Evaluate Retrieval and Ranking</a><ul>
<li><a class="reference internal" href="#a-python-script"><strong>A. Python Script</strong></a></li>
<li><a class="reference internal" href="#b-prerequisites-how-to-run"><strong>B. Prerequisites &amp; How to Run</strong></a></li>
<li><a class="reference internal" href="#c-output-golden-evaluation-dataset-jsonl"><strong>C. Output (<code class="docutils literal notranslate"><span class="pre">golden_evaluation_dataset.jsonl</span></code>)</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/scripts/furo.js?v=4e2eecee"></script>
    </body>
</html>