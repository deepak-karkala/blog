<!doctype html>
<html class="no-js" lang="en" data-content_root="">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Reviews Summarisation" href="ecom_summarisation.html" /><link rel="prev" title="Customer Lifetime Value" href="ecom_cltv.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><!-- Generated with Sphinx 7.1.2 and Furo 2024.05.06 -->
        <title>Real-Time Purchase Intent Scoring - Home</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=387cc868" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/style.css?v=8a7ff5ee" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" /
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Home</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">Home</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Past Experiences</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Past Experiences</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="iot_anomaly.html">Anomaly Detection in Time Series IoT Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="iot_forecasting.html">Energy Demand Forecasting in Time Series IoT Data</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="adas_engine/index.html">ADAS: Data Engine</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of ADAS: Data Engine</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch0_business_challenge.html">Business Challenge and Goals</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch1_ml_problem_framing.html">ML Problem Framing</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch2_operational_strategy.html">Planning, Operational Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch3_pipelines_workflows.html">Workflows, Team, Roles</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch4_testing_strategy.html">Testing Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch6_data_ingestion_workflows.html">Data Ingestion Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch7_scene_understanding_data_mining.html">Scene Understanding &amp; Data Mining</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch8_model_training.html">Model Training &amp; Experimentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch9_packaging_promotion.html">Packaging, Evaluation &amp; Promotion Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch10_deployment_serving.html">Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch11_monitoring_continual_learning.html">Monitoring &amp; Continual Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch12_cost_lifecycle_compliance.html">Cost, Lifecycle, Compliance</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch13_reliability_capacity_maps.html">Reliability, Capacity, Maps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ecom_cltv.html">Customer Lifetime Value</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Real-Time Purchase Intent Scoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_summarisation.html">Reviews Summarisation</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_rag.html">RAG-Based Product Discovery</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../projects/index.html">Projects</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Projects</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/nlp/index.html">Natural Language Processing</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Natural Language Processing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/airbnb_alternate_search/about/index.html">Airbnb Listing description based Semantic Search</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/cv/index.html">Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Computer Vision</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/ecommerce_image_segmentation/about/index.html">Image Segmentation for Ecommerce Products</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/ml/index.html">Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Machine Learning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/airbnb_price_modeling/about/index.html">Predictive Price Modeling for Airbnb listings</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications/index.html">Patents, Papers, Thesis</a></li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../agents/index.html">AI Agents: A Lead Engineer’s Handbook</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of AI Agents: A Lead Engineer’s Handbook</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch1_intro.html">Agent Fundamentals: What, Why, and When?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch2_patterns.html">Agentic Patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch5_context_engineering.html">Context Engineering for AI Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch6_case_studies.html">The State of the Industry: Insights from the Field</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch7_conclusion.html"><strong>Conclusion: The Lead Engineer’s Mental Model for Building Agents</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_cost.html">Cost Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_data.html">Data Management and Knowledge Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_deploy.html">Deployment and Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_guardrails.html">Guardrails</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_hitl.html">Human-in-the-Loop (HITL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_latency.html">Latency Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_llm.html">LLM – Prompts, Goals, and Persona</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_memory.html">Managing Agent Memory (Short-Term and Long-Term)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_monitor.html">Monitoring and Observability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_orchestration.html">Orchestration and Task Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_prod.html">Production Challenges and Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_security.html">Securing AI Agents and Preventing Abuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_tool.html">Tool Use and Integration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_trust.html">Building Trustworthy and Ethical AI Agents</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">MLOps</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of MLOps</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch1_problem_framing.html">ML Problem framing</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of ML Problem framing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/ch2_blueprint_operational_strategy.html">The MLOps Blueprint &amp; Operational Strategy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch2a_platform/index.html">ML Platforms</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of ML Platforms</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/ml_platforms.html">ML Platforms: How to</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/uber.html">Uber Michelangelo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/linkedin.html">LinkedIn DARWIN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/netflix.html">Netflix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/shopify.html">Shopify Merlin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/zomato.html">Zomato: Real-time ML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/coveo.html">Coveo: MLOPs at reasonable scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/monzo.html">Monzo ML Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/didact.html">Didact AI</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch3_project_planning/index.html">Project Planning</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of Project Planning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/prd.html">Project Requirements Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/tech_stack.html">Tech Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/config_management.html">Config Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/pipeline_design.html">Pipeline Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/environment_strategy.html">Environment Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/cicd_branching_model.html">CI/CD Strategy and Branching Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/directory_structure.html">Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/env_branchind_cicd_deployment.html">Environments, Branching, CI/CD, and Deployments Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/project_management.html">Project Management for MLOps</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch4_data_discovery/index.html">Data Sourcing, Discovery</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of Data Sourcing, Discovery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/data_sourcing_discovery.html">Data Sourcing, Discovery &amp; Understanding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/ch4_project.html">Project-Trending Now: Implementing Web Scraping, Ingestion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/industry_case_studies.html">Data Discovery Platforms: Industry Case Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/facebook_nemo.html">Facebook: Nemo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/netflix_metacat.html">Netflix Metacat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/uber_databook.html">Uber Databook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/linkedin_datahub.html">LinkedIn Datahub</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch7_model_development/index.html">Model Development, Tuning, Selection, Ensembles, Calibration</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of Model Development, Tuning, Selection, Ensembles, Calibration</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/ch7_model_development.html">Chapter 7: Model Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/dl_training_playbook.html">How to train DL Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/development.html">Model Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/industry_lessons.html">Model Development: Lessons from production systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/ensembles.html"><strong>Model Ensembles</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/selection.html">Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/tuning_hypopt.html">Hyperparameter Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/expt_tracking.html">ML Expt tracking, Data Lineage, Model Registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/calibration.html">Model Calibration</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch10_deployment_serving/index.html">Model Deployment &amp; Serving</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of Model Deployment &amp; Serving</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/ch10_deployment_serving.html">Chapter 10: Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/guide_deployment_serving.html">Guide: Model Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/guide_inference_stack.html">Deep Dive: Inference Stack</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/index.html">Monitoring, Observability, Drift, Interpretability</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of Monitoring, Observability, Drift, Interpretability</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/ch11_monitor_observe_drift.html">Chapter 11: Monitoring, Observability, Drifts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_monitor_observe_drift.html">Guide: ML System Failures, Data Distribution Shifts, Monitoring, and Observability</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_interpretability_shap_lime.html">Interpretability, SHAP, LIME</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_stack.html">Prometheus + Grafana and ELK Stacks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/index.html">Continual learning, Retraining, A/B Testing</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of Continual learning, Retraining, A/B Testing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/ch12_continual_learning_prod_testing.html">Chapter 12: Continual Learning &amp; Production Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_continual_learning.html">Continual Learning &amp; Model Retraining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_ab_testing.html">A/B Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_ab_testing_industry_lessons.html">A/B Testing &amp; Experimentation: Industry lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_prod_testing_expt.html">Guide: Production Testing &amp; Experimentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/dr_prod_testing_expt.html">Deep Research: Production Testing &amp; Experimentation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/index.html">PyTorch</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of PyTorch</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/general.html">General</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/state_dict.html">state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/distributed_data_parallel.html">Distributed Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/ddp_under_the_hood.html">DDP: Under the Hood</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/dp_ddp.html">DP vs DDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/fsdp.html">FSDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/tensor_parallelism.html">Tensor parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/pipeline_parallelism.html">Pipeline Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/device_mesh.html">Device Mesh</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lld/index.html">Low Level Design</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of Low Level Design</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lld/parking_lot.html">Parking Lot</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../visualization/index.html">Data Visualization Projects</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/past_experiences/ecom_propensity.md.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="real-time-purchase-intent-scoring">
<h1>Real-Time Purchase Intent Scoring<a class="headerlink" href="#real-time-purchase-intent-scoring" title="Permalink to this heading">¶</a></h1>
<section id="id1">
<h2><a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<hr class="docutils" />
<section id="tldr-a-production-grade-mlops-system-for-real-time-purchase-intent-scoring">
<h3><strong>TLDR: A Production-Grade MLOps System for Real-Time Purchase Intent Scoring</strong><a class="headerlink" href="#tldr-a-production-grade-mlops-system-for-real-time-purchase-intent-scoring" title="Permalink to this heading">¶</a></h3>
<ul>
<li><p><strong>Challenge:</strong> The core business challenge was to move beyond a static e-commerce catalog and increase online sales by understanding user intent in real-time. The goal was to build a system that could dynamically predict a user’s likelihood to make a purchase during their session, enabling timely and personalized interventions (like targeted offers) to reduce cart abandonment and improve conversion.</p></li>
<li><p><strong>My Role &amp; Solution:</strong> As the lead <strong>ML Engineer and Data Scientist</strong> on a lean, three-person team, I designed and implemented the end-to-end MLOps platform from the ground up. My solution involved a hybrid batch and streaming architecture to deliver both rich historical context and real-time responsiveness.</p>
<p>My specific contributions included:</p>
<ul class="simple">
<li><p><strong>Feature Engineering Pipelines:</strong> Architected and built both daily batch (Spark) and real-time streaming (Spark Streaming) feature pipelines. I implemented a <strong>Feast Feature Store</strong> to solve training-serving skew and provide a single source of truth for features.</p></li>
<li><p><strong>ML Model Development:</strong> Led the iterative modeling process, starting with simple baselines and progressing to a high-performance <strong>LightGBM</strong> model. I focused on not just offline accuracy (AUC) but also production-critical factors like model calibration.</p></li>
<li><p><strong>ML Training &amp; Deployment Pipelines:</strong> Built a fully automated retraining pipeline using <strong>Apache Airflow</strong> and <strong>Amazon SageMaker</strong>. I created a CI/CD workflow with <strong>GitHub Actions</strong> that automatically deploys new models as canary releases to a SageMaker endpoint, ensuring safe, zero-downtime updates.</p></li>
<li><p><strong>Monitoring &amp; Continual Learning:</strong> Implemented a comprehensive monitoring suite to track operational metrics, data drift, and model performance degradation. This system automatically triggers retraining runs when needed, creating a closed-loop system that adapts to new data.</p></li>
</ul>
<p><strong>Tech Stack:</strong> AWS (SageMaker, Kinesis, EMR, Lambda, ElastiCache), Apache Airflow, MLflow, Feast, DVC, Great Expectations, Spark, LightGBM, GitHub Actions, Terraform.</p>
</li>
<li><p><strong>Impact:</strong> The system delivered a clear and measurable business impact, accomplished by a small, agile team.</p>
<ul class="simple">
<li><p>Increased the overall <strong>user conversion rate by 5.2%</strong> as measured in a two-week A/B test.</p></li>
<li><p>Increased the <strong>Average Order Value (AOV) by 3.5%</strong> for users who interacted with a personalized offer.</p></li>
<li><p>Automated the model lifecycle, <strong>reducing the time to deploy a new model from over a week of manual effort to a 2-hour automated pipeline run.</strong></p></li>
<li><p>Reduced p99 inference latency by <strong>40%</strong> through model quantization and a Redis-based feature cache, ensuring a smooth user experience even during peak traffic.</p></li>
</ul>
</li>
<li><p><strong>System Architecture:</strong> The diagram below illustrates the complete MLOps architecture. The components highlighted in blue represent the core systems I was directly responsible for building and managing.</p></li>
</ul>
<img src="../_static/past_experiences/ecom_propensity/contributions.svg" width="100%" style="background-color: #FCF1EF;"/>
</section>
<hr class="docutils" />
<section id="business-challenge-from-anonymous-clicks-to-intent-driven-conversions">
<h3><strong>1. Business Challenge: From Anonymous Clicks to Intent-Driven Conversions</strong><a class="headerlink" href="#business-challenge-from-anonymous-clicks-to-intent-driven-conversions" title="Permalink to this heading">¶</a></h3>
<p>The paradigm of e-commerce has irrevocably shifted. A static digital catalog is no longer sufficient; today’s consumers, shaped by the hyper-personalized experiences of platforms like Netflix and Amazon, expect a similar level of contextual awareness and tailored guidance from every online interaction. The central challenge for any modern retailer is to transform their digital storefront from a passive repository of products into a dynamic, intelligent system that anticipates user needs and proactively guides them toward a satisfying purchase.</p>
<p>This mandate for personalization is not merely about addressing customers by name. It is about understanding a visitor’s underlying intent at each stage of their journey and adapting the experience in real-time. Whether a user is a first-time anonymous visitor idly browsing or a known, loyal customer executing a specific purchase, the platform must intelligently adjust its content, messaging, and recommendations.</p>
<p>However, this creates a delicate and critical balance. While a vast majority of shoppers are more likely to purchase from a company that offers personalized experiences, they are quickly alienated by tactics perceived as intrusive or overly algorithmic. The most effective personalization is often invisible; the customer simply feels the journey is effortless and intuitive. This elevates the importance of not just prediction, but also interpretability, ensuring every action taken is genuinely helpful rather than jarring.</p>
<p>The business case for mastering this challenge is compelling and quantifiable. Industry analyses consistently demonstrate a significant return on investment. Brands that excel at personalization report a <strong>10-15% lift in revenue</strong> and an increase in marketing spend efficiency of <strong>10-30%</strong>. By analyzing a user’s behavior as it happens—their “clickstream”—an e-commerce platform can make timely interventions that directly influence outcomes. Proactive, data-driven actions can reduce cart abandonment, increase average order value through targeted upsells, and ultimately boost the overall conversion rate.</p>
<p>To measure success, the system’s performance must be tied directly to core business outcomes:</p>
<ul class="simple">
<li><p><strong>Primary Business KPIs:</strong> Conversion Rate, Average Order Value (AOV), and overall Revenue Lift.</p></li>
<li><p><strong>Secondary Engagement KPIs:</strong> Click-Through Rate (CTR) on personalized offers, increased Session Duration, and reduced Bounce Rates, which serve as leading indicators of user satisfaction.</p></li>
</ul>
<p>Therefore, the fundamental business challenge is to translate the high-velocity, high-volume stream of raw customer clicks into a nuanced understanding of purchase intent, and to use that understanding to power a real-time personalization engine that measurably improves the customer experience and drives tangible business growth.</p>
</section>
<hr class="docutils" />
<section id="ml-problem-framing">
<h3>2. ML Problem Framing<a class="headerlink" href="#ml-problem-framing" title="Permalink to this heading">¶</a></h3>
<p>Before a single line of code is written, a successful machine learning project begins with a rigorous process of problem framing. This is the crucial stage of translating a business vision into a well-defined, feasible, and measurable machine learning task. A flawed framing can lead a project astray, wasting resources on a model that, no matter how accurate, fails to deliver real-world value.</p>
<p>This section outlines the blueprint for the purchase propensity model, ensuring that the technical solution is precisely aligned with the business objectives.</p>
<section id="setting-the-business-objectives">
<h4>2.1 Setting the Business Objectives<a class="headerlink" href="#setting-the-business-objectives" title="Permalink to this heading">¶</a></h4>
<p>Every ML project must be anchored to a genuine business need. For this project, the primary objective is to <strong>increase online sales by delivering timely, personalized interventions to users during their browsing session.</strong></p>
<p>This high-level goal was defined in collaboration with key stakeholders, each with a unique perspective:</p>
<ul class="simple">
<li><p><strong>Product &amp; Marketing Teams:</strong> Focused on increasing <code class="docutils literal notranslate"><span class="pre">Conversion</span> <span class="pre">Rate</span></code> and <code class="docutils literal notranslate"><span class="pre">Average</span> <span class="pre">Order</span> <span class="pre">Value</span> <span class="pre">(AOV)</span></code>. They are interested in identifying “on the fence” customers who could be nudged towards a purchase with a targeted offer, and “high-intent” customers who could be receptive to up-sell or cross-sell recommendations.</p></li>
<li><p><strong>Sales &amp; Finance Teams:</strong> Concerned with the return on investment (ROI) of promotions. They want to ensure that discounts are not offered to users who would have purchased anyway, maximizing margin.</p></li>
<li><p><strong>MLOps &amp; Engineering Teams:</strong> Focused on system stability, maintainability, and performance. Their primary concerns are the prediction service’s <code class="docutils literal notranslate"><span class="pre">latency</span></code> and <code class="docutils literal notranslate"><span class="pre">throughput</span></code>, ensuring it does not degrade the overall user experience.</p></li>
</ul>
<p>A critical requirement established early on was the need for <strong>model interpretability</strong>. To gain trust and enable debugging, the marketing team needs to understand <em>why</em> a user was assigned a high or low propensity score. This allows them to analyze the effectiveness of different personalization strategies and diagnose underperforming campaigns.</p>
</section>
<section id="is-machine-learning-the-right-approach">
<h4>2.2 Is Machine Learning the Right Approach?<a class="headerlink" href="#is-machine-learning-the-right-approach" title="Permalink to this heading">¶</a></h4>
<p>While a simple, rules-based system could be implemented (e.g., “if a user has an item in their cart for &gt; 1 hour, show a banner”), this approach is brittle and fails to capture the nuanced complexity of user behavior. Machine learning is the correct approach for this problem for several key reasons:</p>
<ol class="arabic simple">
<li><p><strong>Complex Patterns:</strong> User purchase intent is influenced by a non-linear combination of dozens of signals (browsing history, click patterns, time of day, product characteristics). ML models are designed to learn these intricate patterns from data, something a human-defined rules engine cannot do effectively.</p></li>
<li><p><strong>Scale:</strong> The system must evaluate millions of user events per day across thousands of products and users. ML provides a scalable way to make millions of cheap, consistent predictions.</p></li>
<li><p><strong>Adaptability:</strong> Customer behavior changes with market trends, seasonality, and marketing campaigns. A static rules engine would quickly become outdated. An ML system, through retraining, can adapt to these evolving patterns.</p></li>
</ol>
<p>The following decision flow confirms that ML is a suitable and necessary tool for this challenge.</p>
<img src="../_static/past_experiences/ecom_propensity/ml_use_case_evaluation.svg" width="70%" style="background-color: #FCF1EF;"/>
</section>
<section id="defining-the-ml-problem">
<h4>2.3 Defining the ML Problem<a class="headerlink" href="#defining-the-ml-problem" title="Permalink to this heading">¶</a></h4>
<p>With the need for ML established, the business problem is translated into a precise technical formulation.</p>
<ul class="simple">
<li><p><strong>Ideal Outcome:</strong> Deliver the optimal intervention (e.g., a specific recommendation, a tailored discount, or no action at all) to each user at the right moment to maximize the probability of a valuable conversion.</p></li>
<li><p><strong>Model’s Goal:</strong> To empower the “ideal outcome,” the model’s direct goal is to <strong>predict the probability (a score from 0.0 to 1.0) that a user will complete a purchase within their current session.</strong></p></li>
<li><p><strong>ML Task Type:</strong> This is framed as a <strong>Regression</strong> problem. Predicting a continuous probability score is more flexible than a simple binary classification. It allows the business to define multiple thresholds for different actions, tailoring the aggressiveness of the intervention to the model’s confidence.</p></li>
<li><p><strong>Proxy Label:</strong> The ground truth for training the model will be the <code class="docutils literal notranslate"><span class="pre">purchase</span></code> event within a session. If a user makes a purchase, their session is labeled with a target value of 1; otherwise, it is labeled 0. This is a strong proxy for purchase intent, though it’s acknowledged that it doesn’t account for post-purchase events like returns. It is, however, far superior to weaker proxies like <code class="docutils literal notranslate"><span class="pre">add_to_cart</span></code> (which suffers from high abandonment rates) or <code class="docutils literal notranslate"><span class="pre">product_view</span></code> (which can optimize for “clickbait” products rather than genuine interest).</p></li>
</ul>
</section>
<section id="assessing-feasibility-risks">
<h4>2.4 Assessing Feasibility &amp; Risks<a class="headerlink" href="#assessing-feasibility-risks" title="Permalink to this heading">¶</a></h4>
<p>A critical analysis was performed to ensure the project was achievable and to identify potential roadblocks early.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Category</p></th>
<th class="head text-left"><p>Checkpoint</p></th>
<th class="head text-left"><p>Assessment &amp; Mitigation Strategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Data</strong></p></td>
<td class="text-left"><p>Sufficient labeled data?</p></td>
<td class="text-left"><p><strong>Green:</strong> Large volumes of historical clickstream and purchase data are available.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Features available at serving time?</p></td>
<td class="text-left"><p><strong>Yellow (Risk):</strong> Historical features (e.g., <code class="docutils literal notranslate"><span class="pre">lifetime_value</span></code>) are not available for anonymous, first-time users. <strong>Mitigation:</strong> Two distinct models or feature sets will be required: one for known users and a simpler, session-based model for anonymous users.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Privacy/Regulatory compliant?</p></td>
<td class="text-left"><p><strong>Yellow (Risk):</strong> Use of user data must comply with GDPR/CCPA. <strong>Mitigation:</strong> The project will only use first-party data. All features will be aggregated and anonymized where possible. A legal review is a required step before production.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Problem &amp; Model</strong></p></td>
<td class="text-left"><p>High reliability required?</p></td>
<td class="text-left"><p><strong>Medium:</strong> A wrong prediction (e.g., offering a needless discount) has a direct financial cost, but is not catastrophic.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Latency target achievable?</p></td>
<td class="text-left"><p><strong>Red (High Risk):</strong> The requirement for <strong>&lt;100ms p99</strong> inference latency is a significant technical challenge. <strong>Mitigation:</strong> This requires careful model selection (favoring efficiency), infrastructure optimization (e.g., GPUs, Triton server), and an online feature store.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Adversarial attacks likely?</p></td>
<td class="text-left"><p><strong>Low:</strong> Not a primary concern for this use case, unlike fraud detection.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Ethics &amp; Fairness</strong></p></td>
<td class="text-left"><p>Potential for bias?</p></td>
<td class="text-left"><p><strong>Yellow (Risk):</strong> The model could learn to offer better discounts to users from high-income zip codes or unfairly penalize new users. <strong>Mitigation:</strong> Implement fairness monitoring. Regularly audit model predictions across key user segments.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Cost &amp; ROI</strong></p></td>
<td class="text-left"><p>Positive ROI projected?</p></td>
<td class="text-left"><p><strong>Green:</strong> A preliminary cost-benefit analysis, based on a conservative estimate of a 2-3% conversion lift, shows a strong positive ROI against the projected infrastructure and personnel costs.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="defining-success-metrics">
<h4>2.5 Defining Success Metrics<a class="headerlink" href="#defining-success-metrics" title="Permalink to this heading">¶</a></h4>
<p>To measure progress and ultimate success, a clear distinction is made between business, model, and operational metrics. These are not static goals but will be tracked continuously throughout the project lifecycle.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Metric Type</p></th>
<th class="head text-left"><p>Success Metrics</p></th>
<th class="head text-left"><p>How to Measure &amp; Evaluate</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Business Success</strong></p></td>
<td class="text-left"><p><strong>Conversion Rate Lift:</strong> The primary measure of impact.</p></td>
<td class="text-left"><p>Measured via rigorous <strong>A/B testing</strong>, comparing the model-driven personalization strategy against a non-personalized control group. A statistically significant lift of <strong>&gt;2%</strong> is the target.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Increase in Average Order Value (AOV):</strong></p></td>
<td class="text-left"><p>Measured in the same A/B test. An increase indicates successful up-selling and cross-selling.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Model Evaluation</strong></p></td>
<td class="text-left"><p><strong>Area Under the ROC Curve (AUC):</strong> The primary offline metric for optimizing the model’s ability to rank users by their likelihood to purchase. Target <strong>AUC &gt; 0.85</strong>.</p></td>
<td class="text-left"><p>Evaluated on a held-out test set during each training run and tracked in MLflow.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Precision/Recall at specific thresholds:</strong> Satisficing metrics.</p></td>
<td class="text-left"><p>E.g., at the 0.9 score threshold for offering discounts, <strong>Precision must be &gt;= 75%</strong> to minimize wasted marketing spend.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Operational Health</strong></p></td>
<td class="text-left"><p><strong>Inference Latency:</strong> A critical non-functional requirement.</p></td>
<td class="text-left"><p>Monitored in real-time. The service must maintain <strong>p99 latency &lt; 100ms</strong>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Serving Cost:</strong></p></td>
<td class="text-left"><p>Measured as <strong>cost per 1,000 inferences</strong>. Must be tracked to ensure the solution remains within the projected budget.</p></td>
</tr>
</tbody>
</table>
</div>
<p>By meticulously framing the problem through these five lenses, we establish a robust foundation. We have a clear business mandate, a well-defined technical problem, an understanding of the risks, and a precise definition of what success looks like. This blueprint will guide all subsequent engineering and data science decisions.</p>
</section>
</section>
<hr class="docutils" />
<section id="mlops-project-planning-and-operational-strategy">
<h3>3. MLOps Project Planning and Operational Strategy<a class="headerlink" href="#mlops-project-planning-and-operational-strategy" title="Permalink to this heading">¶</a></h3>
<p>Building a real-time personalization system is not about creating a single, static model; it is about engineering a robust, evolving ecosystem that can adapt to changing data and business needs. Adopting a Machine Learning Operations (MLOps) mindset from the outset is paramount to success. This section outlines the strategic plan, architectural principles, and technology choices that will govern the project’s entire lifecycle.</p>
<section id="the-mlops-first-mindset-building-a-system-not-just-a-model">
<h4>3.1 The MLOps-First Mindset: Building a System, Not Just a Model<a class="headerlink" href="#the-mlops-first-mindset-building-a-system-not-just-a-model" title="Permalink to this heading">¶</a></h4>
<p>MLOps is a set of practices that combines Machine Learning, DevOps, and Data Engineering to automate and streamline the end-to-end machine learning lifecycle. The fundamental goal is to move away from artisanal, one-off model development and toward an industrialized process that enables continuous integration, continuous delivery, and continuous training (CI/CD/CT).</p>
<p>This is especially critical for a purchase intent model, where real-world user behavior is constantly changing, rendering static models obsolete and prone to performance degradation—a phenomenon known as model drift. By embracing MLOps, we commit to building a system that is reproducible, testable, and maintainable from day one.</p>
</section>
<section id="the-architectural-triad-balancing-offline-nearline-and-online-computation">
<h4>3.2 The Architectural Triad: Balancing Offline, Nearline, and Online Computation<a class="headerlink" href="#the-architectural-triad-balancing-offline-nearline-and-online-computation" title="Permalink to this heading">¶</a></h4>
<p>A core strategic decision in designing any large-scale personalization system is how to partition computational tasks. The architecture will be a hybrid, leveraging the strengths of three distinct modes to balance computational complexity, data freshness, and response latency:</p>
<ul class="simple">
<li><p><strong>Offline Computation:</strong> Reserved for tasks that are computationally intensive and not subject to real-time constraints.</p>
<ul>
<li><p><strong>Use Cases:</strong> Large-scale model training on years of historical data; batch feature computation (e.g., calculating customer lifetime value).</p></li>
</ul>
</li>
<li><p><strong>Online (Real-Time) Computation:</strong> For tasks that must respond to user interactions immediately, operating under strict latency SLAs (sub-100 milliseconds).</p>
<ul>
<li><p><strong>Use Cases:</strong> Model inference (scoring); real-time feature retrieval from a low-latency online feature store.</p></li>
</ul>
</li>
<li><p><strong>Nearline Computation:</strong> An intelligent compromise for tasks triggered by real-time events but executed asynchronously.</p>
<ul>
<li><p><strong>Use Cases:</strong> Session-based feature updates after a user’s session ends, ensuring the next visit benefits from the context of the last one without performing complex aggregations in the real-time path.</p></li>
</ul>
</li>
</ul>
</section>
<section id="the-mlops-stack-canvas-technology-stack-selection">
<h4>3.3 The MLOps Stack Canvas: Technology Stack Selection<a class="headerlink" href="#the-mlops-stack-canvas-technology-stack-selection" title="Permalink to this heading">¶</a></h4>
<p>Using the MLOps Stack Canvas framework, we can systematically select the tools and technologies for each component of our system. The following table outlines our chosen stack, focusing on a combination of managed AWS services for infrastructure and best-of-breed open-source tools for core ML capabilities to balance speed of development with flexibility and control.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>MLOps Canvas Block</p></th>
<th class="head text-left"><p>Capability</p></th>
<th class="head text-left"><p>Chosen Tool/Technology</p></th>
<th class="head text-left"><p>Justification &amp; Trade-offs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Value Proposition</strong></p></td>
<td class="text-left"><p>Business Goal Alignment</p></td>
<td class="text-left"><p><strong>Propensity Scoring Model</strong></p></td>
<td class="text-left"><p>The core ML asset designed to directly influence conversion rate and AOV by enabling targeted, real-time personalization.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data Sources &amp; Versioning</strong></p></td>
<td class="text-left"><p>Data Ingestion</p></td>
<td class="text-left"><p><strong>AWS Kinesis Data Streams</strong></p></td>
<td class="text-left"><p>Chosen for its seamless integration with the AWS ecosystem (API Gateway, Lambda) and its ability to handle high-throughput, real-time data ingestion with managed scaling.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Data Storage</p></td>
<td class="text-left"><p><strong>Amazon S3 Data Lake</strong></p></td>
<td class="text-left"><p>The definitive source of truth for all historical data. Its cost-effectiveness, durability, and integration with the entire AWS analytics stack make it the ideal choice for an offline store.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Data Versioning</p></td>
<td class="text-left"><p><strong>DVC (Data Version Control)</strong></p></td>
<td class="text-left"><p>Tracks large data files and models alongside code in Git without bloating the repository. Provides crucial reproducibility for datasets used in training.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Experiment Management</strong></p></td>
<td class="text-left"><p>Experiment Tracking</p></td>
<td class="text-left"><p><strong>MLflow Tracking</strong></p></td>
<td class="text-left"><p>An open-source standard for logging experiment parameters, metrics, and artifacts. Provides a clear, auditable history of all model development efforts, fostering collaboration and reproducibility.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>ML Frameworks</p></td>
<td class="text-left"><p><strong>Scikit-learn, XGBoost, TensorFlow/PyTorch</strong></p></td>
<td class="text-left"><p>A pragmatic selection. Start with simpler, highly performant models like LightGBM/XGBoost for a strong, interpretable baseline. Use deep learning frameworks for more complex sequential modeling if justified by performance needs.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Feature Store &amp; Workflows</strong></p></td>
<td class="text-left"><p>Feature Store</p></td>
<td class="text-left"><p><strong>Feast (Open Source)</strong></p></td>
<td class="text-left"><p><strong>Chosen to avoid vendor lock-in</strong> and for its strong integration with multiple online (Redis, DynamoDB) and offline (S3, Redshift) stores. Provides the core abstraction needed to solve training-serving skew.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Workflow Orchestration</p></td>
<td class="text-left"><p><strong>Apache Airflow</strong></p></td>
<td class="text-left"><p>The industry standard for orchestrating complex, scheduled batch workflows. Ideal for managing the daily/weekly feature engineering and model retraining pipelines.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>DevOps &amp; Code Management</strong></p></td>
<td class="text-left"><p>Source Control</p></td>
<td class="text-left"><p><strong>Git (GitHub)</strong></p></td>
<td class="text-left"><p>The universal standard for source code management and collaborative development.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>CI/CD</p></td>
<td class="text-left"><p><strong>GitHub Actions</strong></p></td>
<td class="text-left"><p>Tightly integrated with the source code repository. Used for automating code builds, static analysis, and unit/integration testing for all system components.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>CI/CT/CD</strong></p></td>
<td class="text-left"><p>ML Pipeline Orchestration</p></td>
<td class="text-left"><p><strong>Airflow + SageMaker SDK</strong></p></td>
<td class="text-left"><p>Airflow will act as the master orchestrator, triggering and managing parameterized AWS SageMaker Training and Processing Jobs. This provides a balance of open-source control and managed, scalable execution.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p>Automated ML Testing</p></td>
<td class="text-left"><p><strong>Pytest, Great Expectations</strong></p></td>
<td class="text-left"><p>Pytest for unit testing Python code. Great Expectations for data validation and quality checks within the feature and training pipelines to detect data issues early.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Model Registry</strong></p></td>
<td class="text-left"><p>Model Registry</p></td>
<td class="text-left"><p><strong>MLflow Model Registry</strong></p></td>
<td class="text-left"><p>Provides a central hub for managing the lifecycle of ML models. Manages model versions, stages (Staging, Production), and annotations, and integrates seamlessly with the CI/CD pipeline for automated deployments.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Deployment &amp; Serving</strong></p></td>
<td class="text-left"><p>Model Serving</p></td>
<td class="text-left"><p><strong>Amazon SageMaker Endpoints</strong></p></td>
<td class="text-left"><p>A fully managed service that simplifies deploying models as scalable, secure API endpoints. It handles infrastructure provisioning, autoscaling, and monitoring, accelerating the path to production.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Release Strategy</p></td>
<td class="text-left"><p><strong>Canary Deployment</strong></p></td>
<td class="text-left"><p>A safe, gradual rollout strategy. Traffic is incrementally shifted to the new model version while key business and operational metrics are monitored, minimizing the “blast radius” of a potential failure.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Monitoring &amp; Observability</strong></p></td>
<td class="text-left"><p>Infrastructure &amp; Model</p></td>
<td class="text-left"><p><strong>Amazon CloudWatch</strong></p></td>
<td class="text-left"><p>The native AWS monitoring solution. Used for tracking operational metrics (latency, error rate, CPU/memory), setting alerts on performance thresholds, and collecting logs from all services.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Data &amp; Concept Drift</p></td>
<td class="text-left"><p><strong>Evidently AI / WhyLabs</strong></p></td>
<td class="text-left"><p>Specialized open-source/SaaS tools for ML monitoring. Will be used to compare production data distributions against a baseline (training data) to detect data drift and concept drift, triggering alerts or retraining pipelines.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Metadata Store</strong></p></td>
<td class="text-left"><p>ML Metadata</p></td>
<td class="text-left"><p><strong>MLflow Tracking Server Backend</strong></p></td>
<td class="text-left"><p>Acts as the central “logbook.” MLflow automatically captures and stores metadata from experiments, data versions, and model artifacts, creating a comprehensive, searchable lineage for the entire ML system.</p></td>
</tr>
</tbody>
</table>
</div>
<p>This strategically chosen stack provides a powerful and balanced MLOps foundation. It leverages managed services to reduce operational overhead where appropriate (ingestion, serving) while using flexible, open-source tools for core ML lifecycle management (experimentation, model registry) to maintain control and avoid vendor lock-in.</p>
</section>
<section id="core-mlops-pipelines-and-workflows">
<h4>3.4 Core MLOps Pipelines and Workflows<a class="headerlink" href="#core-mlops-pipelines-and-workflows" title="Permalink to this heading">¶</a></h4>
<p>To operationalize the MLOps strategy, the entire system is decomposed into a series of interconnected, automated pipelines. These pipelines represent the key workflows that govern the lifecycle of our data and models, transforming the ML process from a manual, research-oriented task into a reliable, repeatable engineering discipline. The following table outlines the primary pipelines to be developed, each designed as a distinct, automated workflow.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Pipeline / Workflow</p></th>
<th class="head text-left"><p>Trigger</p></th>
<th class="head text-left"><p>Inputs</p></th>
<th class="head text-left"><p>Key Steps</p></th>
<th class="head text-left"><p>Outputs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Real-Time Clickstream Ingestion</strong></p></td>
<td class="text-left"><p>User action on client (web/mobile app)</p></td>
<td class="text-left"><p>JSON event payload from client tracker.</p></td>
<td class="text-left"><p>1. <strong>Collection:</strong> API Gateway receives the event.<br>2. <strong>Buffering:</strong> Event is published to an AWS Kinesis Data Stream.<br>3. <strong>Initial Processing:</strong> An AWS Lambda function consumes the event, performs initial validation, and enriches it with server-side info (e.g., geo-location from IP).</p></td>
<td class="text-left"><p>Enriched event message pushed to a “processed” Kinesis stream and simultaneously archived to an S3 “raw” data lake bucket.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Feature Engineering Pipeline</strong></p></td>
<td class="text-left"><p><strong>Streaming:</strong> New message on the “processed” Kinesis stream.<br><br><strong>Batch:</strong> Daily/Hourly schedule via Airflow.</p></td>
<td class="text-left"><p><strong>Streaming:</strong> Enriched clickstream events.<br><br><strong>Batch:</strong> Historical clickstream data from the S3 Data Lake.</p></td>
<td class="text-left"><p><strong>Streaming (Real-time):</strong><br>1. Consume enriched events.<br>2. Perform stateless transformations.<br>3. Update/compute session-level features (e.g., <code class="docutils literal notranslate"><span class="pre">pages_viewed_in_session</span></code>).<br>4. Write latest feature values to the Feast Online Store (DynamoDB/Redis).<br><br><strong>Batch (Offline):</strong><br>1. Run a scheduled Spark job on historical data.<br>2. Compute complex, long-term aggregations (e.g., <code class="docutils literal notranslate"><span class="pre">90_day_purchase_count</span></code>, <code class="docutils literal notranslate"><span class="pre">customer_lifetime_value</span></code>).<br>3. Backfill the Feast Offline Store (S3 Parquet) with historical values.</p></td>
<td class="text-left"><p><strong>Online Store:</strong> Up-to-the-millisecond feature values for active users.<br><br><strong>Offline Store:</strong> A complete, point-in-time correct history of all feature values.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>ML Training Pipeline (CI/CT)</strong></p></td>
<td class="text-left"><p>1. <strong>Scheduled:</strong> Weekly/bi-weekly via Airflow.<br>2. <strong>On-demand:</strong> Manual trigger by an ML Engineer.<br>3. <strong>Automated:</strong> Alert from the Monitoring system (e.g., significant data drift detected).</p></td>
<td class="text-left"><p>1. <code class="docutils literal notranslate"><span class="pre">model_training_config.yaml</span></code> (hyperparameters, etc.).<br>2. <code class="docutils literal notranslate"><span class="pre">feature_list.py</span></code> (defines features to use).<br>3. Historical feature data from the Feast Offline Store.</p></td>
<td class="text-left"><p>1. <strong>Data Fetching:</strong> Generate a point-in-time correct training dataset using Feast.<br>2. <strong>Data Validation:</strong> Use Great Expectations to validate the training data against predefined quality checks.<br>3. <strong>Model Training:</strong> Execute the training script on AWS SageMaker.<br>4. <strong>Model Evaluation:</strong> Evaluate model performance (AUC, Precision) on a held-out test set.<br>5. <strong>Model Registration:</strong> If performance exceeds the production model’s baseline, version and register the new model artifact and its metrics to the MLflow Model Registry.</p></td>
<td class="text-left"><p>A versioned, validated, and registered model artifact in the MLflow Model Registry, ready for deployment.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Deployment Pipeline (CD)</strong></p></td>
<td class="text-left"><p>A model is promoted to the “Production” stage in the MLflow Model Registry.</p></td>
<td class="text-left"><p>1. A registered model artifact from MLflow.<br>2. <code class="docutils literal notranslate"><span class="pre">deployment_config.yaml</span></code> (instance types, scaling policies).</p></td>
<td class="text-left"><p>1. <strong>Trigger:</strong> A webhook from MLflow triggers a GitHub Actions workflow.<br>2. <strong>Build:</strong> Build a new Docker container with the model artifact.<br>3. <strong>Push:</strong> Push the container to Amazon ECR.<br>4. <strong>Deploy:</strong> Initiate a canary release on the Amazon SageMaker Endpoint, shifting a small percentage (e.g., 5%) of traffic to the new model version.</p></td>
<td class="text-left"><p>A new version of the model is safely deployed and serving a portion of live production traffic.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Real-Time Inference</strong> (Logical Flow)</p></td>
<td class="text-left"><p>API call from the e-commerce backend application.</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">user_id</span></code> and any real-time context (e.g., <code class="docutils literal notranslate"><span class="pre">product_id</span></code> being viewed).</p></td>
<td class="text-left"><p>1. <strong>Feature Retrieval:</strong> Fetch the latest feature vector for the <code class="docutils literal notranslate"><span class="pre">user_id</span></code> from the Feast Online Store.<br>2. <strong>Prediction:</strong> Pass the feature vector to the deployed model on the SageMaker Endpoint.<br>3. <strong>Response:</strong> Return the propensity score (e.g., <code class="docutils literal notranslate"><span class="pre">{&quot;propensity&quot;:</span> <span class="pre">0.85}</span></code>) to the backend application.</p></td>
<td class="text-left"><p>A real-time propensity score, enabling a personalized action within the user’s session.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Monitoring &amp; Retraining Loop</strong></p></td>
<td class="text-left"><p>Continuous (for monitoring)<br><br>Alert-based (for retraining)</p></td>
<td class="text-left"><p>Live prediction requests and model responses.</p></td>
<td class="text-left"><p>1. <strong>Log:</strong> Log all predictions and features to a dedicated monitoring stream.<br>2. <strong>Monitor:</strong> Continuously compare the distribution of live features and predictions against the training set baseline to detect drift.<br>3. <strong>Alert:</strong> If drift exceeds a predefined threshold, send an alert (e.g., to Slack, PagerDuty).<br>4. <strong>Trigger:</strong> The alert automatically triggers the <strong>ML Training Pipeline</strong> to start a new retraining run with the latest data.</p></td>
<td class="text-left"><p>Actionable alerts for on-call engineers and an automated, closed-loop system that can adapt to changing data without manual intervention.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="project-stages-and-timeline">
<h4>3.5.1 Project Stages and Timeline<a class="headerlink" href="#project-stages-and-timeline" title="Permalink to this heading">¶</a></h4>
<p>A project of this complexity requires a structured, iterative approach to manage risk and deliver value incrementally. The project will be executed in distinct phases, moving from initial planning and experimentation to a full production rollout. This phased approach allows the team to learn and adapt, ensuring the final system is robust, scalable, and aligned with business needs.</p>
<p>The project is broken down into five major stages, each with specific goals and deliverables.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Duration (Est.)</p></th>
<th class="head text-left"><p>Key Activities &amp; Deliverables</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Stage 1: Ideation &amp; Planning</strong></p></td>
<td class="text-left"><p><strong>Weeks 1-2</strong></p></td>
<td class="text-left"><p>- <strong>Define Business Case:</strong> Solidify KPIs (Conversion Rate, AOV), success criteria, and projected ROI.<br>- <strong>Problem Framing:</strong> Finalize the ML problem as propensity scoring; define model inputs/outputs.<br>- <strong>Feasibility Study:</strong> Complete data discovery, risk assessment, and initial cost estimation.<br>- <strong>Tech Stack Selection:</strong> Finalize the MLOps stack (as defined in Sec 3.3).<br>- <strong>Project Plan:</strong> Create a detailed project plan and roadmap.<br> <strong>Deliverable:</strong> A signed-off Project Charter document.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Stage 2: Offline Model Experimentation</strong></p></td>
<td class="text-left"><p><strong>Weeks 3-6</strong></p></td>
<td class="text-left"><p>- <strong>Data Preparation:</strong> Ingest and clean a historical dataset for initial modeling.<br>- <strong>Feature Engineering (Offline):</strong> Develop and test an initial set of features using Python/Spark.<br>- <strong>Baseline Model:</strong> Train a simple baseline model (e.g., Logistic Regression) to establish a performance floor.<br>- <strong>Advanced Model Training:</strong> Experiment with more complex models (LightGBM, XGBoost) and log all experiments in MLflow.<br>- <strong>Model Selection:</strong> Select the best-performing “champion” model based on offline evaluation metrics (AUC, Precision).<br> <strong>Deliverable:</strong> A trained and registered v1 model artifact with a comprehensive evaluation report.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Stage 3: MLOps Pipeline Development</strong></p></td>
<td class="text-left"><p><strong>Weeks 7-14</strong></p></td>
<td class="text-left"><p>- <strong>Infrastructure as Code (IaC):</strong> Develop Terraform scripts for all AWS resources.<br>- <strong>Pipeline Implementation:</strong> Build, test, and document the core automated pipelines:<br>  - Real-time Ingestion Pipeline (Kinesis/Lambda).<br>  - Batch &amp; Streaming Feature Engineering Pipelines (Feast/Spark).<br>  - ML Training Pipeline (Airflow/SageMaker).<br>  - CI/CD workflows for all components (GitHub Actions).<br> <strong>Deliverable:</strong> Fully automated, tested, and production-ready MLOps pipelines.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Stage 4: Deployment &amp; Initial Serving</strong></p></td>
<td class="text-left"><p><strong>Weeks 15-16</strong></p></td>
<td class="text-left"><p>- <strong>Staging Deployment:</strong> Deploy the entire system to a pre-production (staging) environment for end-to-end testing.<br>- <strong>Shadow Deployment:</strong> Deploy the champion model into production in “shadow mode.” It will receive live traffic and make predictions, but its outputs will only be logged, not acted upon. This validates the live feature pipeline and model performance on real data without user impact.<br>- <strong>Canary Release:</strong> Begin a canary release, routing 1% of live traffic to the model for active personalization. Closely monitor system and business metrics.<br> <strong>Deliverable:</strong> The model is live in production, serving a small fraction of users.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Stage 5: Monitoring &amp; Iteration</strong></p></td>
<td class="text-left"><p><strong>Ongoing</strong></p></td>
<td class="text-left"><p>- <strong>Ramp-Up:</strong> Gradually increase traffic to the new model (e.g., 10% -&gt; 50% -&gt; 100%) based on monitoring results.<br>- <strong>A/B Testing:</strong> Conduct formal A/B tests to rigorously measure the business impact (lift in Conversion Rate, AOV) against the control group.<br>- <strong>Monitoring &amp; Alerting:</strong> Continuously monitor for operational issues, data drift, and concept drift.<br>- <strong>Continual Learning:</strong> Execute scheduled and trigger-based retraining pipelines to keep the model fresh.<br>- <strong>Iterate:</strong> Use insights from monitoring and A/B tests to inform the development of the next generation of models (v2).<br> <strong>Deliverable:</strong> A stable, monitored production system and a data-driven roadmap for future improvements.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="cross-functional-team-roles">
<h4>3.5.2 Cross-Functional Team &amp; Roles<a class="headerlink" href="#cross-functional-team-roles" title="Permalink to this heading">¶</a></h4>
<p>Success requires a tight-knit, cross-functional team where responsibilities are clear but collaboration is constant. For a project of this scale, the core team would consist of:</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Role</p></th>
<th class="head text-left"><p>Primary Responsibilities</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Product Manager</strong></p></td>
<td class="text-left"><p>Owns the business vision and roadmap. Defines requirements, prioritizes features, and acts as the bridge between technical teams and business stakeholders. Responsible for measuring the final business impact (ROI).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data Engineer</strong></p></td>
<td class="text-left"><p>Designs, builds, and maintains the real-time data ingestion and processing pipelines. Ensures data quality, reliability, and scalability. Owns the “Bronze” and “Silver” layers of the data lake.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>ML Engineer</strong></p></td>
<td class="text-left"><p><strong>Leads the MLOps strategy.</strong> Designs and builds the core ML pipelines: feature engineering (in collaboration with Data Engineering), automated training, and model deployment. Owns the production model’s operational health, including monitoring, alerting, and retraining strategies.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data Scientist</strong></p></td>
<td class="text-left"><p><strong>Leads the modeling strategy.</strong> Explores data, develops features, and experiments with different ML algorithms. Responsible for model evaluation, interpretation, and selecting the “champion” model for deployment.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Platform/DevOps Engineer</strong></p></td>
<td class="text-left"><p>Manages the underlying cloud infrastructure (AWS), Kubernetes, and CI/CD tooling (GitHub Actions). Ensures the platform is secure, scalable, and reliable.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="versioning-and-governance-strategy">
<h4>3.5.3 Versioning and Governance Strategy<a class="headerlink" href="#versioning-and-governance-strategy" title="Permalink to this heading">¶</a></h4>
<p>To ensure reproducibility, auditability, and controlled changes, a strict versioning and governance strategy will be applied to all assets.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Asset</p></th>
<th class="head text-left"><p>Versioning Tool</p></th>
<th class="head text-left"><p>Governance Strategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Code</strong></p></td>
<td class="text-left"><p><strong>Git (GitHub)</strong></p></td>
<td class="text-left"><p>All code (pipelines, model logic, IaC) is managed in Git. All changes are made via Pull Requests (PRs), which require at least one peer review and must pass all automated CI checks (linting, unit tests) before being merged into the main branch.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data</strong></p></td>
<td class="text-left"><p><strong>DVC</strong></p></td>
<td class="text-left"><p>The specific version of the dataset used to train a production model is “snapshotted” using DVC. The DVC metadata file is checked into Git, allowing for perfect recreation of the training data for any model version.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Models</strong></p></td>
<td class="text-left"><p><strong>MLflow Model Registry</strong></p></td>
<td class="text-left"><p>Every trained model candidate is registered in MLflow. Models are promoted through distinct stages: <strong>Staging</strong> -&gt; <strong>Production</strong>. Only users with specific permissions (e.g., Lead ML Engineer) can approve the promotion of a model to the Production stage, which serves as the trigger for the deployment pipeline.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Features</strong></p></td>
<td class="text-left"><p><strong>Feast Registry &amp; Git</strong></p></td>
<td class="text-left"><p>Feature definitions are declarative Python code and are versioned in the same Git repository as the pipeline code. The Feast registry stores the state of the deployed features. Changes to feature definitions follow the same PR and review process as any other code change.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Infrastructure</strong></p></td>
<td class="text-left"><p><strong>Terraform &amp; Git</strong></p></td>
<td class="text-left"><p>All cloud infrastructure is defined as code using Terraform and versioned in Git. Changes to production infrastructure also require a reviewed PR, ensuring a controlled and auditable process.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="a-comprehensive-ml-testing-strategy-the-mlops-crucible">
<h4>3.6 A Comprehensive ML Testing Strategy: The MLOps Crucible<a class="headerlink" href="#a-comprehensive-ml-testing-strategy-the-mlops-crucible" title="Permalink to this heading">¶</a></h4>
<p>In MLOps, testing is not a single stage but a continuous, multi-layered process that ensures quality and reliability from the first line of code to the model serving live traffic. A failure in data, code, or the model itself can lead to silent, costly degradation of the user experience. Therefore, we adopt a holistic testing strategy that scrutinizes every component of the system.</p>
<p>This strategy is modeled after the <strong>MLOps Test Pyramid</strong> (based on <a class="reference external" href="https://martinfowler.com/articles/practical-test-pyramid.html">Martin Fowler’s The Practical Test Pyramid</a>), which advocates for a tiered approach. We push tests as low down the pyramid as possible for fast feedback, with a broad base of fast, isolated unit tests and progressively fewer, more integrated tests as we move up the stack.</p>
<p>The following table details the specific tests that will be implemented across the four primary layers of our system: <strong>Data &amp; Features</strong>, <strong>Code &amp; Pipelines</strong>, <strong>Model</strong>, and <strong>Infrastructure &amp; Serving</strong>.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Testing Layer</p></th>
<th class="head text-left"><p>Test Type / Purpose</p></th>
<th class="head text-left"><p>Implementation Strategy &amp; Tooling</p></th>
<th class="head text-left"><p>Lifecycle Stage (Where &amp; When)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Data &amp; Features</strong></p></td>
<td class="text-left"><p><strong>Schema &amp; Quality Validation</strong><br>To ensure all incoming data conforms to expected formats and quality standards before it corrupts downstream processes.</p></td>
<td class="text-left"><p><strong>Great Expectations</strong> suites will be defined and versioned in Git. Checks include:<br>- <code class="docutils literal notranslate"><span class="pre">user_id</span></code>, <code class="docutils literal notranslate"><span class="pre">event_timestamp</span></code> must not be null.<br>- <code class="docutils literal notranslate"><span class="pre">event_type</span></code> must be in the set <code class="docutils literal notranslate"><span class="pre">['view',</span> <span class="pre">'add_to_cart',</span> <span class="pre">'purchase']</span></code>.<br>- <code class="docutils literal notranslate"><span class="pre">price</span></code> and <code class="docutils literal notranslate"><span class="pre">quantity</span></code> columns must be positive numbers.<br>These suites will be integrated directly into our Airflow data pipelines.</p></td>
<td class="text-left"><p><strong>CI/CD &amp; Operational:</strong> Executed as a distinct task within the Airflow data ingestion and feature engineering pipelines. A failure on a critical expectation will halt the pipeline and trigger an alert.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Feature Distribution Validation</strong><br>To detect data and concept drift by monitoring shifts in the statistical properties of our features.</p></td>
<td class="text-left"><p><strong>Evidently AI</strong> will be used to:<br>1. <strong>Offline:</strong> Generate a reference profile from the training dataset.<br>2. <strong>Online:</strong> Periodically run a job that profiles live inference data from the monitoring S3 bucket and compares it to the reference profile. Statistical tests (e.g., Kolmogorov-Smirnov) will detect significant drift.</p></td>
<td class="text-left"><p><strong>Offline (CI/CD):</strong> A reference profile is generated and stored as an artifact during the training pipeline.<br><br><strong>Production (Monitoring):</strong> A scheduled Airflow DAG runs daily to generate a new profile and perform the drift comparison, sending alerts on significant drift.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Code &amp; Pipelines</strong></p></td>
<td class="text-left"><p><strong>Unit Tests</strong><br>To verify the correctness of individual functions and logical components in isolation.</p></td>
<td class="text-left"><p><strong>Pytest</strong> will be used to test all Python code, including:<br>- Feature transformation logic (e.g., testing that a <code class="docutils literal notranslate"><span class="pre">sessionize</span></code> function correctly groups events).<br>- Utility functions and data processing classes.<br>- API logic in the model serving application (using FastAPI’s <code class="docutils literal notranslate"><span class="pre">TestClient</span></code>).<br>All external dependencies (e.g., database connections) will be mocked.</p></td>
<td class="text-left"><p><strong>Dev &amp; CI:</strong> Executed locally by developers and automatically on every <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">push</span></code> in the GitHub Actions CI pipeline. A failing test will block the Pull Request from being merged.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Pipeline Integration Tests</strong><br>To ensure that different components of a pipeline (e.g., Airflow tasks) work together correctly.</p></td>
<td class="text-left"><p><strong>Pytest</strong> scripts will orchestrate tests against a staging environment:<br>- <strong>Feature Pipeline:</strong> A test will trigger the feature engineering DAG with a small, fixed input file on S3 and assert that the expected features are written correctly to a staging Feast feature store.<br>- <strong>Training Pipeline:</strong> A test will run the entire training DAG on a small dataset to ensure it can successfully produce and register a model in the staging MLflow registry.</p></td>
<td class="text-left"><p><strong>Staging (CD):</strong> Executed automatically by GitHub Actions after a successful deployment of pipeline code to the staging environment.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Model</strong></p></td>
<td class="text-left"><p><strong>Behavioral &amp; Robustness Tests</strong><br>To validate that the model has learned correct, logical behaviors beyond just its aggregate accuracy.</p></td>
<td class="text-left"><p>Custom <strong>Pytest</strong> checks will be created to validate model logic:<br>- <strong>Invariance Test:</strong> A user’s propensity score should not change if only their <code class="docutils literal notranslate"><span class="pre">user_id</span></code> is changed while all behavioral features remain identical.<br>- <strong>Directional Expectation Test:</strong> Adding a high-intent event like <code class="docutils literal notranslate"><span class="pre">add_to_cart</span></code> to a session’s event stream should <em>increase</em> the resulting propensity score.<br>- <strong>Robustness Test:</strong> Introduce minor noise (e.g., slightly alter a product price) and assert that the prediction does not change dramatically.</p></td>
<td class="text-left"><p><strong>CI/CD:</strong> Executed as a mandatory step in the model training pipeline after the model is trained but before it is registered. A failure here prevents the model from being considered for deployment.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Sliced Evaluation &amp; Fairness</strong><br>To ensure the model performs reliably and fairly across critical business segments, preventing hidden biases.</p></td>
<td class="text-left"><p>The model evaluation script in the training pipeline will calculate and log key metrics (AUC, Precision) for predefined user segments, including:<br>- Anonymous vs. Logged-in Users<br>- New vs. Returning Customers<br>- Mobile vs. Desktop Users<br>The results will be stored as artifacts in MLflow.</p></td>
<td class="text-left"><p><strong>CI/CD:</strong> Executed as part of the model training pipeline. A significant performance drop (e.g., &gt;10%) on any critical slice relative to the overall performance will flag the model for manual review.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Infrastructure &amp; Serving</strong></p></td>
<td class="text-left"><p><strong>API Contract &amp; Smoke Test</strong><br>To confirm that a newly deployed model endpoint is live, responsive, and adheres to its API schema.</p></td>
<td class="text-left"><p>A <strong>Pytest</strong> script will be run post-deployment to:<br>1. Ping the <code class="docutils literal notranslate"><span class="pre">/health</span></code> endpoint and expect a <code class="docutils literal notranslate"><span class="pre">200</span> <span class="pre">OK</span></code> response.<br>2. Send a valid sample request and assert that the response is a <code class="docutils literal notranslate"><span class="pre">200</span> <span class="pre">OK</span></code> and that the JSON body contains a <code class="docutils literal notranslate"><span class="pre">propensity</span></code> key with a float value between 0.0 and 1.0.</p></td>
<td class="text-left"><p><strong>Staging &amp; Production (CD):</strong> Executed automatically by the GitHub Actions CD workflow immediately after an endpoint deployment. A failure will trigger an automatic rollback to the previous stable version.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Performance &amp; Load Test</strong><br>To ensure the serving infrastructure meets the stringent latency and throughput requirements under realistic load.</p></td>
<td class="text-left"><p><strong>Locust</strong>, an open-source load testing tool, will be used.<br>- A <code class="docutils literal notranslate"><span class="pre">locustfile.py</span></code> will define user behavior (sending prediction requests).<br>- The test will simulate peak traffic against the staging endpoint to validate that <strong>p99 latency remains &lt; 100ms</strong> and to identify the maximum sustainable QPS (Queries Per Second).</p></td>
<td class="text-left"><p><strong>Staging (CD):</strong> Executed as a final automated step in the CD pipeline before a model can be promoted to production. Performance results are published to the PR.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="data-sourcing-discovery-and-characteristics">
<h3>4. Data Sourcing, Discovery, and Characteristics<a class="headerlink" href="#data-sourcing-discovery-and-characteristics" title="Permalink to this heading">¶</a></h3>
<p>Every robust machine learning system is built upon a foundation of high-quality, relevant data. This phase is where we identify, acquire, and understand the raw ingredients that will fuel our propensity models. The process is systematic, moving from high-level requirements to a granular understanding of the data’s profile and potential challenges.</p>
<section id="data-sourcing-discovery-plan">
<h4>4.1 Data Sourcing &amp; Discovery Plan<a class="headerlink" href="#data-sourcing-discovery-plan" title="Permalink to this heading">¶</a></h4>
<p>The following plan maps the standard framework for data sourcing to the specific needs of the e-commerce propensity scoring project.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Framework Step</p></th>
<th class="head text-left"><p>Application to the Propensity Scoring Project</p></th>
<th class="head text-left"><p>Key Rationale &amp; Chosen Tools</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Identifying Data Requirements</strong></p></td>
<td class="text-left"><p>To predict purchase intent, we require three core data domains:<br>1. <strong>Behavioral Data:</strong> Real-time user interactions (views, clicks, searches, add-to-carts).<br>2. <strong>Transactional Data:</strong> Historical purchases, returns, and order values.<br>3. <strong>Customer Data:</strong> User demographics and segment information (for known users).<br>4. <strong>Product Catalog Data:</strong> Product attributes, categories, and pricing.</p></td>
<td class="text-left"><p>These domains provide the necessary signals for short-term intent (session behavior) and long-term context (user history). This rich combination is essential for building an accurate, personalized model.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Exploring Data Sources</strong></p></td>
<td class="text-left"><p>All required data is first-party, sourced from internal systems:<br>- <strong>Clickstream Events:</strong> A custom tracking system or a platform like Snowplow sending JSON events from the website and mobile apps.<br>- <strong>Transactional Database:</strong> A production OLTP database (e.g., PostgreSQL, MySQL) holding order and payment information.<br>- <strong>CRM System:</strong> The central repository for customer profile data.<br>- <strong>Product Information Management (PIM):</strong> The system holding the product catalog.</p></td>
<td class="text-left"><p>Relying on first-party data simplifies governance and ensures compliance with privacy regulations like GDPR. These sources represent the ground truth for business operations.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Data Collection &amp; Ingestion Strategy</strong></p></td>
<td class="text-left"><p>A hybrid ingestion strategy is required:<br>- <strong>Streaming (for Behavioral Data):</strong> High-velocity clickstream events will be ingested in real-time to power session-based features.<br>- <strong>Batch (for other data):</strong> Transactional, Customer, and Product data, which change less frequently, will be ingested via daily batch jobs.</p></td>
<td class="text-left"><p>This dual approach optimizes for both freshness and cost. Real-time ingestion is critical for session-based personalization, while batch processing is more efficient for slower-moving, structured data sources. <br><strong>Tools:</strong> <strong>AWS Kinesis</strong> for streaming; <strong>Apache Airflow</strong> for batch orchestration.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Initial Storage &amp; Versioning</strong></p></td>
<td class="text-left"><p>Data will be landed in an <strong>Amazon S3 Data Lake</strong> structured by layers (Bronze, Silver, Gold).<br>- <strong>Bronze:</strong> Raw, immutable event data.<br>- <strong>Silver:</strong> Cleaned, sessionized, and structured data (Parquet format).<br>- <strong>DVC (Data Version Control)</strong> will be used to snapshot and version key datasets (e.g., the specific data cut used for a production model training run), with the pointers stored in Git.</p></td>
<td class="text-left"><p>The layered data lake provides a clear, auditable data flow. Parquet is optimal for analytical performance. DVC ensures that every model training run is reproducible by linking the model version to the exact data version it was trained on.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Exploratory Data Analysis (EDA)</strong></p></td>
<td class="text-left"><p>A dedicated phase of analysis will be conducted to:<br>- Profile feature distributions (e.g., session length, purchase frequency).<br>- Identify data quality issues (missing values, outliers).<br>- Validate initial hypotheses (e.g., “longer sessions correlate with higher purchase intent”).<br>- Understand differences between anonymous and identified user behavior.</p></td>
<td class="text-left"><p>EDA is critical for informing feature engineering and model selection. It de-risks the modeling phase by uncovering potential issues and biases in the data early. <br><strong>Tools:</strong> <strong>Jupyter Notebooks</strong>, <strong>Pandas</strong>, <strong>Matplotlib</strong>, <strong>Seaborn</strong>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data Documentation &amp; Discovery</strong></p></td>
<td class="text-left"><p>Initial documentation will be lean but effective:<br>- A version-controlled <strong><code class="docutils literal notranslate"><span class="pre">data_sources.md</span></code></strong> file will document the schema, origin, and ownership of each key dataset.<br>- <strong>AWS Glue Data Catalog</strong> will be used to make Silver and Gold layer tables in S3 discoverable and queryable via Athena.<br>- EDA findings will be documented in shareable notebooks.</p></td>
<td class="text-left"><p>For the project’s scale, a full-featured data discovery platform (like Amundsen) is overkill. A pragmatic combination of code-based documentation and a managed data catalog provides the necessary discovery and governance without excessive overhead.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Early Governance &amp; Security</strong></p></td>
<td class="text-left"><p>- <strong>Access Control:</strong> Strict AWS IAM roles will be defined for each pipeline and user type, adhering to the principle of least privilege.<br>- <strong>Privacy:</strong> A PII scan will be part of the ingestion pipeline, with automated masking/hashing of sensitive customer data before it lands in the analytical layers. All processes must be GDPR compliant.</p></td>
<td class="text-left"><p>Security and privacy are not afterthoughts. Integrating governance from day one is essential for building a trustworthy and compliant production system, especially when operating in Europe.</p></td>
</tr>
</tbody>
</table>
</div>
<!--
#### 4.2 Data Characteristics

The following table provides an estimated profile of the key data domains for this project, assuming a mid-sized European e-commerce retailer.

| Data Type | Daily Volume (Average) | Data Velocity | Data Profile & Governance Notes |
| :--- | :--- | :--- | :--- |
| **Behavioral (Clickstream) Data** | **4 - 8 GB per day.** <br>This is the highest volume data, generated by ~250k daily user sessions, spiking to 1M+ sessions on peak days. | **High-Velocity Event Streams.** <br>Data is generated continuously. Must be ingested in near real-time via **AWS Kinesis** to support live session feature engineering. | **Semi-Structured (JSON).** <br>Events include `page_view`, `add_to_cart`, `search_query`, `product_view`. Contains anonymous `session_id` and `customer_id` (if logged in). <br>**Governance:** Requires explicit user consent for tracking under GDPR. Raw data retention policies must be defined. |
| **Transactional Data** | **50 - 200 MB per day.** <br>Generated by 4k-6k daily orders, with returns and payment status updates. | **Near Real-Time Events, Processed in Batch.** <br>Individual transactions occur in real-time, but are ingested into the data lake via hourly/daily batch jobs orchestrated by **Airflow**. | **Highly Structured.** <br>Consists of tables with well-defined schemas. <br>**Governance:** Subject to financial auditing. Must handle multiple European currencies (€, £, CHF). GDPR compliance is critical. |
| **Customer / CRM Data** | **< 100 MB per day.** <br>Primarily updates to existing profiles and new sign-ups. | **Low / On-Change.** <br>Synchronized via daily batch jobs from the CRM system, orchestrated by **Airflow**. | **Highly Structured & Sensitive.** <br>Contains Personally Identifiable Information (PII). <br>**Governance:** Strict GDPR rules apply. PII must be masked or hashed in analytical environments. Must support the "right to be forgotten." |
| **Product Catalog Data**| **< 50 MB per day.** <br>Primarily updates to product attributes (stock levels, pricing) and a smaller number of new product additions. | **Low / On-Change.** <br>Synchronized via daily batch jobs from the PIM system, orchestrated by **Airflow**. | **Highly Structured.** <br>Contains product `sku`, `category`, `description`, `price`, and `stock_level`. <br>**Governance:** The single source of truth for product information. Data consistency is critical. |
-->
</section>
<section id="key-technical-considerations-for-implementation">
<h4>4.3 Key Technical Considerations for Implementation<a class="headerlink" href="#key-technical-considerations-for-implementation" title="Permalink to this heading">¶</a></h4>
<p>The data discovery and profiling phase has surfaced several critical technical challenges that must be addressed in the implementation of our data and feature pipelines.</p>
<ol class="arabic simple">
<li><p><strong>Solving for Long-Lived Session State:</strong> User shopping sessions can span days. Computing session-based features in real-time requires a stateful stream processing solution that can manage state for millions of concurrent sessions without excessive memory consumption. This will be a key design challenge in the Feature Engineering section.</p></li>
<li><p><strong>Point-in-Time Correctness for Training Data:</strong> To avoid data leakage, training datasets must be assembled with point-in-time accuracy (i.e., joining features as they were at the exact moment of each historical event). Our use of <strong>Feast</strong> is intended to solve this directly, but the implementation of the data loading into the Feast offline store must be done carefully.</p></li>
<li><p><strong>Online/Offline Feature Store Synchronization:</strong> The system must guarantee that the features used for real-time inference in the online store are consistent with the features used for batch training in the offline store. The design of our feature ingestion pipelines will be critical to maintaining this consistency and preventing training-serving skew.</p></li>
<li><p><strong>Handling Extreme Class Imbalance:</strong> The “purchase” event is rare (our conversion rate is ~2%). This extreme imbalance must be addressed during the model training phase using techniques like over/under-sampling (e.g., SMOTE) or by using appropriate class weights to prevent the model from simply predicting “no purchase” for every session.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="data-engineering-and-pipelines">
<h3>5. Data Engineering and Pipelines<a class="headerlink" href="#data-engineering-and-pipelines" title="Permalink to this heading">¶</a></h3>
<section id="the-data-engineering-lifecycle-from-raw-data-to-ml-ready-features">
<h4>5.1 The Data Engineering Lifecycle: From Raw Data to ML-Ready Features<a class="headerlink" href="#the-data-engineering-lifecycle-from-raw-data-to-ml-ready-features" title="Permalink to this heading">¶</a></h4>
<p>The following plan maps the standard data engineering lifecycle to our e-commerce propensity project, outlining the specific steps and tools for our batch-oriented pipelines, which are primarily responsible for creating our historical training data and populating our feature store’s offline tables.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Lifecycle Stage</p></th>
<th class="head text-left"><p>Application to the Propensity Scoring Project</p></th>
<th class="head text-left"><p>Key Rationale &amp; Chosen Tools</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Generation &amp; Sourcing</strong></p></td>
<td class="text-left"><p>- <strong>Clickstream Events:</strong> Ingested from internal trackers.<br>- <strong>Transactional &amp; CRM Data:</strong> Pulled from production databases (PostgreSQL).<br>- <strong>Product Catalog:</strong> Pulled from PIM system.</p></td>
<td class="text-left"><p>The data required is entirely first-party. Our pipelines must interact with both event streams and relational databases.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Storage</strong></p></td>
<td class="text-left"><p>- <strong>Raw (Bronze Layer):</strong> All source data landed as-is in <strong>Amazon S3</strong>.<br>- <strong>Cleaned (Silver Layer):</strong> Data cleaned, sessionized, and stored in <strong>S3 as Parquet</strong>.<br>- <strong>Features (Gold Layer):</strong> Final, aggregated features stored in the <strong>Feast Offline Store (S3 Parquet)</strong>.</p></td>
<td class="text-left"><p>S3 provides a cost-effective, scalable, and durable foundation for our data lake. Parquet is chosen for its analytical efficiency. Feast manages the Gold layer for ML consumption.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Ingestion &amp; Transformation (ETL)</strong></p></td>
<td class="text-left"><p>The core batch pipeline, orchestrated by <strong>Apache Airflow</strong>, will perform the following steps on a daily schedule:</p></td>
<td class="text-left"><p>Airflow is the chosen orchestrator for its robustness and extensive ecosystem of operators. The pipeline is designed to be modular and idempotent.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>1. Data Cleaning &amp; Wrangling:</strong></p></td>
<td class="text-left"><p>- Handle missing values (e.g., impute missing <code class="docutils literal notranslate"><span class="pre">product_category</span></code> with ‘Unknown’).<br>- Correct data types (e.g., ensure <code class="docutils literal notranslate"><span class="pre">event_timestamp</span></code> is a valid timestamp).<br>- Standardize text (lowercase search queries, remove special characters).<br>- Filter out bot traffic based on heuristics (e.g., sessions with abnormally high event counts).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>2. Data Transformation:</strong></p></td>
<td class="text-left"><p>- <strong>Sessionization:</strong> Group individual clickstream events into user sessions based on <code class="docutils literal notranslate"><span class="pre">user_id</span></code>/<code class="docutils literal notranslate"><span class="pre">session_id</span></code> and a 30-minute inactivity window. This is a critical transformation for creating session-based features.<br>- <strong>Feature Engineering:</strong> Calculate historical aggregates (e.g., <code class="docutils literal notranslate"><span class="pre">user_purchase_count_90d</span></code>, <code class="docutils literal notranslate"><span class="pre">avg_order_value</span></code>). This will be a complex Spark job for performance at scale.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>3. Data Labeling &amp; Splitting:</strong></p></td>
<td class="text-left"><p>- <strong>Labeling:</strong> For each session, determine the target label: <code class="docutils literal notranslate"><span class="pre">1</span></code> if a <code class="docutils literal notranslate"><span class="pre">purchase</span></code> event exists, <code class="docutils literal notranslate"><span class="pre">0</span></code> otherwise.<br>- <strong>Splitting (for Training Pipeline):</strong> Implement a time-based split on the historical data (e.g., train on weeks 1-3, validate on week 4). For reproducibility, also implement a group-based split to ensure all events from a single user remain in the same set.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>4. Data Validation:</strong></p></td>
<td class="text-left"><p>- <strong>Great Expectations</strong> will be integrated as a dedicated task in the Airflow DAG. An “Expectation Suite” will run against the final, ML-ready dataset to:<br>  - <code class="docutils literal notranslate"><span class="pre">expect_column_to_exist:</span> <span class="pre">['user_id',</span> <span class="pre">'propensity_score_target',</span> <span class="pre">...]</span></code><br>  - <code class="docutils literal notranslate"><span class="pre">expect_column_values_to_not_be_null:</span> <span class="pre">'user_id'</span></code><br>  - <code class="docutils literal notranslate"><span class="pre">expect_column_mean_to_be_between:</span> <span class="pre">'session_duration',</span> <span class="pre">min_value=0,</span> <span class="pre">max_value=3600</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>5. Data Versioning:</strong></p></td>
<td class="text-left"><p>- <strong>DVC</strong> will be used to version the final training dataset. An Airflow task will run <code class="docutils literal notranslate"><span class="pre">dvc</span> <span class="pre">add</span></code> and <code class="docutils literal notranslate"><span class="pre">dvc</span> <span class="pre">push</span></code> to create a new version of the data, linked to the Git commit of the pipeline code that produced it.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Serving (Offline)</strong></p></td>
<td class="text-left"><p>The final, versioned dataset in the Feast offline store is made available to the ML Training Pipeline.</p></td>
<td class="text-left"><p>The Data Engineering pipeline’s output is the direct input for the ML Training pipeline, creating a seamless, automated workflow.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="real-time-streaming-pipeline-design-architecture">
<h4>5.2 Real-Time Streaming Pipeline: Design &amp; Architecture<a class="headerlink" href="#real-time-streaming-pipeline-design-architecture" title="Permalink to this heading">¶</a></h4>
<p>While batch pipelines prepare our historical data, the real-time pipeline is the system’s central nervous system, responsible for processing live clickstream data to enable in-session personalization.</p>
<section id="core-architecture">
<h5>5.2.1 Core Architecture<a class="headerlink" href="#core-architecture" title="Permalink to this heading">¶</a></h5>
<p>The pipeline will follow a standard streaming pattern, leveraging managed AWS services for scalability and reliability.</p>
<img src="../_static/past_experiences/ecom_propensity/streaming_pipeline.svg" width="70%" style="background-color: #FCF1EF;"/>
</section>
<section id="key-challenges-and-solutions-for-real-time-feature-engineering">
<h5>5.2.2 Key Challenges and Solutions for Real-Time Feature Engineering<a class="headerlink" href="#key-challenges-and-solutions-for-real-time-feature-engineering" title="Permalink to this heading">¶</a></h5>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Challenge</p></th>
<th class="head text-left"><p>Our Solution &amp; Rationale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>State Management for Long-Lived Sessions</strong></p></td>
<td class="text-left"><p>User sessions can last for days, making it infeasible to hold all state in memory. <strong>Our Approach:</strong> We will leverage <strong>Spark Structured Streaming’s state management capabilities</strong>. We will use <code class="docutils literal notranslate"><span class="pre">flatMapGroupsWithState</span></code> to manage the state for each active user session. For robustness at scale, Spark’s state will be checkpointed to a reliable file system (<strong>Amazon S3</strong>). We will configure a <strong>state timeout</strong> (e.g., 48 hours of inactivity) to automatically clean up the state for abandoned sessions, preventing infinite state growth. This is a pragmatic balance, avoiding the complexity of an external state store like RocksDB for our initial implementation while still handling long-lived sessions reliably.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Ensuring Training-Serving Consistency</strong></p></td>
<td class="text-left"><p>This is the most critical MLOps challenge. <strong>Our Solution:</strong> The <strong>Feast Feature Store</strong> is the core of our strategy. The exact same feature transformation logic (written in Python/Spark) will be used by:<br>1. The <strong>batch pipeline</strong> to compute historical features and populate the Feast offline store.<br>2. The <strong>streaming pipeline</strong> to compute real-time features and populate the Feast online store.<br>By defining the feature logic once and applying it to both paths, we drastically reduce the risk of skew.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Creating Point-in-Time Correct Training Data</strong></p></td>
<td class="text-left"><p>Generating historical training data must avoid using future information. <strong>Our Solution:</strong> This is another primary benefit of <strong>Feast</strong>. When the ML Training Pipeline requests data, it will provide an “entity DataFrame” containing <code class="docutils literal notranslate"><span class="pre">user_id</span></code>, <code class="docutils literal notranslate"><span class="pre">session_id</span></code>, and <code class="docutils literal notranslate"><span class="pre">event_timestamp</span></code>. Feast’s <code class="docutils literal notranslate"><span class="pre">get_historical_features()</span></code> method will automatically perform the complex point-in-time join against the offline feature store, ensuring that for each event, only feature values that were available <em>at that exact timestamp</em> are joined. This eliminates manual, error-prone temporal join logic.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Online/Offline Store Synchronization</strong></p></td>
<td class="text-left"><p>The online and offline stores must be consistent. <strong>Our Approach:</strong> The data flow is unidirectional and designed for consistency.<br>1. The <strong>streaming pipeline</strong> is the source of truth for real-time features. It writes directly to the <strong>Online Store (Amazon ElastiCache for Redis)</strong> for low latency.<br>2. In parallel, the same streaming job writes the computed features to a table in our <strong>S3 Data Lake</strong>.<br>3. A separate, periodic batch job (part of the feature pipeline DAG) picks up these files and appends them to the main <strong>Feast Offline Store</strong> tables.<br>This ensures the online store is always freshest, while the offline store is kept reliably in sync with a slight, acceptable delay. <strong>Note:</strong> ElastiCache for Redis is chosen over DynamoDB based on benchmark data suggesting superior performance for this type of key-value lookup workload.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Handling High Ingestion Throughput &amp; Spikes</strong></p></td>
<td class="text-left"><p>The system must handle traffic spikes during sales events. <strong>Our Solution:</strong> We use managed, auto-scaling AWS services. <strong>API Gateway</strong> and <strong>Kinesis Data Streams</strong> are designed to handle massive, spiky throughput. Our <strong>Spark Structured Streaming</strong> job will be run on a cluster (e.g., EMR or Databricks) with autoscaling enabled to add or remove worker nodes based on the volume of data in the Kinesis stream.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="how-do-we-choose-the-optimal-trigger-interval-for-our-spark-structured-streaming-job">
<h4>5.3 How do we choose the optimal Trigger Interval for our Spark Structured Streaming job?<a class="headerlink" href="#how-do-we-choose-the-optimal-trigger-interval-for-our-spark-structured-streaming-job" title="Permalink to this heading">¶</a></h4>
<section id="factors-influencing-the-trigger-interval-choice">
<h5>Factors Influencing the Trigger Interval Choice<a class="headerlink" href="#factors-influencing-the-trigger-interval-choice" title="Permalink to this heading">¶</a></h5>
<ol class="arabic simple">
<li><p><strong>Business Requirements &amp; Feature Freshness (The “Why”)</strong></p>
<ul class="simple">
<li><p><strong>What it is:</strong> This is the most important factor. How up-to-date do the features need to be to provide business value?</p></li>
<li><p><strong>For Our Project:</strong> We are predicting purchase intent for <em>in-session</em> personalization. If a user adds an item to their cart, we want that action to be reflected in their feature vector relatively quickly so we can personalize their <em>next</em> click. A feature that is 5 minutes old is still useful, but one that is 30 minutes old might miss the opportunity. There is no business value in sub-second freshness for this use case.</p></li>
<li><p><strong>Consideration:</strong> A shorter interval provides fresher features but comes at a higher cost.</p></li>
</ul>
</li>
<li><p><strong>Cost &amp; Efficiency (The “How Much”)</strong></p>
<ul class="simple">
<li><p><strong>What it is:</strong> Each time the trigger fires, Spark spins up tasks, plans the execution, and writes output. Very frequent triggers lead to high operational overhead.</p></li>
<li><p><strong>CPU Churn:</strong> A very short trigger (e.g., 5 seconds) means the Spark cluster is constantly busy planning and executing very small batches of work, which is inefficient and leads to high CPU costs for the amount of data processed.</p></li>
<li><p><strong>The “Small File Problem”:</strong> This is a classic data engineering challenge. If our trigger is 10 seconds and we are writing the output to our S3 Data Lake (for the offline store), we will generate 6 files per minute, or 8,640 tiny files per day. Data lakes are optimized for reading a small number of large files, not thousands of small ones. This severely degrades the performance of any downstream analytical queries or model training jobs.</p></li>
<li><p><strong>Consideration:</strong> A longer interval is more cost-effective and creates fewer, larger, and more optimized files in the data lake.</p></li>
</ul>
</li>
<li><p><strong>End-to-End Latency (The “How Fast”)</strong></p>
<ul class="simple">
<li><p><strong>What it is:</strong> The total time from a user’s click to that event being reflected in a feature available for inference.</p></li>
<li><p><strong>Components:</strong> <code class="docutils literal notranslate"><span class="pre">Latency</span> <span class="pre">=</span> <span class="pre">Kinesis</span> <span class="pre">Ingestion</span> <span class="pre">Lag</span> <span class="pre">+</span> <span class="pre">Spark</span> <span class="pre">Scheduling</span> <span class="pre">Delay</span> <span class="pre">+</span> <span class="pre">Trigger</span> <span class="pre">Interval</span> <span class="pre">+</span> <span class="pre">Micro-Batch</span> <span class="pre">Processing</span> <span class="pre">Time</span> <span class="pre">+</span> <span class="pre">Feature</span> <span class="pre">Store</span> <span class="pre">Write</span> <span class="pre">Time</span></code>.</p></li>
<li><p><strong>The Trigger’s Role:</strong> The trigger interval is often the <em>largest and most controllable</em> component of this latency. A 1-minute trigger guarantees a <em>minimum</em> of 1 minute of data staleness for some events.</p></li>
<li><p><strong>Consideration:</strong> You must ensure your <strong>Micro-Batch Processing Time &lt; Trigger Interval</strong>. If it takes 45 seconds to process a micro-batch, setting a 30-second trigger is impossible; the pipeline will continuously fall behind, and its lag will grow indefinitely.</p></li>
</ul>
</li>
<li><p><strong>Operational Stability &amp; State Management (The “How Reliable”)</strong></p>
<ul class="simple">
<li><p><strong>What it is:</strong> The reliability of the streaming job itself.</p></li>
<li><p><strong>Stateful Operations:</strong> Our pipeline is stateful because we are calculating session-based features (e.g., <code class="docutils literal notranslate"><span class="pre">pages_viewed_in_session</span></code>). Spark needs to hold the state for every active user session in memory (spilling to checkpoint storage on S3).</p></li>
<li><p><strong>Impact of Interval:</strong> A longer trigger interval means each micro-batch contains more data and potentially updates the state for more users at once. This increases the memory pressure on the Spark executors during that batch. A very long interval could increase the risk of Out-Of-Memory (OOM) errors if a single batch becomes too large to process.</p></li>
<li><p><strong>Recovery Time:</strong> If a job fails, Spark will recover from the last successful checkpoint. A shorter trigger interval means checkpoints happen more frequently, so the amount of data to reprocess upon failure is smaller, leading to faster recovery.</p></li>
</ul>
</li>
</ol>
</section>
<section id="summary-of-trade-offs">
<h5>Summary of Trade-offs<a class="headerlink" href="#summary-of-trade-offs" title="Permalink to this heading">¶</a></h5>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Trigger Interval</p></th>
<th class="head text-left"><p>Feature Freshness</p></th>
<th class="head text-left"><p>Cost</p></th>
<th class="head text-left"><p>Data Lake Friendliness</p></th>
<th class="head text-left"><p>Latency</p></th>
<th class="head text-left"><p>Stability Risk</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Very Short (e.g., &lt; 10s)</strong></p></td>
<td class="text-left"><p>Excellent</p></td>
<td class="text-left"><p>High</p></td>
<td class="text-left"><p>Poor (many small files)</p></td>
<td class="text-left"><p>Low</p></td>
<td class="text-left"><p>Higher (CPU churn, risk of falling behind)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Medium (e.g., 30s - 2 min)</strong></p></td>
<td class="text-left"><p>Good</p></td>
<td class="text-left"><p>Medium</p></td>
<td class="text-left"><p>Good</p></td>
<td class="text-left"><p>Medium</p></td>
<td class="text-left"><p>Balanced</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Long (e.g., &gt; 5 min)</strong></p></td>
<td class="text-left"><p>Fair / Poor</p></td>
<td class="text-left"><p>Low</p></td>
<td class="text-left"><p>Excellent (fewer, larger files)</p></td>
<td class="text-left"><p>High</p></td>
<td class="text-left"><p>Lower (but larger recovery on failure)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="recommendation-for-our-project">
<h5>Recommendation for Our Project<a class="headerlink" href="#recommendation-for-our-project" title="Permalink to this heading">¶</a></h5>
<p>Given the factors above, a fixed <code class="docutils literal notranslate"><span class="pre">ProcessingTime</span></code> trigger is the most appropriate choice.</p>
<p><strong>The recommended trigger interval for this project is between 30 seconds and 2 minutes.</strong></p>
<p>A good starting point would be <strong>1 minute</strong>.</p>
<p><strong>Justification:</strong></p>
<ol class="arabic simple">
<li><p><strong>Business Value:</strong> A 1-minute feature freshness is more than sufficient for effective in-session personalization. A user’s overall intent doesn’t change drastically on a second-by-second basis. This interval allows us to capture significant actions (like adding to cart) and use them to influence the experience within the next few page loads.</p></li>
<li><p><strong>Cost-Effectiveness:</strong> A 1-minute interval avoids the severe “small file problem” and prevents excessive CPU churn, keeping operational costs reasonable. It produces 1,440 files per day, which is manageable for downstream systems.</p></li>
<li><p><strong>Latency:</strong> This adds a predictable latency component that fits well within the overall system design. It is not trying to be a sub-second, real-time trading system.</p></li>
<li><p><strong>Stability:</strong> It creates micro-batches of a manageable size for our stateful session calculations, providing a good balance between memory pressure and recovery time.</p></li>
</ol>
<p>This interval is an initial, reasoned choice. In a true production rollout, this parameter would be fine-tuned based on load testing and by monitoring the pipeline’s processing time and end-to-end latency metrics in the staging environment.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="feature-engineering-and-pipelines-crafting-the-predictive-signals">
<h3>6. Feature Engineering and Pipelines: Crafting the Predictive Signals<a class="headerlink" href="#feature-engineering-and-pipelines-crafting-the-predictive-signals" title="Permalink to this heading">¶</a></h3>
<p>Feature engineering is the alchemical process of transmuting raw data into the valuable, predictive signals that fuel machine learning models. It is widely acknowledged that the quality and relevance of features often have a more significant impact on model performance than the choice of the algorithm itself.</p>
<p>This section details the feature engineering strategy for our propensity scoring model. We will cover the specific features to be created, the design of the hybrid batch and streaming pipelines responsible for their computation, and the role of the Feature Store as the central nervous system for managing and serving these features consistently across training and inference.</p>
<section id="feature-engineering-lifecycle-and-strategy">
<h4>6.1 Feature Engineering Lifecycle and Strategy<a class="headerlink" href="#feature-engineering-lifecycle-and-strategy" title="Permalink to this heading">¶</a></h4>
<p>Our feature engineering process follows a structured lifecycle, ensuring that every feature is well-designed, validated, and managed.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Lifecycle Stage</p></th>
<th class="head text-left"><p>Application to the Propensity Scoring Project</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Feature Ideation</strong></p></td>
<td class="text-left"><p>Based on domain knowledge and EDA, we hypothesize that a user’s purchase intent is a function of their long-term habits, their current session’s intensity, and their interactions with specific products. This leads to four categories of features: User, Product, Session, and User-Product interaction features.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Data Sourcing</strong></p></td>
<td class="text-left"><p>All features will be derived from the “Silver” layer of our S3 data lake, which contains cleaned, structured clickstream, transactional, and customer data.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Feature Transformation &amp; Generation</strong></p></td>
<td class="text-left"><p>A hybrid approach is adopted:<br>- <strong>Batch Pipeline (Daily):</strong> Computes long-term historical features (User &amp; Product level).<br>- <strong>Streaming Pipeline (Real-Time):</strong> Computes volatile, short-term session features (Session &amp; Session-Product level).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Feature Validation</strong></p></td>
<td class="text-left"><p>Data quality checks (using <strong>Great Expectations</strong>) will be embedded within the generation pipelines to validate features before they are saved to the Feature Store. Checks will include null-value constraints, range checks, and distribution comparisons.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Feature Storage &amp; Management</strong></p></td>
<td class="text-left"><p>We will use <strong>Feast</strong> as our centralized Feature Store. All engineered features will be registered and stored in Feast’s offline (S3) and online (ElastiCache for Redis) stores.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Feature Serving &amp; Monitoring</strong></p></td>
<td class="text-left"><p>Features will be served by Feast to the training and inference pipelines. Production features will be monitored for drift using <strong>Evidently AI</strong>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="a-lexicon-of-features-for-purchase-intent">
<h4>6.2 A Lexicon of Features for Purchase Intent<a class="headerlink" href="#a-lexicon-of-features-for-purchase-intent" title="Permalink to this heading">¶</a></h4>
<p>The following table details the specific features that will be engineered. They are categorized by entity and computation frequency, aligning with our hybrid pipeline strategy.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Feature Category</p></th>
<th class="head text-left"><p>Computation</p></th>
<th class="head text-left"><p>Feature Name</p></th>
<th class="head text-left"><p>Description &amp; Rationale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>User Features</strong></p></td>
<td class="text-left"><p><strong>Batch (Daily)</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">lifetime_purchase_count</span></code></p></td>
<td class="text-left"><p>Total number of purchases made by a user. Strong indicator of loyalty.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">avg_order_value_90d</span></code></p></td>
<td class="text-left"><p>Average monetary value of a user’s orders over the last 90 days. Indicates spending habits.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">days_since_last_purchase</span></code></p></td>
<td class="text-left"><p>Time elapsed since the user’s last purchase. Recency is a powerful predictor of re-engagement.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">preferred_product_category</span></code></p></td>
<td class="text-left"><p>The product category most frequently purchased by the user. Captures long-term affinity.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Product Features</strong></p></td>
<td class="text-left"><p><strong>Batch (Daily)</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">purchase_count_30d</span></code></p></td>
<td class="text-left"><p>Total number of times a product has been purchased in the last 30 days. A measure of product popularity.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">view_to_purchase_rate_30d</span></code></p></td>
<td class="text-left"><p>The ratio of purchases to views for a product in the last 30 days. A proxy for the product’s conversion efficiency.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">avg_price_7d</span></code></p></td>
<td class="text-left"><p>The average selling price of the product over the last 7 days. Captures price volatility.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Session Features</strong></p></td>
<td class="text-left"><p><strong>Streaming (Real-Time)</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">session_duration_seconds</span></code></p></td>
<td class="text-left"><p>The elapsed time since the start of the current session. Measures user engagement.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">distinct_products_viewed</span></code></p></td>
<td class="text-left"><p>The count of unique products viewed in the current session. Indicates browsing breadth.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">add_to_cart_count</span></code></p></td>
<td class="text-left"><p>The number of “add to cart” events in the session. A very strong intent signal.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">is_weekend</span></code>, <code class="docutils literal notranslate"><span class="pre">hour_of_day</span></code></p></td>
<td class="text-left"><p>Temporal features that capture contextual user behavior patterns.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">device_type</span></code>, <code class="docutils literal notranslate"><span class="pre">channel_type</span></code></p></td>
<td class="text-left"><p>Contextual features describing how the user is accessing the platform.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Session-Product Features</strong></p></td>
<td class="text-left"><p><strong>Streaming (Real-Time)</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">product_views_in_session</span></code></p></td>
<td class="text-left"><p>The number of times the current user has viewed a specific product in the current session. Indicates specific interest.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">time_since_last_view_of_product</span></code></p></td>
<td class="text-left"><p>The time elapsed since the user last viewed the specific product. Captures recency of interest within the session.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="architecting-the-feature-engineering-pipelines">
<h4>6.3 Architecting the Feature Engineering Pipelines<a class="headerlink" href="#architecting-the-feature-engineering-pipelines" title="Permalink to this heading">¶</a></h4>
<p>We will implement two parallel pipelines, each tailored to the specific freshness requirements of our features.</p>
<section id="the-daily-batch-feature-pipeline">
<h5>6.3.1 The Daily Batch Feature Pipeline<a class="headerlink" href="#the-daily-batch-feature-pipeline" title="Permalink to this heading">¶</a></h5>
<p>This pipeline is responsible for computing features that are based on long-term historical data and do not need to be updated in real-time.</p>
<ul class="simple">
<li><p><strong>Orchestration:</strong> An <strong>Apache Airflow</strong> DAG, scheduled to run once daily after midnight.</p></li>
<li><p><strong>Compute Engine:</strong> An <strong>AWS EMR</strong> cluster running <strong>Apache Spark</strong>. Spark is chosen for its ability to efficiently process large volumes of historical data from the S3 data lake.</p></li>
<li><p><strong>Workflow Steps (within the Airflow DAG):</strong></p>
<ol class="arabic simple">
<li><p><strong>Launch EMR Cluster:</strong> The DAG begins by provisioning a transient EMR cluster.</p></li>
<li><p><strong>Run Spark Job:</strong> Submits a Spark job that reads from the “Silver” layer tables, computes all “User Features” and “Product Features”, and saves the output to a temporary S3 location.</p></li>
<li><p><strong>Validate Features:</strong> A Python task runs a <strong>Great Expectations</strong> suite on the newly computed features to ensure quality.</p></li>
<li><p><strong>Load to Feast:</strong> A Python task calls <code class="docutils literal notranslate"><span class="pre">feast</span> <span class="pre">materialize</span></code> to load the validated features from the temporary location into the <strong>Feast offline store (S3)</strong>. A subset (e.g., features for recently active users) is also materialized to the <strong>Feast online store (Redis)</strong> to refresh the long-term context for active users.</p></li>
<li><p><strong>Terminate EMR Cluster:</strong> The DAG terminates the EMR cluster to save costs.</p></li>
</ol>
</li>
</ul>
</section>
<section id="the-real-time-streaming-feature-pipeline">
<h5>6.3.2 The Real-Time Streaming Feature Pipeline<a class="headerlink" href="#the-real-time-streaming-feature-pipeline" title="Permalink to this heading">¶</a></h5>
<p>This pipeline processes live clickstream events to compute and serve volatile, session-based features with low latency.</p>
<ul class="simple">
<li><p><strong>Core Challenge:</strong> As identified in our planning phase, the primary challenge is managing state for potentially millions of long-lived user sessions.</p></li>
<li><p><strong>Compute Engine:</strong> A continuously running <strong>Spark Structured Streaming</strong> application on a persistent AWS EMR cluster (or a similar service like Databricks).</p></li>
<li><p><strong>State Management Solution:</strong> We will use Spark’s built-in stateful streaming capabilities (<code class="docutils literal notranslate"><span class="pre">flatMapGroupsWithState</span></code>). To handle the large state and ensure fault tolerance, the state will be checkpointed to <strong>Amazon S3</strong>. A session state timeout (e.g., 24 hours of inactivity) will be implemented within the stateful function to automatically purge expired session data, preventing infinite state growth.</p></li>
<li><p><strong>Workflow Steps (Continuous Application):</strong></p>
<ol class="arabic simple">
<li><p><strong>Consume from Kinesis:</strong> The Spark job reads enriched events from the “processed” Kinesis Data Stream.</p></li>
<li><p><strong>Stateful Transformation:</strong> The <code class="docutils literal notranslate"><span class="pre">flatMapGroupsWithState</span></code> operation is applied, grouped by <code class="docutils literal notranslate"><span class="pre">user_id</span></code> and <code class="docutils literal notranslate"><span class="pre">session_id</span></code>. Within the stateful function, all “Session Features” and “Session-Product Features” are updated based on the incoming events.</p></li>
<li><p><strong>Write to Online Store:</strong> The output stream of updated features is written directly to the <strong>Feast online store (Amazon ElastiCache for Redis)</strong>. Redis is chosen over DynamoDB for its superior in-memory performance, which is critical for meeting our sub-100ms inference latency SLA.</p></li>
<li><p><strong>Write to Offline Archive:</strong> In parallel, the output stream is also written to a “Gold” table in the S3 data lake. This provides a historical archive of real-time features, which is essential for ensuring point-in-time correctness during training. The <strong>Feast</strong> framework will later use this offline archive to create consistent training datasets.</p></li>
</ol>
</li>
</ul>
<p>This hybrid pipeline architecture provides a robust and efficient solution. The batch pipeline cost-effectively handles large-scale historical computations, while the streaming pipeline delivers the ultra-fresh, session-level features required for impactful, real-time personalization. The Feast Feature Store acts as the unifying layer, ensuring consistency between these two worlds.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="model-development-iteration">
<h3>7. Model Development &amp; Iteration<a class="headerlink" href="#model-development-iteration" title="Permalink to this heading">¶</a></h3>
<p>The goal here is not to build a single, perfect model in one attempt, but to follow a rigorous, iterative process of experimentation. We will start with the simplest possible baselines, measure performance meticulously, and justify every increase in complexity with tangible improvements in our ability to predict purchase intent.</p>
<section id="foundations-for-success-the-modeling-blueprint">
<h4>7.1 Foundations for Success: The Modeling Blueprint<a class="headerlink" href="#foundations-for-success-the-modeling-blueprint" title="Permalink to this heading">¶</a></h4>
<p>Before the first model is trained, we establish the strategic foundations that will guide our entire development process.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Factor to Consider</p></th>
<th class="head text-left"><p>Decision / Choice Made</p></th>
<th class="head text-left"><p>Rationale &amp; Trade-offs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Success Metrics</strong></p></td>
<td class="text-left"><p><strong>Optimizing:</strong> <strong>Area Under the ROC Curve (AUC)</strong>.<br><strong>Satisficing:</strong> p99 Inference Latency &lt; 100ms; Precision at a 0.9 score threshold &gt; 75%.<br><strong>Business KPI:</strong> Increase in Conversion Rate.</p></td>
<td class="text-left"><p><strong>AUC</strong> is the primary metric because it measures the model’s ability to correctly rank sessions by their likelihood to convert, which is vital for targeting different user segments. <strong>Latency</strong> is a hard constraint for the real-time user experience. <strong>Precision</strong> at the high-score threshold is a key business constraint to minimize giving discounts to users who would have purchased anyway.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Data Splitting</strong></p></td>
<td class="text-left"><p><strong>Strict Temporal Split.</strong> The training data will use events up to a specific cutoff date (e.g., end of Week 3), and the model will be evaluated on events from the subsequent period (Week 4).</p></td>
<td class="text-left"><p>This is the only valid approach for a time-dependent problem like predicting user behavior. A random split would leak future information into the training set, resulting in an overly optimistic and misleading evaluation of the model’s true performance.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Baseline Models</strong></p></td>
<td class="text-left"><p>1. <strong>Heuristic Baseline:</strong> A simple rule (e.g., “predict ‘purchase’ if <code class="docutils literal notranslate"><span class="pre">add_to_cart_count</span></code> &gt; 0”).<br>2. <strong>ML Baseline:</strong> A <strong>Logistic Regression</strong> model using only basic session-level features.</p></td>
<td class="text-left"><p>The heuristic baseline establishes the absolute minimum performance bar and confirms the value of using ML. The simple logistic regression model validates the end-to-end training pipeline and provides a robust, interpretable initial performance floor to beat.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Primary Model Family</strong></p></td>
<td class="text-left"><p><strong>Gradient Boosted Trees (LightGBM/XGBoost)</strong></p></td>
<td class="text-left"><p>This model family is the industry standard for tabular data. It offers state-of-the-art performance, is computationally efficient, and provides good interpretability through feature importance plots. It strikes the best balance of accuracy and operational feasibility for this problem.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Experiment Tracking</strong></p></td>
<td class="text-left"><p><strong>MLflow</strong></p></td>
<td class="text-left"><p>Aligned with our tech stack, MLflow will be used to automatically log the parameters, metrics, code versions, and artifacts for every single training run. This ensures complete reproducibility and simplifies the comparison of different model iterations.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>6. Debugging &amp; Diagnostics</strong></p></td>
<td class="text-left"><p><strong>SHAP (SHapley Additive exPlanations)</strong> will be used for feature importance and explaining individual predictions. <strong>Learning curves</strong> will be analyzed to diagnose bias vs. variance issues.</p></td>
<td class="text-left"><p>SHAP is critical for gaining business trust by explaining <em>why</em> a particular user session was given a high or low score. This is invaluable for debugging and refining personalization strategies.</p></td>
</tr>
</tbody>
</table>
</div>
<!--
#### 7.2 A Step-by-Step Experimental Journey

Our approach to model development was methodical. We started with the simplest baselines and incrementally added complexity, ensuring each step delivered a measurable improvement in our primary metric, **AUC**, while respecting our satisficing constraints.

| Experiment No. | Model/Technique Applied | Key Features Used | Result & Key Learning |
| :--- | :--- | :--- | :--- |
| **1. Heuristic Baseline** | Rule: Predict `purchase` if `add_to_cart_count > 0` | Session `add_to_cart_count` | **Result:** Achieved an AUC of ~0.62. High recall but very poor precision.<br>**Learning:** This simple rule captures some intent but generates too many false positives. It's a weak but useful baseline to demonstrate the value of a more nuanced approach. |
| **2. Simple ML Baseline**| **Logistic Regression** | Basic session features (`session_duration`, `page_view_count`, `device_type`). | **Result:** **AUC of 0.75.**<br>**Learning:** A simple linear model significantly outperforms the heuristic, confirming that combining multiple session signals is effective. The model's coefficients provided initial, interpretable insights (e.g., longer sessions and desktop users are more likely to convert). |
| **3. Gradient Boosting**| **LightGBM** | Same basic session features as above. | **Result:** **AUC of 0.80.**<br>**Learning:** LightGBM's ability to capture non-linear interactions between features provided a major performance lift. For example, it could learn that a long session on mobile has a different intent signal than a long session on desktop. This became our new champion model. |
| **4. Adding User History** | LightGBM | Session features + **Long-Term User Features** (`lifetime_purchase_count`, `days_since_last_purchase`). *Note: This model is only applicable to known, identified users.* | **Result:** **AUC of 0.84** for the *identified user segment*. <br>**Learning:** Historical context is the single most powerful predictor for known users. Past behavior is a strong indicator of future behavior. This validated our strategy of needing two distinct models (or feature sets) for anonymous vs. known users. |
| **5. Adding Real-Time Features**| LightGBM | All previous features + **Real-time Streaming Features** (`add_to_cart_count_in_session`, `product_views_in_session`). | **Result:** **Overall AUC increased to 0.87**. The biggest gains were seen in correctly identifying intent early in a session, even for anonymous users.<br>**Learning:** Fresh, in-session activity provides a powerful signal of immediate intent. This confirmed the business value of investing in the real-time feature pipeline. |
| **6. Hyperparameter Optimization**| **LightGBM with SageMaker Automatic Model Tuning** (Bayesian Search) | All available features. | **Result:** **Final AUC of 0.89-0.90**. <br>**Learning:** Automated HPO provided a final, incremental boost by fine-tuning the model's complexity (e.g., number of trees, learning rate), leading to a more generalized model with slightly better performance on the held-out test set. |

**Final Outcome:** Through this structured, iterative process, we developed a final LightGBM model that was over **25% better (in terms of AUC)** than our initial simple ML baseline. Crucially, the experiments validated our core architectural decisions: the need for both batch and streaming features, and the necessity of handling anonymous and identified users differently. The final model demonstrates a strong ability to distinguish between browsing sessions and those with genuine purchase intent, providing a reliable foundation for the personalization engine.

-->
</section>
</section>
<hr class="docutils" />
<section id="ml-training-pipelines">
<h3>8. ML Training Pipelines<a class="headerlink" href="#ml-training-pipelines" title="Permalink to this heading">¶</a></h3>
<p>The experimental model from the previous phase must be transformed into a standardized, automated, and reliable production workflow. The ML Training Pipeline is the MLOps “assembly line” responsible for this. It codifies every step, from data ingestion to model validation, ensuring that every production model is built and evaluated with the same rigor and consistency.</p>
<section id="training-pipeline-design-and-architecture">
<h4>8.1 Training Pipeline Design and Architecture<a class="headerlink" href="#training-pipeline-design-and-architecture" title="Permalink to this heading">¶</a></h4>
<p>Our training pipeline will be orchestrated by <strong>Apache Airflow</strong>. It will be designed as a Directed Acyclic Graph (DAG) that can be triggered on a schedule (e.g., weekly) or by an alert from our monitoring system, indicating model or data drift. This pipeline will leverage managed AWS services for scalable computation to avoid managing a dedicated training cluster.</p>
<section id="architecture-diagram">
<h5><strong>Architecture Diagram</strong><a class="headerlink" href="#architecture-diagram" title="Permalink to this heading">¶</a></h5>
<div class="highlight-mermaid notranslate"><div class="highlight"><pre><span></span>graph TD
    subgraph Airflow Orchestration
        A[Start] --&gt; B(Fetch Training Data from Feast);
        B --&gt; C(Validate Input Data);
        C --&gt; D(Train Model on SageMaker);
        D --&gt; E(Evaluate Model Performance);
        E --&gt; F{Performance &gt; Production?};
        F -- Yes --&gt; G(Behavioral &amp; Fairness Tests);
        G --&gt; H{All Tests Pass?};
        H -- Yes --&gt; I(Register Model in MLflow);
        I --&gt; J[End];
        F -- No --&gt; K(End &amp; Alert);
        H -- No --&gt; L(End &amp; Alert);
    end

    subgraph AWS &amp; MLOps Tooling
        B --&gt; Feast(Feast Offline Store - S3);
        C --&gt; GE(Great Expectations);
        D --&gt; SM(SageMaker Training Job);
        E --&gt; MLflow(MLflow Tracking Server);
        G --&gt; Pytest(Pytest Checklists);
        I --&gt; MLflowRegistry(MLflow Model Registry);
    end
</pre></div>
</div>
</section>
</section>
<section id="pipeline-components-and-implementation-plan">
<h4>8.2 Pipeline Components and Implementation Plan<a class="headerlink" href="#pipeline-components-and-implementation-plan" title="Permalink to this heading">¶</a></h4>
<p>The Airflow DAG will be composed of the following modular, containerized tasks.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Stage</p></th>
<th class="head text-left"><p>Pipeline Task (Component)</p></th>
<th class="head text-left"><p>Implementation Details &amp; Tools</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Data Preparation</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">fetch_and_validate_data</span></code></p></td>
<td class="text-left"><p><strong>Script (<code class="docutils literal notranslate"><span class="pre">data_prep.py</span></code>):</strong><br>- Fetches a point-in-time correct training dataset from the <strong>Feast</strong> offline store using <code class="docutils literal notranslate"><span class="pre">get_historical_features()</span></code> specifying a fresh cutoff date.<br>- <strong>Data Validation (Great Expectations):</strong> The script then runs a validation suite against the fetched data to check for schema compliance, null values, and feature ranges. <br><strong>Output:</strong> A validated training dataset is saved to a versioned S3 path for the current pipeline run.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Model Training</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">train_model</span></code></p></td>
<td class="text-left"><p><strong>Script (<code class="docutils literal notranslate"><span class="pre">train.py</span></code>):</strong><br>- This script is packaged into a Docker container and submitted as an <strong>Amazon SageMaker Training Job</strong> using Airflow’s <code class="docutils literal notranslate"><span class="pre">SageMakerTrainingOperator</span></code>.<br>- <strong>MLflow Tracking:</strong> The script is heavily instrumented with <strong>MLflow</strong> to log all parameters, metrics (like training loss), and the final trained model artifact directly to the MLflow server.<br><strong>Output:</strong> A trained model artifact logged in MLflow and associated with the current pipeline run.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Offline Evaluation</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">evaluate_model</span></code></p></td>
<td class="text-left"><p><strong>Script (<code class="docutils literal notranslate"><span class="pre">evaluate.py</span></code>):</strong><br>- Loads the trained model from the MLflow run and the held-out test set.<br>- <strong>Calculates Core Metrics:</strong> Computes <strong>AUC</strong>, Precision, Recall, and F1-score.<br>- <strong>Compares to Production:</strong> Fetches the metrics of the current “Production” model from the MLflow Model Registry and compares the new model’s AUC. The result (e.g., <code class="docutils literal notranslate"><span class="pre">is_better:</span> <span class="pre">True/False</span></code>) is passed to the next task via XComs. <br><strong>Output:</strong> A detailed evaluation report (<code class="docutils literal notranslate"><span class="pre">evaluation.json</span></code>) and performance plots saved as MLflow artifacts.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Comprehensive Testing &amp; Validation</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">run_advanced_tests</span></code></p></td>
<td class="text-left"><p><strong>Script (<code class="docutils literal notranslate"><span class="pre">advanced_tests.py</span></code>):</strong><br>- This task executes only if the new model is better than the production model.<br>- <strong>Sliced Evaluation:</strong> It evaluates AUC on critical user segments (Anonymous vs. Known, Mobile vs. Desktop) and asserts that performance does not drop by more than a set threshold (e.g., 5%) on any slice.<br>- <strong>Behavioral Testing (<code class="docutils literal notranslate"><span class="pre">pytest</span></code>):</strong> Runs a suite of predefined behavioral tests:<br>  - <strong>Invariance Test:</strong> Asserts that the prediction is unchanged when a non-predictive feature (e.g., <code class="docutils literal notranslate"><span class="pre">session_id</span></code>) is altered.<br>  - <strong>Directional Test:</strong> Asserts that adding an <code class="docutils literal notranslate"><span class="pre">add_to_cart</span></code> event to a session’s data <em>increases</em> its propensity score.<br>- <strong>Fairness &amp; Calibration (Conceptual):</strong> While not implemented in V1, this task would include checks for fairness metrics and model calibration here.<br><strong>Output:</strong> A boolean flag indicating if all advanced tests passed.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Model Registration</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">register_model</span></code></p></td>
<td class="text-left"><p><strong>Script (<code class="docutils literal notranslate"><span class="pre">register.py</span></code>):</strong><br>- This task runs only if all previous validation and testing steps pass.<br>- It uses the MLflow client to take the model artifact from the current run and registers it as a new version in the <strong>MLflow Model Registry</strong>.<br>- The new version is initially placed in the <strong>“Staging”</strong> stage, ready for the CD pipeline to pick it up for deployment.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="artifacts-to-be-implemented">
<h4>8.3 Artifacts to be Implemented<a class="headerlink" href="#artifacts-to-be-implemented" title="Permalink to this heading">¶</a></h4>
<p>This plan requires the development and versioning of the following key artifacts:</p>
<ol class="arabic simple">
<li><p><strong>Python Scripts (<code class="docutils literal notranslate"><span class="pre">/src</span></code>):</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data_prep.py</span></code>: Logic for fetching data from Feast and running validation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train.py</span></code>: The core model training logic for our LightGBM model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">evaluate.py</span></code>: Script for calculating and comparing model performance metrics.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">advanced_tests.py</span></code>: The script containing the sliced evaluation and behavioral tests.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">register.py</span></code>: A utility script to interact with the MLflow Model Registry.</p></li>
</ul>
</li>
<li><p><strong>Unit Tests (<code class="docutils literal notranslate"><span class="pre">/tests</span></code>):</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test_data_prep.py</span></code>: Unit tests for the data preparation logic with mocked Feast responses.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_train.py</span></code>: Unit tests for the training script’s utility functions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_evaluate.py</span></code>: Unit tests for the evaluation metric calculation logic.</p></li>
</ul>
</li>
<li><p><strong>Pipeline Definition (<code class="docutils literal notranslate"><span class="pre">/pipelines/dags</span></code>):</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_retraining_dag.py</span></code>: The Airflow DAG file that defines the tasks, dependencies, schedule, and alerting for the entire training pipeline.</p></li>
</ul>
</li>
<li><p><strong>Infrastructure as Code (<code class="docutils literal notranslate"><span class="pre">/infrastructure</span></code>):</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sagemaker.tf</span></code>: Terraform configuration for the IAM roles required for SageMaker Training Jobs to access S3 and Feast.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">airflow_connections.tf</span></code>: Terraform to configure the AWS connections required by Airflow.</p></li>
</ul>
</li>
<li><p><strong>Integration &amp; E2E Tests (<code class="docutils literal notranslate"><span class="pre">/pipelines/tests</span></code>):</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test_training_pipeline_integration.py</span></code>: A <code class="docutils literal notranslate"><span class="pre">pytest</span></code> script designed to be run in a staging environment. It triggers the Airflow DAG with a small dataset and asserts that a model is successfully trained and registered in a staging MLflow instance. This validates the entire pipeline flow and component integrations.</p></li>
</ul>
</li>
<li><p><strong>CI/CD Workflows (<code class="docutils literal notranslate"><span class="pre">/.github/workflows</span></code>):</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ci_training_pipeline.yml</span></code>: A GitHub Actions workflow that runs on every PR. It will execute:</p>
<ul>
<li><p>Linting and static analysis on all Python code.</p></li>
<li><p>Unit tests for all <code class="docutils literal notranslate"><span class="pre">src</span></code> components.</p></li>
<li><p>Airflow DAG syntax validation (<code class="docutils literal notranslate"><span class="pre">airflow</span> <span class="pre">dags</span> <span class="pre">test</span></code>).</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">cd_training_pipeline.yml</span></code>: A GitHub Actions workflow triggered on merge to <code class="docutils literal notranslate"><span class="pre">main</span></code>. It will deploy the updated Airflow DAG and any associated containers to the production Airflow environment.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="deployment-serving-and-inference">
<h3>9. Deployment, Serving, and Inference<a class="headerlink" href="#deployment-serving-and-inference" title="Permalink to this heading">¶</a></h3>
<p>With a validated and versioned model artifact in our MLflow Registry, we now shift focus to the critical “last mile” of MLOps: deploying the model as a scalable, low-latency service and building the real-time inference pipeline to consume it. This is where the model begins to deliver tangible business value.</p>
<section id="overarching-deployment-and-serving-strategy">
<h4>9.1 Overarching Deployment and Serving Strategy<a class="headerlink" href="#overarching-deployment-and-serving-strategy" title="Permalink to this heading">¶</a></h4>
<p>Our strategy is guided by the project’s core requirements: low latency for a real-time user experience and high availability to support a production e-commerce site.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Strategic Factor</p></th>
<th class="head text-left"><p>Decision / Choice</p></th>
<th class="head text-left"><p>Rationale &amp; Trade-offs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Deployment Pattern</strong></p></td>
<td class="text-left"><p><strong>Online Prediction (Model-as-a-Service)</strong></p></td>
<td class="text-left"><p>The business need for in-session personalization requires immediate, synchronous predictions. Batch prediction is too slow and would result in stale, irrelevant user experiences.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Serving Architecture</strong></p></td>
<td class="text-left"><p><strong>Microservice on a Managed Platform (Amazon SageMaker)</strong></p></td>
<td class="text-left"><p>We will deploy the model as a standalone microservice to decouple it from the main e-commerce application. This allows for independent scaling and updates. We’ve chosen <strong>Amazon SageMaker Endpoints</strong> to abstract away the underlying Kubernetes complexity, reduce operational overhead, and leverage built-in features like autoscaling and monitoring.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. API Style</strong></p></td>
<td class="text-left"><p><strong>REST API</strong></p></td>
<td class="text-left"><p>A RESTful API using JSON payloads is chosen for its simplicity and broad compatibility. While gRPC offers lower latency, the overhead of managing Protobufs and gRPC clients was deemed unnecessary for this initial version. Performance can be optimized via other means.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Release Strategy</strong></p></td>
<td class="text-left"><p><strong>Canary Release followed by A/B Testing</strong></p></td>
<td class="text-left"><p>New model versions will be deployed using a <strong>Canary Release</strong> strategy (e.g., starting with 5% of traffic) to safely validate operational performance. Once stable, a formal <strong>A/B test</strong> will be conducted to measure the new model’s impact on the primary business KPI (Conversion Rate) before a full rollout.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>5. Model &amp; Infrastructure Governance</strong></p></td>
<td class="text-left"><p><strong>CI/CD for Serving &amp; Automated Rollbacks</strong></p></td>
<td class="text-left"><p>All infrastructure will be managed via Terraform. Deployments will be automated through GitHub Actions. Critical monitoring alerts (e.g., a spike in p99 latency or error rate on the canary) will be configured to trigger an automated rollback to the previous stable model version.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="pre-deployment-preparations-packaging-the-model-for-serving">
<h4>9.2 Pre-Deployment Preparations: Packaging the Model for Serving<a class="headerlink" href="#pre-deployment-preparations-packaging-the-model-for-serving" title="Permalink to this heading">¶</a></h4>
<p>Before deployment, the model artifact must be packaged into a self-contained, reproducible unit.</p>
<ol class="arabic simple">
<li><p><strong>Model Serialization:</strong> The trained LightGBM model will be serialized using <code class="docutils literal notranslate"><span class="pre">joblib</span></code> or <code class="docutils literal notranslate"><span class="pre">pickle</span></code>.</p></li>
<li><p><strong>Containerization (Docker):</strong> We will create a <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code> that:</p>
<ul class="simple">
<li><p>Starts from a standard Python base image.</p></li>
<li><p>Copies over the model artifact and any necessary source code (e.g., pre-processing logic).</p></li>
<li><p>Installs all dependencies from a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file.</p></li>
<li><p>Uses a production-grade web server like <strong>Gunicorn</strong> to run a simple <strong>FastAPI</strong> application that wraps the model.</p></li>
</ul>
</li>
<li><p><strong>API Definition (FastAPI):</strong> The FastAPI app will define:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">/health</span></code> endpoint for health checks.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">/predict</span></code> endpoint that accepts a JSON payload matching the feature vector schema, runs the prediction, and returns the propensity score. Pydantic will be used for request/response validation.</p></li>
</ul>
</li>
<li><p><strong>Container Registry:</strong> The final Docker image will be version-tagged and pushed to <strong>Amazon Elastic Container Registry (ECR)</strong>.</p></li>
</ol>
</section>
<section id="the-real-time-inference-pipeline">
<h4>9.3 The Real-Time Inference Pipeline<a class="headerlink" href="#the-real-time-inference-pipeline" title="Permalink to this heading">¶</a></h4>
<p>This is not a scheduled pipeline like our training DAG but the logical flow of a single, live prediction request. Optimizing this path for latency is critical.</p>
<section id="id2">
<h5><strong>Architecture Diagram</strong><a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<!--
```mermaid
sequenceDiagram
    participant App as E-commerce Backend
    participant APIGW as API Gateway
    participant SME as SageMaker Endpoint
    participant F_Online as Feast Online Store (Redis)
    participant Log as Monitoring & Logging

    App->>+APIGW: POST /predict (request_id, user_id, session_id)
    APIGW->>+SME: Invoke Endpoint
    SME->>+F_Online: get_online_features(user_id, session_id)
    F_Online->>-SME: Return Feature Vector
    SME->>SME: Run Model.predict(features)
    SME->>-APIGW: Return Propensity Score
    APIGW->>-App: { "propensity": 0.85 }
    SME->>Log: Log Request, Features, and Prediction
```
-->
<img src="../_static/past_experiences/ecom_propensity/inference_sequence_diagram.svg" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="latency-budget-p99-100ms">
<h5><strong>Latency Budget (p99 &lt; 100ms)</strong><a class="headerlink" href="#latency-budget-p99-100ms" title="Permalink to this heading">¶</a></h5>
<p>The total time is the sum of each step. Our optimization targets are:</p>
<ul class="simple">
<li><p><strong>Network Overhead (App &lt;-&gt; SME):</strong> ~5-15ms</p></li>
<li><p><strong>Feature Retrieval (SME -&gt; Redis):</strong> <strong>&lt; 10ms</strong>. This is why ElastiCache for Redis was chosen.</p></li>
<li><p><strong>Model Inference (on SageMaker):</strong> <strong>&lt; 30ms</strong>. LightGBM is extremely fast.</p></li>
<li><p><strong>Pre/Post-processing:</strong> &lt; 5ms</p></li>
<li><p><strong>Safety Buffer:</strong> ~40ms</p></li>
</ul>
</section>
</section>
<section id="implementation-plan-for-the-inference-system-artifacts">
<h4>9.4 Implementation Plan for the Inference System Artifacts<a class="headerlink" href="#implementation-plan-for-the-inference-system-artifacts" title="Permalink to this heading">¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Artifact Category</p></th>
<th class="head text-left"><p>File(s) / Component</p></th>
<th class="head text-left"><p>Description &amp; Key Logic</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Python Scripts</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">/src/serving/app.py</span></code></p></td>
<td class="text-left"><p>A <strong>FastAPI</strong> application with <code class="docutils literal notranslate"><span class="pre">/predict</span></code> and <code class="docutils literal notranslate"><span class="pre">/health</span></code> endpoints. The <code class="docutils literal notranslate"><span class="pre">/predict</span></code> endpoint will:<br>1. Receive a request with <code class="docutils literal notranslate"><span class="pre">user_id</span></code> and <code class="docutils literal notranslate"><span class="pre">session_id</span></code>.<br>2. Initialize a <code class="docutils literal notranslate"><span class="pre">Feast</span></code> FeatureStore client.<br>3. Call <code class="docutils literal notranslate"><span class="pre">fs.get_online_features()</span></code> to retrieve the feature vector from Redis.<br>4. Call <code class="docutils literal notranslate"><span class="pre">model.predict_proba()</span></code> on the feature vector.<br>5. Return the JSON response.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">/src/serving/preprocessor.py</span></code></p></td>
<td class="text-left"><p>Contains any pre-processing logic (e.g., one-hot encoding for <code class="docutils literal notranslate"><span class="pre">device_type</span></code>) that must be applied to the feature vector before inference. This ensures consistency with the training pipeline.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Containerization</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code></p></td>
<td class="text-left"><p>Defines the steps to build the serving container image, including installing dependencies and setting the <code class="docutils literal notranslate"><span class="pre">CMD</span></code> to run the FastAPI app with Gunicorn.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">.dockerignore</span></code></p></td>
<td class="text-left"><p>Prevents unnecessary files (like local test data) from being copied into the container, keeping it lean.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Unit Tests</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">/tests/unit/test_serving_app.py</span></code></p></td>
<td class="text-left"><p><strong>Pytest</strong> unit tests for the FastAPI application using <code class="docutils literal notranslate"><span class="pre">TestClient</span></code>. It will:<br>- Test the <code class="docutils literal notranslate"><span class="pre">/health</span></code> endpoint.<br>- Test the <code class="docutils literal notranslate"><span class="pre">/predict</span></code> endpoint with a valid payload, mocking the <code class="docutils literal notranslate"><span class="pre">Feast</span></code> client to return a sample feature vector and asserting the response format.<br>- Test with an invalid payload and assert a <code class="docutils literal notranslate"><span class="pre">422</span> <span class="pre">Unprocessable</span> <span class="pre">Entity</span></code> error.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Infrastructure as Code</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">/infrastructure/sagemaker_endpoint.tf</span></code></p></td>
<td class="text-left"><p><strong>Terraform</strong> configuration to define the <code class="docutils literal notranslate"><span class="pre">aws_sagemaker_model</span></code>, <code class="docutils literal notranslate"><span class="pre">aws_sagemaker_endpoint_configuration</span></code>, and <code class="docutils literal notranslate"><span class="pre">aws_sagemaker_endpoint</span></code> resources. This code will reference the ECR image URI and define instance types, scaling policies, and the canary traffic split.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Integration Tests</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">/tests/integration/test_deployed_endpoint.py</span></code></p></td>
<td class="text-left"><p>A <strong>Pytest</strong> script intended to be run <em>after</em> a deployment. It sends a real HTTP request to the deployed SageMaker endpoint’s URL and validates the response. This is the final smoke test in the CD pipeline.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>CI/CD Workflow</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">/.github/workflows/cd_serving.yml</span></code></p></td>
<td class="text-left"><p>A <strong>GitHub Actions</strong> workflow responsible for Continuous Delivery of the model.<br>1. <strong>Trigger:</strong> On promotion of a model to “Production” in the MLflow Registry.<br>2. <strong>Build &amp; Push:</strong> Builds the Docker image for the new model and pushes it to ECR.<br>3. <strong>Deploy:</strong> Runs <code class="docutils literal notranslate"><span class="pre">terraform</span> <span class="pre">apply</span></code> to update the SageMaker endpoint configuration with the new container, implementing the canary traffic shift.<br>4. <strong>Test:</strong> Executes the integration test script (<code class="docutils literal notranslate"><span class="pre">test_deployed_endpoint.py</span></code>).<br>5. <strong>Rollback:</strong> If the integration test fails, it automatically runs <code class="docutils literal notranslate"><span class="pre">terraform</span> <span class="pre">apply</span></code> with the previous stable configuration.</p></td>
</tr>
</tbody>
</table>
</div>
<p>This comprehensive plan ensures that our validated model is deployed into a performant, scalable, and reliable serving environment, with automation and safety checks built into every step of the process.</p>
</section>
</section>
<hr class="docutils" />
<section id="monitoring-observability-and-model-evolution">
<h3>10. Monitoring, Observability, and Model Evolution<a class="headerlink" href="#monitoring-observability-and-model-evolution" title="Permalink to this heading">¶</a></h3>
<p>Deploying a model into production is not the finish line; it is the starting line for its operational life. A model’s performance inevitably degrades over time as it encounters data patterns and user behaviors that differ from its training set. A comprehensive Monitoring and Observability strategy is therefore non-negotiable for maintaining model accuracy, ensuring system reliability, and delivering sustained business value.</p>
<p>Our strategy is built on a proactive, multi-layered approach that covers the health of our infrastructure, the quality of our data, and the performance of the model itself.</p>
<section id="monitoring-and-observability-plan">
<h4>10.1 Monitoring and Observability Plan<a class="headerlink" href="#monitoring-and-observability-plan" title="Permalink to this heading">¶</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Pillar</p></th>
<th class="head text-left"><p>Capability</p></th>
<th class="head text-left"><p>Implementation Strategy &amp; Tooling</p></th>
<th class="head text-left"><p>Alerting &amp; Action Plan</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Service Health &amp; Reliability</strong></p></td>
<td class="text-left"><p><strong>Operational Monitoring</strong><br>To track the health and performance of the SageMaker serving endpoint.</p></td>
<td class="text-left"><p><strong>Amazon CloudWatch</strong> will be used to monitor key metrics for the SageMaker Endpoint:<br>- <strong>Latency (p90, p99):</strong> The time taken to serve a prediction.<br>- <strong>Invocation Errors (5xx Rate):</strong> The rate of server-side errors.<br>- <strong>Throttles:</strong> Indicates if the service is unable to handle the request volume.<br>- <strong>CPU/Memory/GPU Utilization:</strong> To inform scaling policies.</p></td>
<td class="text-left"><p><strong>CloudWatch Alarms</strong> will be configured:<br>- <strong>High Latency Alert (P0):</strong> If p99 latency &gt; 100ms for 5 consecutive minutes, an alert is sent to the on-call MLOps engineer via PagerDuty.<br>- <strong>High Error Rate Alert (P1):</strong> If the 5xx error rate exceeds 1% over a 15-minute period, an alert is sent to the team’s Slack channel.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Data Quality &amp; Integrity</strong></p></td>
<td class="text-left"><p><strong>Input Data Validation</strong><br>To ensure the data being sent to the live model for inference is valid and well-formed.</p></td>
<td class="text-left"><p><strong>Pydantic</strong> models will be used within the <strong>FastAPI</strong> serving application to automatically validate the schema and data types of incoming prediction requests. Malformed requests will be rejected with a <code class="docutils literal notranslate"><span class="pre">422</span></code> error and logged for analysis.</p></td>
<td class="text-left"><p><strong>Logging &amp; Monitoring:</strong> The rate of <code class="docutils literal notranslate"><span class="pre">422</span></code> errors will be monitored in CloudWatch. A sudden spike indicates a potential issue with an upstream client application and triggers a <strong>P2 alert</strong> to the development team’s Slack channel.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Feature Drift Detection</strong><br>To detect shifts in the distribution of live features compared to the training data.</p></td>
<td class="text-left"><p><strong>Evidently AI</strong> will be used in a scheduled <strong>Airflow DAG</strong> that runs daily:<br>1. The DAG reads a sample of the last 24 hours of inference requests (features) logged in S3.<br>2. It compares the distribution of each feature against a reference profile generated from the training data.<br>3. It calculates the <strong>Population Stability Index (PSI)</strong> for key numerical features and the <strong>Chi-Squared test</strong> for categorical features.</p></td>
<td class="text-left"><p><strong>Drift Alert (P1):</strong> If the PSI for a critical feature (e.g., <code class="docutils literal notranslate"><span class="pre">session_duration</span></code>) exceeds <strong>0.25</strong>, an alert with the Evidently AI report is sent to the MLOps and Data Science team’s Slack channel for investigation.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>3. Model Performance</strong></p></td>
<td class="text-left"><p><strong>Prediction Drift Detection</strong><br>To provide an early warning of potential model performance degradation when ground truth labels are not yet available.</p></td>
<td class="text-left"><p>The same daily <strong>Airflow DAG</strong> using <strong>Evidently AI</strong> will also monitor the distribution of the model’s output scores (the propensity scores).<br>It will calculate the <strong>PSI</strong> or <strong>Kolmogorov-Smirnov (K-S) statistic</strong> comparing the distribution of today’s predictions to the distribution on the validation set.</p></td>
<td class="text-left"><p><strong>Prediction Drift Alert (P1):</strong> A significant shift in the prediction distribution (e.g., PSI &gt; 0.2) triggers an alert. This is often the first sign of concept drift or a data quality issue and prompts an immediate investigation.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Ground Truth Performance Monitoring</strong><br>To track the model’s actual predictive accuracy on live traffic.</p></td>
<td class="text-left"><p>An <strong>Airflow DAG</strong> will run daily to join the logged predictions with the ground truth <code class="docutils literal notranslate"><span class="pre">purchase</span></code> events (which become available after a session ends). It will calculate and log the following metrics to <strong>MLflow</strong>:<br>- <strong>Overall AUC</strong><br>- <strong>Precision and Recall</strong></p></td>
<td class="text-left"><p><strong>Performance Degradation Alert (P1):</strong> If the overall <strong>AUC drops by more than 5%</strong> from the validation set performance for two consecutive days, an alert is triggered, suggesting the need for retraining or model analysis.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p></p></td>
<td class="text-left"><p><strong>Fairness &amp; Bias Monitoring</strong><br>To ensure the model performs equitably across key user segments.</p></td>
<td class="text-left"><p>The performance monitoring DAG will also compute <strong>sliced evaluation metrics</strong>. It will calculate and compare the <strong>AUC for key segments</strong> (e.g., Anonymous vs. Known users, Mobile vs. Desktop).</p></td>
<td class="text-left"><p><strong>Fairness Alert (P2):</strong> If the performance disparity between any two critical segments becomes too large (e.g., the difference in AUC &gt; 10%), an alert is sent for manual review. This could indicate that the model has developed a bias and needs to be retrained with more balanced data or different features.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>4. Explainability (XAI)</strong></p></td>
<td class="text-left"><p><strong>Local &amp; Global Explanations</strong><br>To understand <em>why</em> the model makes certain predictions, for debugging, and to build business trust.</p></td>
<td class="text-left"><p><strong>SHAP (SHapley Additive exPlanations)</strong> will be integrated:<br>- For a sample of live predictions, the model service will also compute and log the SHAP values for each feature.<br>- These values will be used to generate:<br>  - <strong>Global Feature Importance</strong> plots to understand the model’s overall behavior.<br>  - <strong>Local Explanations</strong> to debug specific high-stakes or anomalous predictions (e.g., “Why was this high-value cart user given a low propensity score?”).</p></td>
<td class="text-left"><p>This is primarily for offline analysis and debugging, not real-time alerting. The logged SHAP values are a critical input for any root cause analysis following a performance alert.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="continual-learning-production-testing-evolving-the-model">
<h3>11. Continual Learning &amp; Production Testing: Evolving the Model<a class="headerlink" href="#continual-learning-production-testing-evolving-the-model" title="Permalink to this heading">¶</a></h3>
<p>A deployed model is not the end of the journey; it is the beginning of a continuous cycle of monitoring, learning, and adaptation. To ensure our propensity scoring model remains accurate and delivers sustained business value, we must implement a robust strategy for continual learning and production testing. This strategy moves us from a static deployment to a dynamic, self-improving system.</p>
<section id="the-imperative-to-evolve-triggers-for-model-updates">
<h4>11.1 The Imperative to Evolve: Triggers for Model Updates<a class="headerlink" href="#the-imperative-to-evolve-triggers-for-model-updates" title="Permalink to this heading">¶</a></h4>
<p>Our models will not “age like fine wine.” Their performance will degrade due to data and concept drift. Our continual learning strategy is designed to counteract this by updating the model in response to specific, monitored triggers.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Trigger Type</p></th>
<th class="head text-left"><p>Primary Monitoring Metric</p></th>
<th class="head text-left"><p>Threshold / Condition</p></th>
<th class="head text-left"><p>Retraining Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Performance-based</strong></p></td>
<td class="text-left"><p><strong>Overall AUC (on live data)</strong></p></td>
<td class="text-left"><p>If daily AUC drops by <strong>&gt;5%</strong> from the initial validation AUC for 2 consecutive days.</p></td>
<td class="text-left"><p><strong>High Priority:</strong> Immediately triggers the automated retraining pipeline (<code class="docutils literal notranslate"><span class="pre">model_retraining_dag.py</span></code>).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Drift-based</strong></p></td>
<td class="text-left"><p><strong>Population Stability Index (PSI)</strong></p></td>
<td class="text-left"><p>If PSI on a critical feature (e.g., <code class="docutils literal notranslate"><span class="pre">session_duration</span></code>) or on the prediction distribution exceeds <strong>0.25</strong>.</p></td>
<td class="text-left"><p><strong>Medium Priority:</strong> Triggers an alert for the MLOps/DS team to investigate. If the drift is confirmed to be significant and not an anomaly, a manual retraining run is initiated.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Scheduled</strong></p></td>
<td class="text-left"><p><strong>Time</strong></p></td>
<td class="text-left"><p>Every <strong>2 weeks</strong>, regardless of other triggers.</p></td>
<td class="text-left"><p><strong>Standard Cadence:</strong> Triggers the automated retraining pipeline to capture gradual concept drift and incorporate the latest data patterns.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="retraining-and-data-curation-strategy">
<h4>11.2 Retraining and Data Curation Strategy<a class="headerlink" href="#retraining-and-data-curation-strategy" title="Permalink to this heading">¶</a></h4>
<p>Our primary approach will be <strong>Automated Stateless Retraining</strong> (Stage 2 maturity), which provides a balance of simplicity and robustness.</p>
<ul class="simple">
<li><p><strong>Retraining Mechanism:</strong> When triggered, the Airflow <code class="docutils literal notranslate"><span class="pre">model_retraining_dag.py</span></code> will train a new LightGBM model <strong>from scratch</strong>.</p></li>
<li><p><strong>Data Curation:</strong> The pipeline will use a <strong>sliding window</strong> of the most recent <strong>4 weeks</strong> of data from the Feast offline store. This ensures the model learns from the most relevant, up-to-date user behavior while discarding older, potentially stale patterns.</p></li>
<li><p><strong>Stateful Training (Future Consideration):</strong> While stateless retraining is our initial strategy, we recognize the significant cost and efficiency benefits of stateful fine-tuning. A future iteration of the MLOps platform could explore this, but it would require more complex checkpoint management and lineage tracking.</p></li>
</ul>
</section>
<section id="production-testing-the-a-b-testing-framework">
<h4>11.3 Production Testing: The A/B Testing Framework<a class="headerlink" href="#production-testing-the-a-b-testing-framework" title="Permalink to this heading">¶</a></h4>
<p>No new model, even one that shows superior offline metrics, will be fully deployed without proving its value on live traffic. We will use a rigorous A/B testing framework to measure the causal impact of new models on our primary business KPI.</p>
<p><strong>1. Hypothesis:</strong></p>
<ul class="simple">
<li><p>“Deploying the challenger model (e.g., version v2.1), retrained on the latest data, will cause a statistically significant <strong>increase of at least 1% in the overall user conversion rate</strong> compared to the champion model (v2.0) over a 2-week period.”</p></li>
</ul>
<p><strong>2. Experiment Design:</strong></p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Design Factor</p></th>
<th class="head text-left"><p>Specification</p></th>
<th class="head text-left"><p>Rationale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Primary Metric</strong></p></td>
<td class="text-left"><p><strong>User Conversion Rate</strong></p></td>
<td class="text-left"><p>This directly measures the ultimate business objective of increasing sales.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Guardrail Metrics</strong></p></td>
<td class="text-left"><p>- <strong>p99 Inference Latency (&lt; 100ms)</strong><br>- <strong>Endpoint Error Rate (&lt; 0.1%)</strong><br>- <strong>User Bounce Rate</strong> (should not increase)<br>- <strong>Unsubscribe Rate</strong> (for email campaigns using scores)</p></td>
<td class="text-left"><p>To ensure the new model does not degrade system performance or negatively impact the broader user experience. A breach of these guardrails will halt the experiment.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Unit of Randomization</strong></p></td>
<td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">user_id</span></code></strong></p></td>
<td class="text-left"><p>Ensures a consistent experience for each user across multiple sessions and provides a stable unit for analyzing behavior.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Traffic Allocation</strong></p></td>
<td class="text-left"><p><strong>50% Control (Champion), 50% Treatment (Challenger)</strong></p></td>
<td class="text-left"><p>A standard 50/50 split provides the maximum statistical power for a given total sample size.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Power &amp; Duration</strong></p></td>
<td class="text-left"><p>The experiment will be designed to have <strong>80% statistical power</strong> to detect a <strong>1% relative lift</strong> in conversion rate at a <strong>95% significance level (α=0.05)</strong>. Given our traffic estimates, this will require a run time of approximately <strong>14 days</strong>.</p></td>
<td class="text-left"><p>This ensures we can confidently detect a meaningful business impact while accounting for weekly user behavior cycles.</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>3. Rollout and Decision Workflow:</strong></p>
<p>The deployment and testing of a new model will follow a safe, phased process orchestrated by our CI/CD pipelines and experimentation platform.</p>
<img src="../_static/past_experiences/ecom_propensity/rollout_decision_workflow.svg" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="addressing-advanced-challenges">
<h4>11.4 Addressing Advanced Challenges<a class="headerlink" href="#addressing-advanced-challenges" title="Permalink to this heading">¶</a></h4>
<p>Our experimentation strategy also accounts for common real-world complexities:</p>
<ul class="simple">
<li><p><strong>Variance Reduction:</strong> To increase the sensitivity of our tests and potentially shorten experiment duration, we will implement <strong>CUPED (Controlled-experiment Using Pre-Experiment Data)</strong>. We will use a user’s conversion rate from the pre-experiment period as a covariate to reduce variance in the conversion rate metric measured during the test.</p></li>
<li><p><strong>Interference:</strong> As a multi-category retailer, interference between experiments is low but possible (e.g., a pricing test on one category affecting overall user budget). Our experimentation platform will use a <strong>layering</strong> system, ensuring that this propensity model test runs in a separate “ML Model” layer, orthogonal to UI or pricing experiments.</p></li>
<li><p><strong>Novelty Effects:</strong> We will monitor the daily treatment effect throughout the A/B test. If we observe a significant novelty effect (e.g., a large initial lift that quickly fades), the final decision will be based on the metrics from the period after the effect has stabilized (e.g., the second week of the test).</p></li>
</ul>
<p>This comprehensive strategy for continual learning and production testing creates a closed-loop system. It ensures that our models do not become stale, that updates are rigorously validated against real business metrics, and that we can continuously and safely evolve our personalization capabilities in a data-driven manner.</p>
</section>
</section>
<hr class="docutils" />
<section id="governance-ethics-the-human-element">
<h3>12. Governance, Ethics &amp; The Human Element<a class="headerlink" href="#governance-ethics-the-human-element" title="Permalink to this heading">¶</a></h3>
<p>Building a functional machine learning system is a significant technical achievement. However, building a <em>trustworthy</em>, <em>compliant</em>, and <em>responsible</em> system that earns the confidence of users, stakeholders, and regulators is the hallmark of a mature MLOps practice. This section outlines the principles and practices we will adopt to ensure our propensity scoring model is not only accurate but also governed, fair, and secure.</p>
<section id="comprehensive-model-governance-plan">
<h4>12.1 Comprehensive Model Governance Plan<a class="headerlink" href="#comprehensive-model-governance-plan" title="Permalink to this heading">¶</a></h4>
<p>Governance is the framework of rules, practices, and tools that ensures our ML assets are managed, versioned, and deployed in a controlled and auditable manner.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Governance Component</p></th>
<th class="head text-left"><p>Application to the Propensity Scoring Project</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Reproducibility &amp; Auditability</strong></p></td>
<td class="text-left"><p>Every model in our <strong>MLflow Model Registry</strong> will have a complete, traceable lineage. An auditor can select any production model version and trace it back to:<br>- The <strong>Git commit</strong> of the training pipeline code.<br>- The exact <strong>DVC version</strong> of the data it was trained on.<br>- The specific <strong>hyperparameters</strong> used.<br>- A full record of its <strong>evaluation metrics</strong> and <strong>artifacts</strong>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Documentation</strong></p></td>
<td class="text-left"><p>Each registered model version will be accompanied by a <strong>Model Card</strong> (a versioned <code class="docutils literal notranslate"><span class="pre">model_card.md</span></code> file). This card will detail:<br>- <strong>Model Details:</strong> Algorithm (LightGBM), version.<br>- <strong>Intended Use:</strong> To personalize user experience by scoring purchase intent.<br>- <strong>Evaluation Data:</strong> The date range of the validation set.<br>- <strong>Metrics:</strong> Key performance metrics (AUC, Precision) on both the overall test set and on critical user slices.<br>- <strong>Ethical Considerations:</strong> A summary of fairness evaluations and known limitations.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Security &amp; Access Control</strong></p></td>
<td class="text-left"><p>- <strong>Infrastructure:</strong> All AWS resources (S3 buckets, SageMaker endpoints, Kinesis streams) will be provisioned via <strong>Terraform</strong> with strict IAM policies based on the principle of least privilege.<br>- <strong>Model Registry:</strong> The MLflow Model Registry will have role-based access control. Only lead ML Engineers can approve the transition of a model from “Staging” to “Production.”<br>- <strong>Endpoint Security:</strong> The SageMaker inference endpoint will be secured within our VPC and only accessible via the API Gateway, which requires API key authentication.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Regulatory Compliance (EU AI Act)</strong></p></td>
<td class="text-left"><p>Our propensity model would likely be classified as <strong>“Limited Risk”</strong> under the EU AI Act, as it influences commercial decisions. This classification mandates <strong>transparency</strong>. Our user-facing application must include a clear disclosure (e.g., in the privacy policy) stating that AI is used to personalize product recommendations and offers. If the model’s use were to expand into “High Risk” areas (like determining credit or insurance eligibility), it would trigger far more stringent requirements, including formal conformity assessments.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="responsible-ai-rai-by-design">
<h4>12.2 Responsible AI (RAI) by Design<a class="headerlink" href="#responsible-ai-rai-by-design" title="Permalink to this heading">¶</a></h4>
<p>Responsible AI is not a checklist to be completed at the end but a set of principles to be embedded throughout the project lifecycle.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>RAI Principle</p></th>
<th class="head text-left"><p>Practice within the Propensity Scoring Project</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Fairness &amp; Bias Mitigation</strong></p></td>
<td class="text-left"><p><strong>Problem:</strong> The model could inadvertently create unfair outcomes, such as only offering discounts to users from historically high-spending demographics, effectively penalizing new users or users from different regions.<br><strong>Mitigation Strategy:</strong><br>1. <strong>Measurement:</strong> During the automated evaluation stage, we will measure and compare the model’s AUC and precision across key user segments (e.g., Anonymous vs. Known, by country, by device type).<br>2. <strong>Action:</strong> If a significant performance disparity is detected (e.g., AUC for mobile users is 10% lower than for desktop users), it will trigger a P2 alert. This will prompt a manual review, which could lead to re-weighting samples in the training data or adding features that help the model better understand the underperforming segment.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Explainability (XAI)</strong></p></td>
<td class="text-left"><p><strong>Problem:</strong> Business stakeholders (e.g., the marketing team) need to understand <em>why</em> the model is making its predictions to trust it and build effective campaigns.<br><strong>Implementation:</strong><br>- <strong>Global Explanations:</strong> We will use <strong>SHAP</strong> to generate and save feature importance plots as a standard artifact for every trained model. This shows what factors, on average, drive predictions.<br>- <strong>Local Explanations:</strong> The inference service will have the capability to log SHAP values for a sample of predictions. This allows us to perform “spot checks” and answer specific questions like, “Why was this specific user, who added three items to their cart, given a low propensity score?”</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Privacy-Preserving Techniques</strong></p></td>
<td class="text-left"><p><strong>Problem:</strong> Our data contains sensitive user information that must be protected under GDPR.<br><strong>Implementation:</strong><br>- <strong>Data Minimization:</strong> The training and inference pipelines will only have access to the specific features they need. They will not have access to raw PII like names or email addresses.<br>- <strong>Anonymization &amp; Pseudonymization:</strong> The data engineering pipeline will be responsible for hashing or masking any PII. <code class="docutils literal notranslate"><span class="pre">user_id</span></code> will be a pseudonymized identifier, not a database primary key.<br>- <strong>Right to be Forgotten:</strong> We will implement a process to remove a user’s data from the training sets and feature stores upon receiving a GDPR deletion request.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Security against ML Attacks</strong></p></td>
<td class="text-left"><p><strong>Problem:</strong> While less of a target than fraud or spam models, our system is still vulnerable.<br><strong>Mitigation Strategy:</strong><br>- <strong>Input Sanitization:</strong> The FastAPI serving app validates all incoming feature data against a strict schema. Any requests with anomalous or malformed data are rejected.<br>- <strong>Robust Data Validation:</strong> Our Great Expectations suites in the training pipeline protect us from large-scale data poisoning attacks by validating the statistical properties of our training data before the model is trained.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="holistic-testing-a-production-readiness-assessment">
<h4>12.3 Holistic Testing: A Production Readiness Assessment<a class="headerlink" href="#holistic-testing-a-production-readiness-assessment" title="Permalink to this heading">¶</a></h4>
<p>We use the spirit of the “ML Test Score” rubric to self-assess the production readiness of our system.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Test Category</p></th>
<th class="head text-left"><p>Our Project’s Scorecard</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Features &amp; Data</strong></p></td>
<td class="text-left"><p><strong>High.</strong> All features are defined in code (Feast), versioned in Git, and their costs/benefits are understood. The data pipelines have explicit validation steps (Great Expectations).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Development</strong></p></td>
<td class="text-left"><p><strong>High.</strong> Model specs are code. We have established baselines. We perform sliced evaluation and behavioral tests. Hyperparameters are tuned.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>ML Infrastructure</strong></p></td>
<td class="text-left"><p><strong>High.</strong> Training is reproducible via Airflow/MLflow/DVC. The full pipeline is integration tested. We use a canary release process and have automated rollback capabilities via our CD workflow.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Monitoring</strong></p></td>
<td class="text-left"><p><strong>High.</strong> We have implemented monitoring for service health, data drift, prediction drift, and ground truth performance, with automated alerting on each.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="the-human-element-team-user-experience">
<h4>12.4 The Human Element: Team &amp; User Experience<a class="headerlink" href="#the-human-element-team-user-experience" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><strong>Team Structure &amp; Roles:</strong> This project exemplifies the <strong>Platform-Enabled Model</strong>. The Platform/DevOps team provides the core infrastructure (AWS, Airflow, GitHub Actions). As the <strong>ML Engineer/Data Scientist</strong>, you own the end-to-end ML workflow on top of that platform—from data analysis and model development to building the automated training and deployment pipelines. This structure maximizes ownership and velocity.</p></li>
<li><p><strong>Designing for Trust:</strong> The outputs of this model directly impact the user experience. To maintain trust:</p>
<ul>
<li><p><strong>Feedback Loops:</strong> The system should be able to measure the outcome of its predictions. Did a user who was shown a discount actually convert? This feedback is essential for A/B testing and model improvement.</p></li>
<li><p><strong>Graceful Failure:</strong> What happens if the inference service fails? The e-commerce application must have a fallback mechanism to serve a default, non-personalized experience, ensuring the user journey is never broken due to an ML system failure.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="overall-system-architecture">
<h3>13. Overall System Architecture<a class="headerlink" href="#overall-system-architecture" title="Permalink to this heading">¶</a></h3>
<p>This section provides the unified architectural blueprint of the end-to-end propensity scoring system. It ties together the individual data, training, and inference pipelines into a cohesive, production-grade MLOps platform, illustrating the flow of data and logic from the initial user click to the final, personalized action.</p>
<section id="a-unified-architectural-blueprint">
<h4>13.1 A Unified Architectural Blueprint<a class="headerlink" href="#a-unified-architectural-blueprint" title="Permalink to this heading">¶</a></h4>
<p>The architecture is a hybrid system that leverages managed AWS services for scalability and reliability while using best-of-breed open-source tools for core ML lifecycle management.</p>
<img src="../_static/past_experiences/ecom_propensity/overall_system_architecture_diagram.svg" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="real-time-inference-sequence-diagram">
<h4>13.2 Real-Time Inference Sequence Diagram<a class="headerlink" href="#real-time-inference-sequence-diagram" title="Permalink to this heading">¶</a></h4>
<p>This diagram details the critical path for a single prediction request, highlighting the latency budget for each step.</p>
<img src="../_static/past_experiences/ecom_propensity/inference_sequence_diagram_latency.svg" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="potential-bottlenecks-and-performance-optimizations">
<h4>13.3 Potential Bottlenecks and Performance Optimizations<a class="headerlink" href="#potential-bottlenecks-and-performance-optimizations" title="Permalink to this heading">¶</a></h4>
<p>To meet our strict latency and throughput requirements, we must be proactive about identifying and mitigating potential bottlenecks in the inference path.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Bottleneck</p></th>
<th class="head text-left"><p>Risk Level</p></th>
<th class="head text-left"><p>Mitigation &amp; Optimization Strategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Online Feature Store Latency</strong></p></td>
<td class="text-left"><p><strong>High</strong></p></td>
<td class="text-left"><p>The <code class="docutils literal notranslate"><span class="pre">get_online_features()</span></code> call is the most critical I/O operation. Our choice of <strong>Amazon ElastiCache for Redis</strong> is the primary mitigation. We must also optimize the network path between SageMaker and Redis (e.g., by placing them in the same VPC and Availability Zones). At extreme scale, introducing a local, in-memory cache (e.g., using <code class="docutils literal notranslate"><span class="pre">functools.lru_cache</span></code>) within the inference service itself could cache features for very active users for short periods (e.g., 5 seconds) to further reduce Redis lookups.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Execution Time</strong></p></td>
<td class="text-left"><p><strong>Medium</strong></p></td>
<td class="text-left"><p>Our LightGBM model is very fast, but as we add features or complexity, this can increase. We will implement <strong>model quantization</strong> (converting model weights from float32 to int8), which can provide a 2-3x speedup on CPU-based instances with minimal accuracy loss. We will also use the latest, most optimized version of the LightGBM library.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Cold Starts</strong></p></td>
<td class="text-left"><p><strong>Medium</strong></p></td>
<td class="text-left"><p>If the SageMaker endpoint scales down to zero instances during periods of no traffic, the first request will incur a “cold start” latency penalty of several seconds while the container is provisioned. To prevent this, we will configure the autoscaling policy to <strong>always keep at least one instance warm</strong>. For spiky traffic, we can also use <strong>scheduled scaling</strong> to proactively increase instance counts before anticipated peak shopping times.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Inefficient Request Handling</strong></p></td>
<td class="text-left"><p><strong>Low</strong></p></td>
<td class="text-left"><p>The Python web server (Gunicorn) must be configured correctly with an appropriate number of worker processes to fully utilize the CPU cores on the SageMaker instance. Not doing so can lead to requests queuing up and increased latency under load.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="estimated-monthly-costs">
<h4>13.4 Estimated Monthly Costs<a class="headerlink" href="#estimated-monthly-costs" title="Permalink to this heading">¶</a></h4>
<p>This is a high-level estimate assuming a mid-sized European e-commerce business. Actual costs will vary based on traffic, data volume, and specific AWS instance choices.</p>
<p><strong>Assumptions:</strong></p>
<ul class="simple">
<li><p><strong>Daily Sessions:</strong> ~250,000</p></li>
<li><p><strong>Avg. Events per Session:</strong> 10</p></li>
<li><p><strong>Total Monthly Ingestion Events:</strong> 250k sessions/day * 10 events/session * 30 days = <strong>75 Million</strong></p></li>
<li><p><strong>Total Monthly Inference Requests:</strong> Same as ingestion = <strong>75 Million</strong></p></li>
<li><p><strong>Peak Traffic Factor:</strong> 5x average traffic during peak hours.</p></li>
</ul>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Pipeline Component</p></th>
<th class="head text-left"><p>AWS Service(s)</p></th>
<th class="head text-left"><p>Detailed Cost Calculation &amp; Rationale</p></th>
<th class="head text-left"><p>Estimated Cost (USD)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Data Ingestion</strong></p></td>
<td class="text-left"><p><strong>API Gateway</strong><br><strong>Kinesis Data Streams</strong></p></td>
<td class="text-left"><p><strong>API Gateway:</strong> Priced per million requests.<br>- 75 million requests * ~$1.00/million = <strong>$75</strong><br><br><strong>Kinesis Data Streams:</strong> Priced per Shard Hour and per 1M PUT units. We need two streams (raw, processed) and at least 2 shards per stream for redundancy and peak load handling.<br>- Shard Hours: 2 streams * 2 shards/stream * 720 hours/month * ~$0.015/hour = <strong>$43.80</strong><br>- PUT Payloads: 75M requests * 2 (for both streams) * ~$0.014/million units = <strong>$2.10</strong><br>The primary cost is the infrastructure (shards) to be ready for the traffic.</p></td>
<td class="text-left"><p><strong>$120 - $180</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Feature Engineering</strong></p></td>
<td class="text-left"><p><strong>EMR (Spark)</strong><br><strong>ElastiCache (Redis)</strong></p></td>
<td class="text-left"><p><strong>EMR Cluster:</strong> Priced per instance-hour. A continuous streaming job requires a persistent cluster. We’ll assume a small 3-node cluster (1 master, 2 workers) of <code class="docutils literal notranslate"><span class="pre">m5.xlarge</span></code> instances.<br>- 3 instances * 720 hours/month * (~$0.192/hr EC2 + ~$0.048/hr EMR) = <strong>~$525</strong><br>- Add cost for daily batch jobs (e.g., 2 hours/day on 5 nodes) = <strong>~$75</strong><br><br><strong>ElastiCache for Redis:</strong> Priced per instance-hour. To hold features for active users (~2-3 GB data) with high availability, we need 2 nodes (primary + replica) of <code class="docutils literal notranslate"><span class="pre">cache.m6g.large</span></code>.<br>- 2 nodes * 720 hours/month * ~$0.266/hour = <strong>~$388</strong></p></td>
<td class="text-left"><p><strong>$1,300 - $1,800</strong></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Model Training</strong></p></td>
<td class="text-left"><p><strong>SageMaker Training</strong><br><strong>Airflow Infra</strong></p></td>
<td class="text-left"><p><strong>SageMaker Training Jobs:</strong> Priced per instance-second. Assuming we retrain 4 times per month for 2 hours each on an <code class="docutils literal notranslate"><span class="pre">ml.m5.4xlarge</span></code> instance.<br>- 4 runs/month * 2 hours/run * ~$1.038/hour = <strong>~$8.30</strong>. The training itself is very cheap.<br><br><strong>Airflow Infrastructure:</strong> The main cost is the 24/7 instance needed to run the Airflow scheduler/webserver. Assuming a managed service like MWAA or a <code class="docutils literal notranslate"><span class="pre">t3.medium</span></code> EC2 instance.<br>- 1 instance * 720 hours/month * ~$0.05/hour = <strong>~$36.50</strong></p></td>
<td class="text-left"><p><strong>$80 - $150</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Serving / Inference</strong></p></td>
<td class="text-left"><p><strong>SageMaker Endpoints</strong></p></td>
<td class="text-left"><p>Priced per instance-hour. To handle ~150 req/sec at peak with p99&lt;100ms latency, we need a baseline of 3 <code class="docutils literal notranslate"><span class="pre">ml.c5.xlarge</span></code> instances with autoscaling.<br>- 3 instances * 720 hours/month * ~$0.238/hour = <strong>~$521</strong><br><br>The cost includes a buffer for autoscaling up to 4-5 instances during peak shopping hours or marketing campaigns.</p></td>
<td class="text-left"><p><strong>$600 - $900</strong></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Storage &amp; Logging</strong></p></td>
<td class="text-left"><p><strong>S3</strong><br><strong>CloudWatch</strong></p></td>
<td class="text-left"><p><strong>S3:</strong> Priced per GB-month. Assuming data grows to 1 TB total (raw events, feature store, logs).<br>- 1024 GB * ~$0.023/GB-month = <strong>~$23.50</strong><br>- Add ~$10 for requests (PUT, GET).<br><br><strong>CloudWatch:</strong> Priced per GB of logs ingested/stored. This can be significant. Assuming ~150 GB of logs per month.<br>- 150 GB * ~$0.50/GB (Log Ingestion) = <strong>~$75</strong><br>- Add ~$25 for custom metrics and alarms.</p></td>
<td class="text-left"><p><strong>$140 - $250</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Total Estimated Monthly Cost</strong></p></td>
<td class="text-left"><p><strong>-</strong></p></td>
<td class="text-left"><p><strong>-</strong></p></td>
<td class="text-left"><p><strong>$2,200 - $3,300</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p>This detailed breakdown reveals that the vast majority of the monthly operational cost (~80%) is concentrated in the <strong>24/7 services required for real-time feature engineering and model serving (EMR, ElastiCache, and SageMaker Endpoints)</strong>. This highlights the critical importance of optimizing these components for cost-efficiency, for instance by right-sizing clusters and implementing intelligent autoscaling policies. The cost of actually training the model, by contrast, is almost negligible.</p>
</section>
<section id="deep-dive-calculating-inference-instance-requirements">
<h4>13.5 Deep Dive: Calculating Inference Instance Requirements<a class="headerlink" href="#deep-dive-calculating-inference-instance-requirements" title="Permalink to this heading">¶</a></h4>
<p>Estimating the number of instances is a bottom-up process. We start by understanding the performance of a single server and then scale that out to meet our target throughput. The fundamental goal is to answer: <strong>“How many requests per second (RPS) can a single instance handle?”</strong></p>
<section id="the-core-factors-performance-equation">
<h5>13.5.1 The Core Factors &amp; Performance Equation<a class="headerlink" href="#the-core-factors-performance-equation" title="Permalink to this heading">¶</a></h5>
<p>The throughput of a single inference server instance is governed by this relationship:</p>
<p><code class="docutils literal notranslate"><span class="pre">RPS_per_instance</span> <span class="pre">=</span> <span class="pre">(Concurrency)</span> <span class="pre">/</span> <span class="pre">(Latency_per_request)</span></code></p>
<p>Let’s break down each component:</p>
<ol class="arabic simple">
<li><p><strong>Latency per Request:</strong> The total time to process one request. This is the sum of:</p>
<ul class="simple">
<li><p><strong>I/O Wait Time:</strong> Time spent waiting for network operations. The most significant is the call to the online feature store (<code class="docutils literal notranslate"><span class="pre">~10ms</span></code> for Redis).</p></li>
<li><p><strong>Model Execution Time:</strong> The actual <code class="docutils literal notranslate"><span class="pre">model.predict()</span></code> time. For a well-optimized LightGBM model on a single data row, this is extremely fast, typically <strong>~20-30ms</strong> on a modern CPU.</p></li>
<li><p><strong>Pre/Post-processing:</strong> CPU time for data validation, feature transformations, and JSON serialization. Let’s estimate <strong>~10ms</strong>.</p></li>
<li><p><strong>Total Latency per Request ≈ 40-50ms</strong></p></li>
</ul>
</li>
<li><p><strong>Concurrency:</strong> This is the most critical factor for CPU utilization. It’s the number of requests an instance can process <em>simultaneously</em>.</p>
<ul class="simple">
<li><p><strong>Naive Approach (Synchronous):</strong> A simple server handles one request at a time. Concurrency = 1. This is incredibly inefficient, as the CPU sits idle during the ~10ms of I/O wait time for the feature store.</p></li>
<li><p><strong>Optimized Approach (Asynchronous):</strong> By using an async framework like <strong>FastAPI with Uvicorn workers</strong>, the server can handle many concurrent requests. While one request is waiting for the feature store (I/O-bound), the CPU is free to work on another request’s model execution (CPU-bound). This is the key to maximizing CPU utilization. The concurrency is limited not by waiting, but by the number of parallel workers you can run. A standard rule of thumb for Gunicorn (which manages the Uvicorn workers) is to have <code class="docutils literal notranslate"><span class="pre">(2</span> <span class="pre">*</span> <span class="pre">number_of_vCPUs)</span> <span class="pre">+</span> <span class="pre">1</span></code> workers.</p></li>
</ul>
</li>
</ol>
</section>
<section id="performance-optimization-strategies">
<h5>13.5.2 Performance Optimization Strategies<a class="headerlink" href="#performance-optimization-strategies" title="Permalink to this heading">¶</a></h5>
<p>Before calculating, we must apply optimizations to get the best performance from each instance.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Optimization</p></th>
<th class="head text-left"><p>Technique &amp; Rationale</p></th>
<th class="head text-left"><p>Impact</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>1. Asynchronous Request Handling</strong></p></td>
<td class="text-left"><p>Use <strong>FastAPI + Uvicorn + Gunicorn</strong>. This is the single most important optimization. It allows the server to handle new requests while others are waiting for I/O (like feature store lookups), preventing the CPU from being idle.</p></td>
<td class="text-left"><p><strong>Dramatically increases throughput (5x-10x)</strong> by improving concurrency and CPU utilization.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>2. Model Quantization</strong></p></td>
<td class="text-left"><p>Convert the model’s weights from 32-bit floating-point numbers to 8-bit integers (INT8).</p></td>
<td class="text-left"><p><strong>Reduces model size and speeds up CPU execution by 2-3x</strong> with a negligible impact on the accuracy of a tree-based model like LightGBM.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>3. Right-Sizing the Instance</strong></p></td>
<td class="text-left"><p>Choose a CPU-optimized instance type. For SageMaker, the <code class="docutils literal notranslate"><span class="pre">ml.c5</span></code> family (“c” for compute) is designed for this type of workload, offering a better price-to-performance ratio than general-purpose (<code class="docutils literal notranslate"><span class="pre">ml.m5</span></code>) instances.</p></td>
<td class="text-left"><p><strong>Lowers cost</strong> by providing more CPU power for the same price as a general-purpose instance.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>4. Connection Pooling</strong></p></td>
<td class="text-left"><p>Use a persistent client (like <code class="docutils literal notranslate"><span class="pre">redis-py</span></code>’s connection pool) to communicate with the feature store.</p></td>
<td class="text-left"><p><strong>Reduces latency</strong> by avoiding the overhead of establishing a new TCP connection to Redis for every single prediction request.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="bottom-up-calculation-throughput-of-a-single-instance">
<h5>13.5.3 Bottom-Up Calculation: Throughput of a Single Instance<a class="headerlink" href="#bottom-up-calculation-throughput-of-a-single-instance" title="Permalink to this heading">¶</a></h5>
<p>Let’s apply these principles to calculate the realistic throughput of one <code class="docutils literal notranslate"><span class="pre">ml.c5.xlarge</span></code> instance.</p>
<ul class="simple">
<li><p><strong>Instance Type:</strong> <code class="docutils literal notranslate"><span class="pre">ml.c5.xlarge</span></code></p></li>
<li><p><strong>vCPUs:</strong> 4</p></li>
<li><p><strong>Gunicorn Workers:</strong> <code class="docutils literal notranslate"><span class="pre">(2</span> <span class="pre">*</span> <span class="pre">4)</span> <span class="pre">+</span> <span class="pre">1</span> <span class="pre">=</span> <span class="pre">9</span></code> workers. This means our server can handle 9 requests in parallel.</p></li>
</ul>
<p><strong>Step 1: Calculate Optimized Latency per Request</strong></p>
<ul class="simple">
<li><p>Original Model Latency: 25ms</p></li>
<li><p>Latency after Quantization (2.5x speedup): <code class="docutils literal notranslate"><span class="pre">25ms</span> <span class="pre">/</span> <span class="pre">2.5</span> <span class="pre">=</span> <span class="pre">10ms</span></code></p></li>
<li><p>Total Latency = I/O Wait (10ms) + Optimized Model Execution (10ms) + Processing (10ms) = <strong>30ms</strong></p></li>
</ul>
<p><strong>Step 2: Calculate Max Theoretical RPS per Worker</strong></p>
<ul class="simple">
<li><p>A single worker, if always busy, can process <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">request</span> <span class="pre">/</span> <span class="pre">0.030</span> <span class="pre">seconds</span> <span class="pre">=</span> <span class="pre">~33</span> <span class="pre">RPS</span></code>.</p></li>
</ul>
<p><strong>Step 3: Calculate Max Theoretical RPS per Instance</strong></p>
<ul class="simple">
<li><p>9 workers * 33 RPS/worker = <strong>~300 RPS</strong></p></li>
</ul>
<p><strong>Step 4: Apply a Realistic Utilization Target</strong></p>
<ul class="simple">
<li><p>No system can run at 100% theoretical capacity without latency increasing dramatically due to queuing effects. A safe and standard target for production systems is <strong>70-80% utilization</strong>.</p></li>
<li><p>Realistic Sustainable Throughput = <code class="docutils literal notranslate"><span class="pre">300</span> <span class="pre">RPS</span> <span class="pre">*</span> <span class="pre">0.7</span> <span class="pre">=</span> <span class="pre">**210</span> <span class="pre">RPS</span> <span class="pre">per</span> <span class="pre">instance**</span></code>.</p></li>
</ul>
<p>This number is our key metric for scaling.</p>
</section>
<section id="scaling-analysis-instances-and-costs-by-request-volume">
<h5>13.5.4 Scaling Analysis: Instances and Costs by Request Volume<a class="headerlink" href="#scaling-analysis-instances-and-costs-by-request-volume" title="Permalink to this heading">¶</a></h5>
<p>Now we can create the scaling table. We will assume a baseline of <strong>2 instances</strong> for High Availability (HA) – you never want a single point of failure in production.</p>
<ul class="simple">
<li><p><strong>Cost of one <code class="docutils literal notranslate"><span class="pre">ml.c5.xlarge</span></code> instance:</strong> ~$0.238/hour * 720 hours/month = <strong>~$174 per month</strong></p></li>
</ul>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Target Throughput</p></th>
<th class="head text-left"><p>Calculation</p></th>
<th class="head text-left"><p>Minimum Instances Needed</p></th>
<th class="head text-left"><p>Recommended Instances (for HA &amp; Peak Buffer)</p></th>
<th class="head text-left"><p>Estimated Monthly Cost</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>10 RPS</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">10</span> <span class="pre">/</span> <span class="pre">210</span> <span class="pre">=</span> <span class="pre">0.05</span></code></p></td>
<td class="text-left"><p>1</p></td>
<td class="text-left"><p><strong>2</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">$174</span> <span class="pre">=</span></code> <strong>$348</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>100 RPS</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">100</span> <span class="pre">/</span> <span class="pre">210</span> <span class="pre">=</span> <span class="pre">0.48</span></code></p></td>
<td class="text-left"><p>1</p></td>
<td class="text-left"><p><strong>2</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">$174</span> <span class="pre">=</span></code> <strong>$348</strong></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>1,000 RPS</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">1,000</span> <span class="pre">/</span> <span class="pre">210</span> <span class="pre">=</span> <span class="pre">4.76</span></code></p></td>
<td class="text-left"><p>5</p></td>
<td class="text-left"><p><strong>6</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">6</span> <span class="pre">*</span> <span class="pre">$174</span> <span class="pre">=</span></code> <strong>$1,044</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>10,000 RPS</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">10,000</span> <span class="pre">/</span> <span class="pre">210</span> <span class="pre">=</span> <span class="pre">47.6</span></code></p></td>
<td class="text-left"><p>48</p></td>
<td class="text-left"><p><strong>50</strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">50</span> <span class="pre">*</span> <span class="pre">$174</span> <span class="pre">=</span></code> <strong>$8,700</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Key Insights from the Analysis:</strong></p>
<ul class="simple">
<li><p><strong>Non-Linear Cost at the Start:</strong> The cost is flat for low throughputs because the primary driver is the business requirement for High Availability (minimum of 2 instances), not the load itself.</p></li>
<li><p><strong>Linear Scaling:</strong> Once the load surpasses the capacity of the HA baseline, the cost scales linearly with the number of requests.</p></li>
<li><p><strong>Optimization is Key:</strong> Without the async server and quantization optimizations, our RPS per instance would be much lower (perhaps 20-30 RPS), and the costs would be <strong>7-10 times higher</strong> for the same throughput. This demonstrates that performance optimization is a direct and powerful cost-reduction strategy.</p></li>
<li><p><strong>Validation is Mandatory:</strong> This entire calculation is a robust <em>estimate</em>. It must be validated with a real-world load test (using a tool like Locust) against a staging endpoint to confirm the actual sustainable RPS before deploying to production.</p></li>
</ul>
<hr class="docutils" />
<!--
### **14. Challenges Faced and Lessons Learned: From the Trenches**

Designing a system on paper is one thing; operating it in the real world under the pressures of live traffic, evolving data, and unexpected failures is another. This section moves from theory to practice, presenting a series of detailed post-mortems of production incidents.

---

### **14.1 Challenge 1: The Silent Killer – Training-Serving Skew**

*   **The Scenario:** A new, more powerful Gradient Boosting model (v2) for purchase intent is ready. Offline evaluation shows a 10% improvement in AUC over the existing production model (v1). The team executes a Blue-Green deployment. The new model is deployed to the "green" environment, and after passing health checks, 100% of traffic is switched over. For the first 24 hours, operational dashboards look perfect: latency is low, error rates are zero. However, over the next two weeks, the business analytics team raises an alarm: overall site conversion has mysteriously dropped by 5%.
*   **Discovery (Week 1-2):** The incident was discovered not by an engineering alert, but by a lagging business metric—a classic sign of a "silent failure" in an ML system. The initial investigation was fraught with confusion. The ML team was confident in their model's offline performance, and the operational dashboards showed no anomalies, creating a false sense of security. The lack of clear ownership for the end-to-end business metric slowed the initial response.
*   **Investigation (Week 3):** A cross-functional "war room" was assembled. A data scientist noticed a strange pattern: users arriving from the `social_organic` marketing channel were consistently receiving near-zero propensity scores, despite this segment being known for high engagement. This was the first concrete clue. The team traced the data flow and discovered a subtle but critical discrepancy. The training pipeline, written in Python/Pandas, one-hot encoded `traffic_source='social_organic'` to index `5`. However, the real-time feature generation service, written in Java for low latency, used a different library that mapped the same category to index `15` due to a difference in alphabetical sorting. The model, having never seen index 15 during training, treated it as an "unknown" category and assigned it a default low weight. This was a textbook example of **training-serving skew**. The model was behaving perfectly, but it was being fed corrupted data.
*   **Resolution (Week 4):**
    1.  **Immediate Mitigation:** Traffic was immediately rolled back to the "blue" environment running the v1 model. Conversion rates began to recover within hours.
    2.  **The Systemic Solution:** The post-mortem identified the root cause not as a bug, but as a flawed architectural pattern—duplicated feature logic. This incident catalyzed a strategic shift to adopt a **Feature Store**. All feature transformation logic was centralized into this single, authoritative platform. Both the batch training pipeline and the real-time inference service would now fetch features from this one source, completely eliminating this class of training-serving skew bugs.
*   **Lessons Learned / Retrospective:**
    *   **If you were to start this project from the beginning, what would you change?** "We would have architected the system around a Feature Store from day one. We mistakenly treated feature engineering as a disposable part of the model training script. We now understand that **features are durable assets that must be managed as first-class citizens** in a centralized, language-agnostic platform."
    *   **What were some of the production errors that made you regret your decisions?** "Our biggest regret was our testing strategy. We had excellent unit tests and data validation for the training data, but we lacked true **end-to-end integration tests** that validated the entire pipeline from live data source to prediction. We trusted our offline metrics, which gave us a dangerous false sense of security. The model was 'correct,' but the system was wrong."
    *   **What lessons will you take forward from this project?** "A model's health is the health of the entire data pipeline that feeds it. You must **monitor the inputs as rigorously as you monitor the outputs**, including data quality and distribution checks at inference time."

---

### **14.2 Challenge 2: The Holiday Traffic Meltdown – A Tale of Latency and Throttling**

*   **The Scenario:** It's Black Friday. The team has load-tested the personalization service and configured autoscaling. As peak traffic hits, PagerDuty alerts fire: P99 latency on the personalization API has spiked from 80ms to over 1,500ms. Upstream services begin to time out, site circuit breakers trip, and personalization features are disabled, leading to a significant loss of revenue during the most critical sales period.
*   **Discovery (Live Incident):** The on-call engineer confirmed the high latency and corresponding server-side errors. The first hypothesis—that the model inference pods were the bottleneck—was quickly invalidated. The Kubernetes dashboard showed the pods had scaled correctly and CPU was high but not saturated. The problem had to be a dependency.
*   **Investigation (During Incident):** The engineer shifted focus to the online feature store, a managed DynamoDB table. The dashboards revealed the smoking gun: a high number of **ProvisionedThroughputExceeded** exceptions. The table was being throttled. The team had provisioned capacity based on average traffic but failed to anticipate the "hot key" problem. A small number of highly active "deal hunters" were refreshing pages at an extreme rate, overwhelming the throughput of a few specific database partitions while the overall table capacity remained underutilized. The database's autoscaling couldn't react fast enough to this sudden, concentrated load.
*   **Resolution (During and After Incident):**
    1.  **Immediate Mitigation:** The team manually overrode the DynamoDB table's capacity settings to the maximum and enabled on-demand mode. Within minutes, throttling stopped and latency returned to normal.
    2.  **Long-Term Solution:** The architecture was redesigned for resilience. A **low-latency, in-memory caching layer (using Redis)** was introduced between the inference service and the feature store. Now, a user's feature vector is fetched from DynamoDB once and then cached in Redis for 60 seconds. Subsequent requests for the same user are served from the ultra-fast cache, dramatically reducing the read load on the backend database and making the system resilient to "hot key" issues.
*   **Lessons Learned / Retrospective:**
    *   **If you were to start this project from the beginning, what would you change?** "We would have invested significantly more time in sophisticated load testing. Our tests were too simplistic; they confirmed average load but didn't push the system with realistic, adversarial traffic patterns like 'thundering herds' or highly skewed workloads."
    *   **What were some of the production errors that made you regret your decisions?** "Our biggest regret was treating a managed service like a magic black box. We assumed 'autoscaling' would handle anything. We didn't dig deep enough to understand its specific limitations, like partition-level throughput caps. You are always responsible for the failure modes of your dependencies."
    *   **What lessons will you take forward from this project?** "**Caching is not just a performance optimization; it is a critical architectural pattern for reliability.** It decouples services and prevents cascading failures. In a high-throughput system, your dependencies are your biggest liability."

---

### **14.3 Challenge 3: The "Smart" Model Gets Dumb – A Case of Severe Concept Drift**

*   **The Scenario:** The company partners with an influencer, driving a massive influx of new, young, mobile-first users. Over the next month, the marketing team reports that engagement with personalized offers is plummeting for this new user cohort. The ML monitoring system, which tracks univariate feature drift, shows no major alerts.
*   **Discovery (Month 1):** The problem was subtle. The model's overall accuracy had not dropped significantly because the existing user base was converting as expected. The prediction distribution dashboard, however, showed a clear shift: a new, large peak had formed at the low end of the propensity score range. The model was confidently predicting that almost all new users had zero intent to purchase.
*   **Investigation (Month 2):** The team discovered that the new users had characteristics (no purchase history, short sessions) that the model had correctly learned were strong negative predictors. The model was applying these patterns correctly, but the *meaning* of the patterns had changed. For the established base, "no history" meant an unengaged user. For this new cohort, it simply meant they were a brand new, high-potential user. This was a severe case of **concept drift**, where the statistical relationship between the features and the outcome had fundamentally changed for a new population segment. The univariate drift detectors missed it because the problem was in the *interaction* of the features.
*   **Resolution (Month 3):**
    1.  **Immediate Mitigation:** A rule-based override was implemented to show all new users from this channel a generic "Welcome" offer, temporarily bypassing the ML model for this segment.
    2.  **The Systemic Solution:** The team realized their "one model to rule them all" strategy was too rigid. They accelerated the development of a **Multi-Armed Bandit (MAB) framework** for serving personalizations. The MAB system allows them to deploy multiple "arms"—like the old model, a newly retrained model, and various heuristic offers—simultaneously. The MAB then dynamically learns which arm works best for different user segments and allocates traffic accordingly, creating a robust, adaptive system for handling future concept drifts.
*   **Lessons Learned / Retrospective:**
    *   **If you were to start this project from the beginning, what would you change?** "We would have built our monitoring strategy around **segment-level performance** from the start. We were monitoring global averages and univariate feature distributions, which completely masked the problem. You have to monitor for problems within your most important slices of data."
    *   **What were some of the production errors that made you regret your decisions?** "Our biggest regret was our monolithic model strategy. We assumed a single, complex model would be optimal for all users. When the fundamental context of our user base changed, our rigid system couldn't adapt. We were flying a plane and had no way to steer when the weather changed."
    *   **What lessons will you take forward from this project?** "A machine learning model is a static snapshot of a past reality, but business is a dynamic, living entity. **The MLOps platform must be designed for continuous experimentation and rapid adaptation**, not just for serving a single model. Moving from periodic A/B testing to continuous optimization with frameworks like Multi-Armed Bandits is a critical step in maturing a personalization system."

### **14.4 Challenge 4: The Profit-Eating Model – A Failure of Calibration**

*   **The Scenario:** A new model (v3) is developed, showing a slight but statistically significant improvement in AUC (0.90 vs 0.89) over the production model (v2). It passes all offline tests and is deployed via a 50/50 A/B test. The results after two weeks are baffling and create significant friction. The A/B test dashboard shows the v3 model has a **2% higher conversion rate**, a clear win. However, the Finance department and Marketing team are raising alarms: the **Profit-Per-User for the v3 cohort has dropped by 4%**. The model is successfully encouraging more people to buy, but it's doing so unprofitably.
*   **Discovery (Week 1-2):** This was an incredibly confusing period. The ML team celebrated the conversion lift as a victory. The Marketing team, which owns the discount budget, saw their return on investment plummeting. The core business logic was simple: "if a user's propensity score > 0.9, show them a 15% discount offer to close the sale." The v3 model was simply assigning more users a score above 0.9. The initial hypothesis was a data bug or a problem with the A/B testing instrumentation. The analytics team spent a week validating the data streams; everything was correct. The tension between the teams grew, as the ML metric (conversion) and the business metric (profit) were telling opposite stories.
*   **Investigation (Week 3):** A lead data scientist on the team proposed a different hypothesis: the problem wasn't the model's *ranking* ability (which is what AUC measures), but the *meaning* of its raw prediction scores. The team decided to analyze the model's **calibration**. They plotted a reliability diagram, binning all the predictions from the v3 model (e.g., 0.8-0.85, 0.85-0.9, etc.) and plotting the predicted probability against the *actual* conversion rate for each bin. The discovery was stark: the v3 model was systematically **overconfident**. For the bin of users where the model predicted a ~92% purchase probability, their true, observed conversion rate was only ~75%. The model was great at identifying users who were *more likely* to convert, but its probability estimates were not statistically sound. It was giving the 15% discount to many users who were "on the fence" but not nearly as certain to buy as the model claimed, leading to wasted marketing spend.
*   **Resolution (Week 4):**
    1.  **Immediate Mitigation:** A post-processing fix was implemented. The team used the A/B test data to train a simple **Isotonic Regression** model. This lightweight model takes the raw, uncalibrated output of the main v3 model and transforms it into a properly calibrated probability. This calibration layer was deployed as part of the inference service, and profit metrics for the v3 cohort immediately began to recover.
    2.  **The Systemic Solution:** The incident led to a fundamental change in both the modeling and business logic:
        *   **Modeling:** The training pipeline was updated to make calibration a first-class citizen. **Calibration plots** and the **Brier Score** were added as mandatory evaluation metrics alongside AUC. Any new model candidate now had to be both accurate at ranking and well-calibrated.
        *   **Business Logic:** The simple `score > 0.9` threshold was replaced with a more sophisticated **Expected Value calculation**. The system now uses the calibrated probability to calculate: `Expected_Value = (P_convert * Avg_Cart_Value) - ((P_convert - P_convert_no_discount) * Discount_Amount)`. This allowed the business to make a much more nuanced, profit-aware decision, sometimes choosing *not* to offer a discount even to a high-propensity user if the expected ROI was negative.
*   **Lessons Learned / Retrospective:**
    *   **If you were to start this project from the beginning, what would you change?** "We would have insisted on a deeper understanding of the business action tied to the model's prediction. We optimized for a technical ML metric (AUC) without fully appreciating its connection to the financial outcome. We now know you must **co-design the model's output and the business logic that consumes it.**"
    *   **What were some of the production errors that made you regret your decisions?** "Our biggest regret was equating a higher offline metric with guaranteed business value. The pursuit of that extra 1% of AUC led us to a model that was technically 'better' but practically worse. It was a classic case of **'the operation was a success, but the patient died.'**"
    *   **What lessons will you take forward from this project?** "A model's raw output is not the same as a business-ready probability. For any model whose output is used to trigger financial decisions, **calibration is not optional**. It must be measured, monitored, and corrected as a standard part of the MLOps lifecycle."

---

### **14.5 Challenge 5: The Time-Traveling User – A Time Zone Bug in Batch Features**

*   **The Scenario:** The system had been running stably for months. A newly retrained model (v4) is deployed. Post-deployment monitoring shows no issues. However, a week later, a support ticket is escalated from the EU marketing team. A small but valuable group of long-time loyal customers have complained that their personalized recommendations feel "stale" and "irrelevant."
*   **Discovery (Week 1):** This was an extremely difficult bug to reproduce as it only affected a tiny subset of users. Initial checks of the inference pipeline and online feature store for these specific users showed no errors. Their real-time features were being generated correctly. The ML team was stumped, as the model was behaving as expected given the features it was receiving.
*   **Investigation (Week 2-3):** A dedicated data engineer was assigned to perform a deep dive on the full feature vector for one of the complaining users. The engineer decided to manually reconstruct the feature vector by querying both the online store (for real-time features) and the offline store (for historical batch features). This is where the discrepancy was found. The user's real-time feature `session_count` was `1`, but their historical feature `days_since_last_purchase` was `365`, even though the user had made a purchase just the day before. The model was correctly seeing this user as a "new visitor" (`session_count=1`) but also as a "long-lapsed customer" (`days_since_last_purchase=365`), a contradictory and nonsensical combination that led to poor predictions.
    The root cause was traced to the **daily batch feature engineering job in Airflow**. The job, running on a server configured in UTC, was designed to process the previous day's data. It read raw event timestamps, which were stored in UTC. However, a recent "optimization" to the query was using a `CURRENT_DATE - 1` filter to select the data partition. This seemingly innocent change introduced a critical **time zone bug**. For a user in California (PST, UTC-7) who made a purchase at 8 PM on a Tuesday, the event was logged with a Wednesday UTC timestamp. The batch job running on Wednesday night would see this purchase. However, if they visited again on Wednesday, the batch job hadn't run yet, and their `days_since_last_purchase` feature was still based on year-old data. The feature was effectively off by a full day for any user in a western hemisphere time zone.
*   **Resolution (Week 4):**
    1.  **Immediate Mitigation:** A hotfix was deployed to the Airflow batch job, replacing the date-based partition logic with a more robust timestamp-based window that correctly handled the time zone differences. The team then manually ran a backfill operation to recompute the historical features for all users for the past month.
    2.  **The Systemic Solution:** This incident exposed a major flaw in their testing and data validation strategy. The solution was twofold:
        *   **Feature-level Unit Testing:** The team developed a new class of unit tests for their feature engineering code. These tests now explicitly create synthetic user data with events spanning different time zones and daylight-saving boundaries, asserting that features like `days_since_last_purchase` are calculated correctly in all cases.
        *   **Staging Environment Parity:** A more rigorous policy was enforced that the staging data environment must contain a representative sample of users from all major global time zones. The full end-to-end integration test was updated to include checks for a "synthetic time-traveler" user to catch these bugs before they hit production.
*   **Lessons Learned / Retrospective:**
    *   **If you were to start this project from the beginning, what would you change?** "We would have established a 'zero-trust' policy for time. **Every single timestamp operation, from data partitioning to feature calculation, should have been explicitly time-zone-aware from day one.** We assumed UTC would simplify things, but it just hid the complexity."
    *   **What were some of the production errors that made you regret your decisions?** "Our biggest regret was that our unit tests for feature logic were too simplistic. They tested the transformation functions with clean, perfect data. They didn't test for the messy, real-world edge cases like time zone conversions. This bug sat dormant in our codebase for months before a specific user behavior pattern triggered it."
    *   **What lessons will you take forward from this project?** "Historical features from a batch pipeline are just as critical to the final prediction as real-time features. A bug in a pipeline that runs once a day can be far harder to detect and debug than a bug in a real-time service. **Your monitoring and testing strategy must cover your slowest-moving data pipelines just as rigorously as your fastest ones.**"

---

### **14.6 Challenge 6: The Echo Chamber – A Degenerate Feedback Loop**

*   **The Scenario:** After a year of successful operation, the system is performing well. The models are regularly retrained, and A/B tests consistently show positive lift. However, a product manager raises a strategic concern: while the system is excellent at converting users who show clear intent within a category (e.g., electronics), the cross-category recommendation performance is poor. The business is failing to turn "TV buyers" into "kitchen appliance buyers."
*   **Discovery (Month 1):** This wasn't a bug, but a slow, creeping strategic failure. The ML team began by analyzing the feature importance scores from the last several versions of the model. They noticed a clear trend: the importance of features related to a user's *current* session category was increasing over time, while the importance of historical, cross-category features was diminishing.
*   **Investigation (Month 2):** The team realized they had inadvertently created a **degenerate feedback loop**. The system worked like this:
    1.  A user shows interest in TVs.
    2.  The model correctly predicts a high propensity to buy electronics and gives them a high score.
    3.  The personalization engine shows them more TVs and related electronics.
    4.  The user clicks on these, generating more electronics-related clickstream data.
    5.  At retraining time, this new data reinforces the model's belief that this user *only* cares about electronics.
    The model's own success was creating a "filter bubble" around the user. It became exceptionally good at optimizing for the user's known, local preferences at the expense of ever learning about their potential for exploration. The model was exploiting, but it had forgotten how to explore.
*   **Resolution (Month 3 and beyond):**
    1.  **Immediate Mitigation:** The team worked with the marketing group to inject some controlled randomness. 5% of the personalization slots were now reserved for a "discovery" module that recommended popular items from outside the user's most-browsed category.
    2.  **The Systemic Solution:** This challenge led to a fundamental rethinking of the business objective. The goal wasn't just to maximize next-click conversion, but to maximize **long-term customer lifetime value**. This required a shift from pure supervised learning to a framework that balances exploitation and exploration. The team began a long-term project to replace the simple A/B testing framework with a **Contextual Multi-Armed Bandit (MAB)** system.
        *   The MAB framework treats each personalization strategy (the v5 model, the v6 model, a "cross-sell" heuristic, a "discovery" heuristic) as an "arm."
        *   When a new user arrives, the MAB uses their context (features) to predict which arm is most likely to yield a reward (a purchase).
        *   Crucially, it dedicates a small portion of traffic to "exploring"—trying out less-certain arms to continuously gather data on their performance. This ensures the system never becomes fully entrenched in its own beliefs and can adapt to changes in user taste or discover new cross-sell opportunities.
*   **Lessons Learned / Retrospective:**
    *   **If you were to start this project from the beginning, what would you change?** "We would have framed the problem not as 'predict purchase intent,' but as '**what is the optimal action to show this user right now to maximize long-term value?**' This subtle reframing forces you to think about exploration and the cost of being wrong from day one, leading you naturally toward solutions like bandits."
    *   **What were some of the production errors that made you regret your decisions?** "Our biggest regret was being too successful with our simple A/B testing. Each positive result reinforced our belief in the current approach, making us blind to the long-term, systemic bias we were introducing. We were optimizing a local maximum while ignoring a potentially much larger global one."
    *   **What lessons will you take forward from this project?** "A personalization system that only exploits known preferences will eventually cannibalize its own future. **You must build exploration directly into the DNA of the production system.** This is the only way to ensure the data flywheel doesn't become a feedback loop that just spins in place. The MLOps platform must support not just serving predictions, but managing a portfolio of competing experiments in real-time."


-->
</section>
</section>
</section>
</section>
<hr class="docutils" />
<section id="code-implementation">
<h2>Code Implementation<a class="headerlink" href="#code-implementation" title="Permalink to this heading">¶</a></h2>
<section id="data-ingestion-pipeline">
<h3>Data Ingestion Pipeline<a class="headerlink" href="#data-ingestion-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="architecture">
<h4>Architecture<a class="headerlink" href="#architecture" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_propensity/data_ingestion_pipeline.svg" width="100%" style="background-color: #FCF1EF;"/>
</section>
</section>
<hr class="docutils" />
<section id="feature-engineering-batch">
<h3>Feature Engineering: Batch<a class="headerlink" href="#feature-engineering-batch" title="Permalink to this heading">¶</a></h3>
<section id="id3">
<h4>Architecture<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_propensity/feature_batch.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="iac-terraform">
<h4>IaC (Terraform)<a class="headerlink" href="#iac-terraform" title="Permalink to this heading">¶</a></h4>
<div class="highlight-hcl notranslate"><div class="highlight"><pre><span></span><span class="c1"># ------------------------------------------------------------------</span>
<span class="c1"># ROLES AND PROFILES FOR ALL EMR CLUSTERS (Batch &amp; Streaming)</span>
<span class="c1"># These foundational components are defined once and used by both provisioning methods.</span>
<span class="c1"># ------------------------------------------------------------------</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role&quot;</span><span class="w"> </span><span class="nv">&quot;emr_service_role&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;emr_service_role&quot;</span>
<span class="w">  </span><span class="na">assume_role_policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">jsonencode</span><span class="p">({</span>
<span class="w">    </span><span class="na">Version</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">Statement</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[{</span><span class="w"> </span><span class="na">Action</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;sts:AssumeRole&quot;, Effect = &quot;Allow&quot;, Principal = { Service = &quot;elasticmapreduce.amazonaws.com&quot;</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">}],</span>
<span class="w">  </span><span class="p">})</span>
<span class="p">}</span>
<span class="c1"># Attach AWS managed policy for EMR service role</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role_policy_attachment&quot;</span><span class="w"> </span><span class="nv">&quot;emr_service_policy_attach&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">role</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.emr_service_role.name</span>
<span class="w">  </span><span class="na">policy_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole&quot;</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role&quot;</span><span class="w"> </span><span class="nv">&quot;emr_ec2_role&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;emr_ec2_instance_role&quot;</span>
<span class="w">  </span><span class="na">assume_role_policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">jsonencode</span><span class="p">({</span>
<span class="w">    </span><span class="na">Version</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">Statement</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[{</span><span class="w"> </span><span class="na">Action</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;sts:AssumeRole&quot;, Effect = &quot;Allow&quot;, Principal = { Service = &quot;ec2.amazonaws.com&quot;</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">}],</span>
<span class="w">  </span><span class="p">})</span>
<span class="p">}</span>
<span class="c1"># Attach AWS managed policy for EMR EC2 role</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role_policy_attachment&quot;</span><span class="w"> </span><span class="nv">&quot;emr_ec2_policy_attach&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">role</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.emr_ec2_role.name</span>
<span class="w">  </span><span class="na">policy_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role&quot;</span>
<span class="p">}</span>

<span class="c1"># This instance profile is used by the EC2 instances in BOTH clusters</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_instance_profile&quot;</span><span class="w"> </span><span class="nv">&quot;emr_instance_profile&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;emr_ec2_instance_profile&quot;</span>
<span class="w">  </span><span class="na">role</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.emr_ec2_role.name</span>
<span class="p">}</span>

<span class="c1"># ------------------------------------------------------------------</span>
<span class="c1"># PERSISTENT STREAMING CLUSTER (Managed by Terraform)</span>
<span class="c1"># This is the long-running cluster for our 24/7 real-time feature pipeline.</span>
<span class="c1"># ------------------------------------------------------------------</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_emr_cluster&quot;</span><span class="w"> </span><span class="nv">&quot;streaming_cluster&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;ecom-propensity-streaming-cluster&quot;</span>
<span class="w">  </span><span class="na">release_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;emr-6.9.0&quot;</span>
<span class="w">  </span><span class="na">applications</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;Spark&quot;</span><span class="p">]</span>

<span class="c1">  # This cluster is kept alive</span>
<span class="w">  </span><span class="na">keep_job_flow_alive_when_no_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">true</span>
<span class="w">  </span><span class="na">termination_protection</span><span class="w">            </span><span class="o">=</span><span class="w"> </span><span class="no">true</span>

<span class="w">  </span><span class="nb">ec2_attributes</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">instance_profile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_instance_profile.emr_instance_profile.arn</span>
<span class="c1">    # Additional networking configurations (subnet_id, security_groups) go here</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="nb">master_instance_group</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">instance_type</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;m5.xlarge&quot;</span>
<span class="w">    </span><span class="na">instance_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="nb">core_instance_group</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">instance_type</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;m5.xlarge&quot;</span>
<span class="w">    </span><span class="na">instance_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="c1"> # Start with 2 and autoscale</span>
<span class="c1">    # Autoscaling configurations would be defined here</span>
<span class="w">  </span><span class="p">}</span>

<span class="c1">  # The service and job flow roles defined above are used here</span>
<span class="w">  </span><span class="na">service_role</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.emr_service_role.arn</span>

<span class="w">  </span><span class="nb">tags</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">Project</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;EcomPropensity&quot;</span>
<span class="w">    </span><span class="na">Type</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;PersistentStreaming&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1"># Note: The Airflow DAG does NOT reference this resource directly. </span>
<span class="c1"># It creates its own separate, transient cluster but uses the same IAM roles and profiles.</span>
</pre></div>
</div>
</section>
<section id="python-scripts">
<h4>Python Scripts<a class="headerlink" href="#python-scripts" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># /feature_repo/user_features.py</span>
<span class="kn">from</span> <span class="nn">google.protobuf.duration_pb2</span> <span class="kn">import</span> <span class="n">Duration</span>
<span class="kn">from</span> <span class="nn">feast</span> <span class="kn">import</span> <span class="n">Entity</span><span class="p">,</span> <span class="n">FeatureView</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">FileSource</span>
<span class="kn">from</span> <span class="nn">feast.types</span> <span class="kn">import</span> <span class="n">Float32</span><span class="p">,</span> <span class="n">Int64</span><span class="p">,</span> <span class="n">String</span>

<span class="c1"># Define the user entity</span>
<span class="n">user</span> <span class="o">=</span> <span class="n">Entity</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;user_id&quot;</span><span class="p">,</span> <span class="n">join_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;user_id&quot;</span><span class="p">])</span>

<span class="c1"># Define the source of our batch features</span>
<span class="c1"># This points to the S3 &quot;Gold&quot; bucket where our Spark job will write its output</span>
<span class="n">batch_feature_source</span> <span class="o">=</span> <span class="n">FileSource</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;s3://ecom-propensity-gold-features/user_features/&quot;</span><span class="p">,</span>
    <span class="n">event_timestamp_column</span><span class="o">=</span><span class="s2">&quot;event_timestamp&quot;</span><span class="p">,</span>
    <span class="n">created_timestamp_column</span><span class="o">=</span><span class="s2">&quot;created_timestamp&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define the Feature View for user-level historical features</span>
<span class="n">user_features_view</span> <span class="o">=</span> <span class="n">FeatureView</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;user_historical_features&quot;</span><span class="p">,</span>
    <span class="n">entities</span><span class="o">=</span><span class="p">[</span><span class="n">user</span><span class="p">],</span>
    <span class="n">ttl</span><span class="o">=</span><span class="n">Duration</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">86400</span> <span class="o">*</span> <span class="mi">90</span><span class="p">),</span>  <span class="c1"># 90 days</span>
    <span class="n">schema</span><span class="o">=</span><span class="p">[</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;lifetime_purchase_count&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Int64</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;avg_order_value_90d&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Float32</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;days_since_last_purchase&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Int64</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;preferred_product_category&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">String</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">online</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">source</span><span class="o">=</span><span class="n">batch_feature_source</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;owner&quot;</span><span class="p">:</span> <span class="s2">&quot;ml_team&quot;</span><span class="p">,</span> <span class="s2">&quot;pipeline&quot;</span><span class="p">:</span> <span class="s2">&quot;batch&quot;</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">avg</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">datediff</span><span class="p">,</span> <span class="n">lit</span><span class="p">,</span> <span class="n">window</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span>

<span class="c1"># Setup logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> - </span><span class="si">%(levelname)s</span><span class="s1"> - </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_user_features</span><span class="p">(</span><span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">silver_df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes historical features for each user.&quot;&quot;&quot;</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Computing historical user features...&quot;</span><span class="p">)</span>
    
    <span class="c1"># Ensure all timestamps are handled correctly</span>
    <span class="n">purchase_events</span> <span class="o">=</span> <span class="n">silver_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;event_type&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;purchase&quot;</span><span class="p">)</span>
    
    <span class="c1"># Lifetime purchase count</span>
    <span class="n">lifetime_purchases</span> <span class="o">=</span> <span class="n">purchase_events</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;user_id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span>
        <span class="n">count</span><span class="p">(</span><span class="s2">&quot;event_id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;lifetime_purchase_count&quot;</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Average order value over the last 90 days</span>
    <span class="n">ninety_days_ago</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span> <span class="o">-</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">aov_90d</span> <span class="o">=</span> <span class="n">purchase_events</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;event_timestamp&quot;</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">lit</span><span class="p">(</span><span class="n">ninety_days_ago</span><span class="p">))</span>\
        <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;user_id&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">avg</span><span class="p">(</span><span class="s2">&quot;price&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;avg_order_value_90d&quot;</span><span class="p">))</span>

    <span class="c1"># Days since last purchase</span>
    <span class="n">days_since_purchase</span> <span class="o">=</span> <span class="n">purchase_events</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;user_id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span>
        <span class="nb">max</span><span class="p">(</span><span class="s2">&quot;event_timestamp&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;last_purchase_ts&quot;</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span>
        <span class="s2">&quot;days_since_last_purchase&quot;</span><span class="p">,</span>
        <span class="n">datediff</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()),</span> <span class="n">col</span><span class="p">(</span><span class="s2">&quot;last_purchase_ts&quot;</span><span class="p">))</span>
    <span class="p">)</span>

    <span class="c1"># Join all features together</span>
    <span class="n">features_df</span> <span class="o">=</span> <span class="n">lifetime_purchases</span>\
        <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">aov_90d</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">,</span> <span class="s2">&quot;left&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">days_since_purchase</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;user_id&quot;</span><span class="p">,</span> <span class="s2">&quot;days_since_last_purchase&quot;</span><span class="p">),</span> <span class="s2">&quot;user_id&quot;</span><span class="p">,</span> <span class="s2">&quot;left&quot;</span><span class="p">)</span>

    <span class="c1"># Add other features like preferred_product_category</span>
    <span class="c1"># ... (logic elided for brevity) ...</span>

    <span class="c1"># Add timestamp columns required by Feast</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">features_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;event_timestamp&quot;</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()))</span>\
                           <span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;created_timestamp&quot;</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()))</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Finished computing user features.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">final_df</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># This block runs when the script is submitted to Spark</span>
    
    <span class="c1"># Get S3 paths from arguments passed by Airflow</span>
    <span class="c1"># For example: --silver-path s3://... --output-path s3://...</span>
    
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;BatchFeatureEngineering&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    
    <span class="c1"># silver_df = spark.read.parquet(silver_path)</span>
    <span class="c1"># features_df = compute_user_features(spark, silver_df)</span>
    <span class="c1"># features_df.write.mode(&quot;overwrite&quot;).parquet(output_path)</span>
    
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="unit-tests">
<h4>Unit Tests<a class="headerlink" href="#unit-tests" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="c1"># Add src to path to allow imports</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;src/feature_engineering&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">batch_features</span> <span class="kn">import</span> <span class="n">compute_user_features</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;session&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">spark_session</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a Spark session for testing.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PytestSpark&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">test_compute_user_features</span><span class="p">(</span><span class="n">spark_session</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test the feature computation logic with sample data.&quot;&quot;&quot;</span>
    <span class="c1"># Create sample data representing events</span>
    <span class="n">utc_now</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span>
    <span class="n">yesterday_utc</span> <span class="o">=</span> <span class="n">utc_now</span> <span class="o">-</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># User 1: One purchase yesterday</span>
    <span class="c1"># User 2: Two purchases, one today, one 100 days ago</span>
    <span class="n">mock_data</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;evt1&quot;</span><span class="p">,</span> <span class="s2">&quot;purchase&quot;</span><span class="p">,</span> <span class="s2">&quot;user1&quot;</span><span class="p">,</span> <span class="s2">&quot;prodA&quot;</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="n">yesterday_utc</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;evt2&quot;</span><span class="p">,</span> <span class="s2">&quot;purchase&quot;</span><span class="p">,</span> <span class="s2">&quot;user2&quot;</span><span class="p">,</span> <span class="s2">&quot;prodB&quot;</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="n">utc_now</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;evt3&quot;</span><span class="p">,</span> <span class="s2">&quot;purchase&quot;</span><span class="p">,</span> <span class="s2">&quot;user2&quot;</span><span class="p">,</span> <span class="s2">&quot;prodC&quot;</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="n">utc_now</span> <span class="o">-</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
        <span class="p">(</span><span class="s2">&quot;evt4&quot;</span><span class="p">,</span> <span class="s2">&quot;page_view&quot;</span><span class="p">,</span> <span class="s2">&quot;user1&quot;</span><span class="p">,</span> <span class="s2">&quot;prodB&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">utc_now</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="n">schema</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;event_id&quot;</span><span class="p">,</span> <span class="s2">&quot;event_type&quot;</span><span class="p">,</span> <span class="s2">&quot;user_id&quot;</span><span class="p">,</span> <span class="s2">&quot;product_id&quot;</span><span class="p">,</span> <span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="s2">&quot;event_timestamp&quot;</span><span class="p">]</span>
    <span class="n">silver_df</span> <span class="o">=</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">mock_data</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
    
    <span class="c1"># Compute features</span>
    <span class="n">features_df</span> <span class="o">=</span> <span class="n">compute_user_features</span><span class="p">(</span><span class="n">spark_session</span><span class="p">,</span> <span class="n">silver_df</span><span class="p">)</span>
    <span class="n">features_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;user_id&#39;</span><span class="p">]:</span> <span class="n">row</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">features_df</span><span class="o">.</span><span class="n">collect</span><span class="p">()}</span>
    
    <span class="c1"># Assertions for User 1</span>
    <span class="k">assert</span> <span class="n">features_map</span><span class="p">[</span><span class="s1">&#39;user1&#39;</span><span class="p">][</span><span class="s1">&#39;lifetime_purchase_count&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">features_map</span><span class="p">[</span><span class="s1">&#39;user1&#39;</span><span class="p">][</span><span class="s1">&#39;days_since_last_purchase&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
    
    <span class="c1"># Assertions for User 2</span>
    <span class="k">assert</span> <span class="n">features_map</span><span class="p">[</span><span class="s1">&#39;user2&#39;</span><span class="p">][</span><span class="s1">&#39;lifetime_purchase_count&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="c1"># Only the recent purchase is in the 90-day window</span>
    <span class="k">assert</span> <span class="n">features_map</span><span class="p">[</span><span class="s1">&#39;user2&#39;</span><span class="p">][</span><span class="s1">&#39;avg_order_value_90d&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">20.0</span>
    <span class="k">assert</span> <span class="n">features_map</span><span class="p">[</span><span class="s1">&#39;user2&#39;</span><span class="p">][</span><span class="s1">&#39;days_since_last_purchase&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
</pre></div>
</div>
</section>
<section id="pipeline-airflow-dag">
<h4>Pipeline (Airflow DAG)<a class="headerlink" href="#pipeline-airflow-dag" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.providers.amazon.aws.operators.emr</span> <span class="kn">import</span> <span class="n">EmrCreateJobFlowOperator</span><span class="p">,</span> <span class="n">EmrAddStepsOperator</span><span class="p">,</span> <span class="n">EmrTerminateJobFlowOperator</span>
<span class="kn">from</span> <span class="nn">airflow.providers.great_expectations.operators.great_expectations</span> <span class="kn">import</span> <span class="n">GreatExpectationsOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="c1"># Define the Spark step for EMR</span>
<span class="n">spark_steps</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="s2">&quot;ComputeBatchFeatures&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ActionOnFailure&quot;</span><span class="p">:</span> <span class="s2">&quot;TERMINATE_CLUSTER&quot;</span><span class="p">,</span>
        <span class="s2">&quot;HadoopJarStep&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Jar&quot;</span><span class="p">:</span> <span class="s2">&quot;command-runner.jar&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Args&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="s2">&quot;spark-submit&quot;</span><span class="p">,</span>
                <span class="s2">&quot;--deploy-mode&quot;</span><span class="p">,</span> <span class="s2">&quot;cluster&quot;</span><span class="p">,</span>
                <span class="s2">&quot;s3://ecom-propensity-airflow-artifacts/scripts/batch_features.py&quot;</span><span class="p">,</span>
                <span class="s2">&quot;--silver-path&quot;</span><span class="p">,</span> <span class="s2">&quot;s3://ecom-propensity-silver/...&quot;</span><span class="p">,</span>
                <span class="s2">&quot;--output-path&quot;</span><span class="p">,</span> <span class="s2">&quot;s3://ecom-propensity-gold-features/user_features_temp/&quot;</span>
            <span class="p">],</span>
        <span class="p">},</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># Define the EMR cluster configuration</span>
<span class="n">job_flow_overrides</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="s2">&quot;ecom-propensity-batch-feature-emr&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ReleaseLabel&quot;</span><span class="p">:</span> <span class="s2">&quot;emr-6.9.0&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Applications&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="s2">&quot;Spark&quot;</span><span class="p">}],</span>
    <span class="s2">&quot;Instances&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;InstanceGroups&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="s2">&quot;Master nodes&quot;</span><span class="p">,</span> <span class="s2">&quot;Market&quot;</span><span class="p">:</span> <span class="s2">&quot;ON_DEMAND&quot;</span><span class="p">,</span> <span class="s2">&quot;InstanceRole&quot;</span><span class="p">:</span> <span class="s2">&quot;MASTER&quot;</span><span class="p">,</span> <span class="s2">&quot;InstanceType&quot;</span><span class="p">:</span> <span class="s2">&quot;m5.xlarge&quot;</span><span class="p">,</span> <span class="s2">&quot;InstanceCount&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="s2">&quot;Core nodes&quot;</span><span class="p">,</span> <span class="s2">&quot;Market&quot;</span><span class="p">:</span> <span class="s2">&quot;ON_DEMAND&quot;</span><span class="p">,</span> <span class="s2">&quot;InstanceRole&quot;</span><span class="p">:</span> <span class="s2">&quot;CORE&quot;</span><span class="p">,</span> <span class="s2">&quot;InstanceType&quot;</span><span class="p">:</span> <span class="s2">&quot;m5.xlarge&quot;</span><span class="p">,</span> <span class="s2">&quot;InstanceCount&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
        <span class="p">],</span>
        <span class="s2">&quot;KeepJobFlowAliveWhenNoSteps&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;TerminationProtected&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;VisibleToAllUsers&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;JobFlowRole&quot;</span><span class="p">:</span> <span class="s2">&quot;EMR_EC2_DefaultRole&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ServiceRole&quot;</span><span class="p">:</span> <span class="s2">&quot;EMR_DefaultRole&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s1">&#39;batch_feature_engineering&#39;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s1">&#39;@daily&#39;</span><span class="p">,</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;feature-engineering&#39;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>

    <span class="c1"># 1. Create a transient EMR cluster</span>
    <span class="n">cluster_creator</span> <span class="o">=</span> <span class="n">EmrCreateJobFlowOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;create_emr_cluster&quot;</span><span class="p">,</span>
        <span class="n">job_flow_overrides</span><span class="o">=</span><span class="n">job_flow_overrides</span><span class="p">,</span>
        <span class="n">aws_conn_id</span><span class="o">=</span><span class="s2">&quot;aws_default&quot;</span><span class="p">,</span>
        <span class="n">emr_conn_id</span><span class="o">=</span><span class="s2">&quot;emr_default&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 2. Add the Spark job step</span>
    <span class="n">step_adder</span> <span class="o">=</span> <span class="n">EmrAddStepsOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;run_spark_job&quot;</span><span class="p">,</span>
        <span class="n">job_flow_id</span><span class="o">=</span><span class="s2">&quot;{{ task_instance.xcom_pull(task_ids=&#39;create_emr_cluster&#39;, key=&#39;return_value&#39;) }}&quot;</span><span class="p">,</span>
        <span class="n">steps</span><span class="o">=</span><span class="n">spark_steps</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 3. Validate the output data</span>
    <span class="n">data_validator</span> <span class="o">=</span> <span class="n">GreatExpectationsOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;validate_features&quot;</span><span class="p">,</span>
        <span class="n">expectation_suite_name</span><span class="o">=</span><span class="s2">&quot;user_features.warning&quot;</span><span class="p">,</span> <span class="c1"># Example suite</span>
        <span class="n">data_context_root_dir</span><span class="o">=</span><span class="s2">&quot;/usr/local/airflow/great_expectations&quot;</span><span class="p">,</span>
        <span class="n">data_asset_name</span><span class="o">=</span><span class="s2">&quot;s3://ecom-propensity-gold-features/user_features_temp/&quot;</span>
    <span class="p">)</span>
    
    <span class="c1"># 4. Materialize features to the online store</span>
    <span class="n">feast_materialize</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;feast_materialize&quot;</span><span class="p">,</span>
        <span class="n">bash_command</span><span class="o">=</span><span class="s2">&quot;cd /usr/local/airflow/feature_repo &amp;&amp; feast materialize-incremental $(date -u +&#39;%Y-%m-</span><span class="si">%d</span><span class="s2">T%H:%M:%SZ&#39;)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 5. Terminate the EMR cluster</span>
    <span class="n">cluster_remover</span> <span class="o">=</span> <span class="n">EmrTerminateJobFlowOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;terminate_emr_cluster&quot;</span><span class="p">,</span>
        <span class="n">job_flow_id</span><span class="o">=</span><span class="s2">&quot;{{ task_instance.xcom_pull(task_ids=&#39;create_emr_cluster&#39;, key=&#39;return_value&#39;) }}&quot;</span><span class="p">,</span>
        <span class="n">trigger_rule</span><span class="o">=</span><span class="s2">&quot;all_done&quot;</span><span class="p">,</span> <span class="c1"># Run whether previous steps succeed or fail</span>
    <span class="p">)</span>
    
    <span class="c1"># Define dependencies</span>
    <span class="n">cluster_creator</span> <span class="o">&gt;&gt;</span> <span class="n">step_adder</span> <span class="o">&gt;&gt;</span> <span class="n">data_validator</span> <span class="o">&gt;&gt;</span> <span class="n">feast_materialize</span> <span class="o">&gt;&gt;</span> <span class="n">cluster_remover</span>
</pre></div>
</div>
</section>
<section id="integration-tests">
<h4>Integration Tests<a class="headerlink" href="#integration-tests" title="Permalink to this heading">¶</a></h4>
<p>The purpose of this test is not to verify the Spark logic (that’s the unit test’s job), but to verify that the <strong>entire orchestrated workflow runs correctly in a deployed environment</strong>. It tests the interactions between Airflow, EMR, S3, Great Expectations, and Feast.</p>
</section>
<section id="how-and-when-the-test-is-run">
<h4><strong>1. How and When the Test is Run</strong><a class="headerlink" href="#how-and-when-the-test-is-run" title="Permalink to this heading">¶</a></h4>
<p>This test is fundamentally different from a unit test and cannot be run in a simple CI runner.</p>
<ul class="simple">
<li><p><strong>Environment:</strong> It is designed to run against a fully deployed <strong>staging environment</strong> that mirrors production.</p></li>
<li><p><strong>Trigger:</strong> It is executed as a job in our <strong>Continuous Deployment (CD) pipeline</strong> in GitHub Actions, immediately after the Airflow DAG and its related scripts have been deployed to the staging environment.</p></li>
<li><p><strong>Principle:</strong> It acts as a final “smoke test” or “health check” to confirm that the deployment was successful and the pipeline is operational before it’s ever run on real production data.</p></li>
</ul>
</section>
<section id="required-setup-prerequisites">
<h4><strong>2. Required Setup (Prerequisites)</strong><a class="headerlink" href="#required-setup-prerequisites" title="Permalink to this heading">¶</a></h4>
<ol class="arabic simple">
<li><p>A small, static, and version-controlled sample of input data (<code class="docutils literal notranslate"><span class="pre">sample_silver_data.parquet</span></code>) must be present in a known S3 bucket in the staging account.</p></li>
<li><p>The integration test runner (e.g., a GitHub Actions runner) must have AWS credentials for the staging environment.</p></li>
<li><p>The runner must have API access to the staging Airflow instance.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">feast</span> <span class="kn">import</span> <span class="n">FeatureStore</span>
<span class="kn">from</span> <span class="nn">airflow_client.client</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span> <span class="nn">airflow_client.client.api</span> <span class="kn">import</span> <span class="n">dag_run_api</span>
<span class="kn">from</span> <span class="nn">airflow_client.client.model.dag_run</span> <span class="kn">import</span> <span class="n">DAGRun</span>
<span class="kn">from</span> <span class="nn">airflow_client.client.model.error</span> <span class="kn">import</span> <span class="n">Error</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="c1"># Configure logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># --- Test Configuration ---</span>
<span class="c1"># These would be fetched from environment variables in a CI/CD runner</span>
<span class="n">AIRFLOW_HOST</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;STAGING_AIRFLOW_HOST&quot;</span><span class="p">,</span> <span class="s2">&quot;http://localhost:8080/api/v1&quot;</span><span class="p">)</span>
<span class="n">AIRFLOW_USERNAME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;STAGING_AIRFLOW_USERNAME&quot;</span><span class="p">,</span> <span class="s2">&quot;airflow&quot;</span><span class="p">)</span>
<span class="n">AIRFLOW_PASSWORD</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;STAGING_AIRFLOW_PASSWORD&quot;</span><span class="p">,</span> <span class="s2">&quot;airflow&quot;</span><span class="p">)</span>

<span class="n">DAG_ID</span> <span class="o">=</span> <span class="s2">&quot;batch_feature_engineering&quot;</span>
<span class="n">STAGING_FEAST_REPO_PATH</span> <span class="o">=</span> <span class="s2">&quot;feature_repo/&quot;</span>

<span class="c1"># The specific user we will check for in the feature store after the pipeline runs</span>
<span class="n">TEST_USER_ID</span> <span class="o">=</span> <span class="s2">&quot;user_for_integration_test&quot;</span> 
<span class="c1"># The expected value for this user based on our sample data</span>
<span class="n">EXPECTED_PURCHASE_COUNT</span> <span class="o">=</span> <span class="mi">5</span> 

<span class="c1"># --- Pytest Marker ---</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_batch_feature_pipeline_end_to_end</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Triggers the batch feature engineering DAG in a staging environment and</span>
<span class="sd">    validates that the final feature values are correctly written to the</span>
<span class="sd">    online feature store.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># --- 1. SETUP: Initialize API clients ---</span>
    <span class="n">api_client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="n">AIRFLOW_HOST</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="n">AIRFLOW_USERNAME</span><span class="p">,</span> <span class="n">passwd</span><span class="o">=</span><span class="n">AIRFLOW_PASSWORD</span><span class="p">)</span>
    
    <span class="c1"># --- 2. TRIGGER: Start a new DAG Run ---</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Triggering DAG: </span><span class="si">{</span><span class="n">DAG_ID</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">dag_run_api_instance</span> <span class="o">=</span> <span class="n">dag_run_api</span><span class="o">.</span><span class="n">DAGRunApi</span><span class="p">(</span><span class="n">api_client</span><span class="p">)</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Trigger the DAG with a specific conf to override the default paths</span>
        <span class="c1"># This points the job to our small, static test dataset</span>
        <span class="n">api_response</span> <span class="o">=</span> <span class="n">dag_run_api_instance</span><span class="o">.</span><span class="n">post_dag_run</span><span class="p">(</span>
            <span class="n">dag_id</span><span class="o">=</span><span class="n">DAG_ID</span><span class="p">,</span>
            <span class="n">dag_run</span><span class="o">=</span><span class="n">DAGRun</span><span class="p">(</span>
                <span class="n">conf</span><span class="o">=</span><span class="p">{</span>
                    <span class="s2">&quot;input_path&quot;</span><span class="p">:</span> <span class="s2">&quot;s3://ecom-propensity-staging-data/sample_silver_data.parquet&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;output_path&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;s3://ecom-propensity-staging-gold/test_run_</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">/&quot;</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">dag_run_id</span> <span class="o">=</span> <span class="n">api_response</span><span class="o">.</span><span class="n">dag_run_id</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully triggered DAG run with ID: </span><span class="si">{</span><span class="n">dag_run_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to trigger DAG run: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># --- 3. POLL: Wait for the DAG Run to complete ---</span>
    <span class="n">wait_for_dag_run_completion</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">dag_run_id</span><span class="p">)</span>

    <span class="c1"># --- 4. VALIDATE: Check the final state in the Feature Store ---</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;DAG run complete. Validating results in Feast online store...&quot;</span><span class="p">)</span>
    <span class="n">fs</span> <span class="o">=</span> <span class="n">FeatureStore</span><span class="p">(</span><span class="n">repo_path</span><span class="o">=</span><span class="n">STAGING_FEAST_REPO_PATH</span><span class="p">)</span>
    
    <span class="n">feature_vector</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">get_online_features</span><span class="p">(</span>
        <span class="n">features</span><span class="o">=</span><span class="p">[</span>
            <span class="s2">&quot;user_historical_features:lifetime_purchase_count&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">entity_rows</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="n">TEST_USER_ID</span><span class="p">}],</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Retrieved feature vector: </span><span class="si">{</span><span class="n">feature_vector</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Assert that the feature was updated with the correct value</span>
    <span class="k">assert</span> <span class="n">feature_vector</span><span class="p">[</span><span class="s2">&quot;lifetime_purchase_count&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">EXPECTED_PURCHASE_COUNT</span><span class="p">,</span> \
        <span class="sa">f</span><span class="s2">&quot;Feature validation failed for user </span><span class="si">{</span><span class="n">TEST_USER_ID</span><span class="si">}</span><span class="s2">!&quot;</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Integration test passed successfully!&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">wait_for_dag_run_completion</span><span class="p">(</span><span class="n">api_client</span><span class="p">:</span> <span class="n">Client</span><span class="p">,</span> <span class="n">dag_run_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">600</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Polls the Airflow API until the DAG run is complete or timeout is reached.&quot;&quot;&quot;</span>
    <span class="n">dag_run_api_instance</span> <span class="o">=</span> <span class="n">dag_run_api</span><span class="o">.</span><span class="n">DAGRunApi</span><span class="p">(</span><span class="n">api_client</span><span class="p">)</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    
    <span class="k">while</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span> <span class="o">&lt;</span> <span class="n">timeout_seconds</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">api_response</span> <span class="o">=</span> <span class="n">dag_run_api_instance</span><span class="o">.</span><span class="n">get_dag_run</span><span class="p">(</span><span class="n">dag_id</span><span class="o">=</span><span class="n">DAG_ID</span><span class="p">,</span> <span class="n">dag_run_id</span><span class="o">=</span><span class="n">dag_run_id</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">api_response</span><span class="o">.</span><span class="n">state</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Polling DAG run </span><span class="si">{</span><span class="n">dag_run_id</span><span class="si">}</span><span class="s2">. Current state: </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="s2">&quot;success&quot;</span><span class="p">:</span>
                <span class="k">return</span>
            <span class="k">elif</span> <span class="n">state</span> <span class="o">==</span> <span class="s2">&quot;failed&quot;</span><span class="p">:</span>
                <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DAG run </span><span class="si">{</span><span class="n">dag_run_id</span><span class="si">}</span><span class="s2"> failed.&quot;</span><span class="p">)</span>
            
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="c1"># Wait 30 seconds between polls</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;API error while polling for DAG run status: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
    <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Timeout: DAG run </span><span class="si">{</span><span class="n">dag_run_id</span><span class="si">}</span><span class="s2"> did not complete within </span><span class="si">{</span><span class="n">timeout_seconds</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>

</pre></div>
</div>
</section>
<section id="ci-cd-workflow">
<h4>CI/CD Workflow<a class="headerlink" href="#ci-cd-workflow" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CI/CD for Batch Feature Pipeline</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="nv">staging</span><span class="w"> </span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># This workflow runs when code is pushed to the &#39;staging&#39; branch</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">deploy_to_staging</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy All Artifacts to Staging</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">permissions</span><span class="p">:</span>
<span class="w">      </span><span class="nt">id-token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">write</span>
<span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">read</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">role-to-assume</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-deploy-role</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Airflow DAGs and Scripts to S3</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">aws s3 sync pipelines/dags/ s3://ecom-propensity-staging-airflow/dags/</span>
<span class="w">          </span><span class="no">aws s3 sync src/ s3://ecom-propensity-staging-airflow/src/</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Apply Feast Feature Definitions</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pip install feast</span>
<span class="w">          </span><span class="no">cd feature_repo</span>
<span class="w">          </span><span class="no">feast apply # This would be configured to point to the staging registry</span>

<span class="w">  </span><span class="nt">run-integration-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run End-to-End Integration Test</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deploy_to_staging</span><span class="w"> </span><span class="c1"># This job only runs after the deployment job succeeds</span>
<span class="w">    </span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v4</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{</span><span class="nt"> python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span><span class="w"> </span><span class="p p-Indicator">}</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install Test Dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pip install pytest feast apache-airflow-client</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Execute Integration Test</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_AIRFLOW_HOST</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_HOST }}</span>
<span class="w">          </span><span class="nt">STAGING_AIRFLOW_USERNAME</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_USERNAME }}</span>
<span class="w">          </span><span class="nt">STAGING_AIRFLOW_PASSWORD</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_AIRFLOW_PASSWORD }}</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pytest tests/integration/test_batch_feature_pipeline.py</span>
</pre></div>
</div>
</section>
<section id="the-two-emr-cluster-patterns">
<h4>The Two EMR Cluster Patterns<a class="headerlink" href="#the-two-emr-cluster-patterns" title="Permalink to this heading">¶</a></h4>
<p>Our project uses two types of EMR clusters for two very different jobs, and we use the best tool to manage each one:</p>
<p><strong>1. The Transient Batch Cluster (Handled by Airflow)</strong></p>
<ul class="simple">
<li><p><strong>Purpose:</strong> To run our daily batch feature engineering job. This job is resource-intensive but only needs to run for a few hours each day.</p></li>
<li><p><strong>Provisioning Method:</strong> The <strong>Airflow DAG</strong> uses the <code class="docutils literal notranslate"><span class="pre">EmrCreateJobFlowOperator</span></code>.</p></li>
<li><p><strong>Lifecycle:</strong> <strong>Transient/Ephemeral.</strong> The cluster is created on-demand at the start of the DAG run, performs its task, and is immediately terminated by the <code class="docutils literal notranslate"><span class="pre">EmrTerminateJobFlowOperator</span></code>.</p></li>
<li><p><strong>Why use Airflow for this?</strong> <strong>Cost Optimization.</strong> It is extremely cost-effective. We only pay for the EMR cluster for the 1-2 hours it’s actually running the Spark job. If we used a persistent cluster for this, we would be paying for it to sit idle for the other 22-23 hours of the day.</p></li>
</ul>
<p><strong>2. The Persistent Streaming Cluster (Defined in Terraform)</strong></p>
<ul class="simple">
<li><p><strong>Purpose:</strong> To run our <strong>24/7 Spark Structured Streaming job</strong> for real-time feature engineering. This is a long-running, continuous application that must always be on to process events from Kinesis as they arrive.</p></li>
<li><p><strong>Provisioning Method:</strong> It is a core piece of our infrastructure, just like a database, so it is defined in <strong>Terraform</strong>.</p></li>
<li><p><strong>Lifecycle:</strong> <strong>Persistent/Long-Lived.</strong> Terraform creates it once, and it stays running. We manage its state and configuration through our IaC.</p></li>
<li><p><strong>Why use Terraform for this?</strong> <strong>Infrastructure as Code (IaC) Best Practices.</strong> Long-running, foundational infrastructure should be explicitly declared, version-controlled, and managed via an IaC tool like Terraform. This ensures its state is predictable and auditable. It would be an anti-pattern to have an Airflow DAG launch a “permanent” cluster.</p></li>
</ul>
</section>
</section>
<section id="what-is-the-emr-tf-file-really-for">
<h3>What is the <code class="docutils literal notranslate"><span class="pre">emr.tf</span></code> file <em>really</em> for?<a class="headerlink" href="#what-is-the-emr-tf-file-really-for" title="Permalink to this heading">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">emr.tf</span></code> file has two critical responsibilities:</p>
<ol class="arabic simple">
<li><p><strong>Defining Foundational Components (Roles &amp; Security):</strong> It creates the IAM roles, instance profiles, and security configurations that are required by <em>ANY</em> EMR cluster, regardless of whether it’s the transient batch cluster or the persistent streaming one. Airflow needs to assume these roles to get permission to create its transient cluster.</p></li>
<li><p><strong>Defining the Persistent Streaming Cluster:</strong> The <code class="docutils literal notranslate"><span class="pre">emr.tf</span></code> file is the correct place to define the long-running EMR cluster dedicated to our real-time pipeline.</p></li>
</ol>
</section>
<section id="summary-table">
<h3>Summary Table<a class="headerlink" href="#summary-table" title="Permalink to this heading">¶</a></h3>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-left"><p><strong>Transient Batch Cluster</strong></p></th>
<th class="head text-left"><p><strong>Persistent Streaming Cluster</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Purpose</strong></p></td>
<td class="text-left"><p>Daily historical feature calculation</p></td>
<td class="text-left"><p>24/7 real-time feature processing</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Provisioning</strong></p></td>
<td class="text-left"><p><strong>Airflow</strong> (<code class="docutils literal notranslate"><span class="pre">EmrCreateJobFlowOperator</span></code>)</p></td>
<td class="text-left"><p><strong>Terraform</strong> (<code class="docutils literal notranslate"><span class="pre">aws_emr_cluster</span></code>)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Lifecycle</strong></p></td>
<td class="text-left"><p>Ephemeral (created &amp; destroyed daily)</p></td>
<td class="text-left"><p>Long-lived (stateful infrastructure)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Cost Model</strong></p></td>
<td class="text-left"><p>Pay-per-use (very low cost)</p></td>
<td class="text-left"><p>Always-on (a primary cost driver)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Primary Use Case</strong></p></td>
<td class="text-left"><p>The <strong>Batch Feature Engineering Pipeline</strong></p></td>
<td class="text-left"><p>The <strong>Real-Time Feature Pipeline</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p>In summary, we are deliberately using two different, purpose-built strategies for our compute infrastructure, a common and effective pattern in production MLOps.</p>
</section>
<hr class="docutils" />
<section id="feature-engineering-streaming-pipeline">
<h3>Feature Engineering: Streaming Pipeline<a class="headerlink" href="#feature-engineering-streaming-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="id4">
<h4>Architecture<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_propensity/feature_streaming.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="id5">
<h4>Python Scripts<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">google.protobuf.duration_pb2</span> <span class="kn">import</span> <span class="n">Duration</span>
<span class="kn">from</span> <span class="nn">feast</span> <span class="kn">import</span> <span class="n">Entity</span><span class="p">,</span> <span class="n">FeatureView</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">FileSource</span><span class="p">,</span> <span class="n">ValueType</span>

<span class="c1"># Define session as an entity (can be joined with user_id)</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">Entity</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;session_id&quot;</span><span class="p">,</span> <span class="n">value_type</span><span class="o">=</span><span class="n">ValueType</span><span class="o">.</span><span class="n">STRING</span><span class="p">)</span>

<span class="c1"># The source for these features is the PARQUET data being archived to S3</span>
<span class="c1"># by the streaming job. This is what Feast uses for creating training data.</span>
<span class="n">stream_feature_source</span> <span class="o">=</span> <span class="n">FileSource</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;s3://ecom-propensity-gold-features/session_features/&quot;</span><span class="p">,</span>
    <span class="n">event_timestamp_column</span><span class="o">=</span><span class="s2">&quot;event_timestamp&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define the Feature View for real-time session features</span>
<span class="n">session_features_view</span> <span class="o">=</span> <span class="n">FeatureView</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;session_streaming_features&quot;</span><span class="p">,</span>
    <span class="n">entities</span><span class="o">=</span><span class="p">[</span><span class="n">session</span><span class="p">],</span>
    <span class="n">ttl</span><span class="o">=</span><span class="n">Duration</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">86400</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>  <span class="c1"># 2 days</span>
    <span class="n">schema</span><span class="o">=</span><span class="p">[</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;session_duration_seconds&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Int64</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;distinct_products_viewed_count&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Int64</span><span class="p">),</span>
        <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Int64</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">online</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">source</span><span class="o">=</span><span class="n">stream_feature_source</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;owner&quot;</span><span class="p">:</span> <span class="s2">&quot;ml_team&quot;</span><span class="p">,</span> <span class="s2">&quot;pipeline&quot;</span><span class="p">:</span> <span class="s2">&quot;streaming&quot;</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">from_json</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">,</span> <span class="n">IntegerType</span>

<span class="kn">import</span> <span class="nn">redis</span>
<span class="kn">from</span> <span class="nn">feast</span> <span class="kn">import</span> <span class="n">FeatureStore</span>

<span class="c1"># Setup logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> - </span><span class="si">%(levelname)s</span><span class="s1"> - </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Schema for the incoming JSON data from Kinesis</span>
<span class="n">EVENT_SCHEMA</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;event_type&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">()),</span>
    <span class="n">Field</span><span class="p">(</span><span class="s2">&quot;product_id&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">()),</span>
    <span class="n">Field</span><span class="p">(</span><span class="s2">&quot;session_id&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span> <span class="c1"># session_id is required</span>
    <span class="n">Field</span><span class="p">(</span><span class="s2">&quot;event_timestamp&quot;</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">()),</span>
    <span class="c1"># ... other fields</span>
<span class="p">])</span>

<span class="k">def</span> <span class="nf">update_session_state</span><span class="p">(</span><span class="n">session_id</span><span class="p">,</span> <span class="n">new_events</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The core stateful logic for updating session features.&quot;&quot;&quot;</span>
    
    <span class="c1"># Get current state or initialize a new one</span>
    <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">exists</span><span class="p">:</span>
        <span class="n">current_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">current_state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;session_start_time&quot;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">(),</span>
            <span class="s2">&quot;distinct_products_viewed&quot;</span><span class="p">:</span> <span class="nb">set</span><span class="p">(),</span>
            <span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">:</span> <span class="mi">0</span>
        <span class="p">}</span>

    <span class="c1"># Iterate through new events and update state</span>
    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">new_events</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">product_id</span><span class="p">:</span>
            <span class="n">current_state</span><span class="p">[</span><span class="s2">&quot;distinct_products_viewed&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">event</span><span class="o">.</span><span class="n">product_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">event_type</span> <span class="o">==</span> <span class="s1">&#39;add_to_cart&#39;</span><span class="p">:</span>
            <span class="n">current_state</span><span class="p">[</span><span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Update state object and set a timeout to prevent infinite state growth</span>
    <span class="n">state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span>
    <span class="n">state</span><span class="o">.</span><span class="n">setTimeoutDuration</span><span class="p">(</span><span class="s2">&quot;24 hours&quot;</span><span class="p">)</span> <span class="c1"># Evict state if no events for 24 hours</span>

    <span class="c1"># Yield the updated features</span>
    <span class="k">yield</span> <span class="p">(</span><span class="n">session_id</span><span class="p">,</span> 
           <span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span> <span class="o">-</span> <span class="n">current_state</span><span class="p">[</span><span class="s2">&quot;session_start_time&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">seconds</span><span class="p">,</span> 
           <span class="nb">len</span><span class="p">(</span><span class="n">current_state</span><span class="p">[</span><span class="s2">&quot;distinct_products_viewed&quot;</span><span class="p">]),</span> 
           <span class="n">current_state</span><span class="p">[</span><span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">],</span>
           <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">write_to_feast_online_store</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Writes a micro-batch of features to the Feast Redis store.&quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Writing batch </span><span class="si">{</span><span class="n">epoch_id</span><span class="si">}</span><span class="s2"> to online store...&quot;</span><span class="p">)</span>
    
    <span class="c1"># Using feast&#39;s push mechanism is one way, another is direct to Redis</span>
    <span class="c1"># For performance, direct Redis client is often better.</span>
    <span class="c1"># Note: Connection pooling should be used in a real production job.</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">redis</span><span class="o">.</span><span class="n">Redis</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;your-redis-endpoint&#39;</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">6379</span><span class="p">,</span> <span class="n">db</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
        <span class="n">entity_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;session_id:</span><span class="si">{</span><span class="n">row</span><span class="o">.</span><span class="n">session_id</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">feature_payload</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;session_duration_seconds&quot;</span><span class="p">:</span> <span class="n">row</span><span class="o">.</span><span class="n">session_duration_seconds</span><span class="p">,</span>
            <span class="s2">&quot;distinct_products_viewed_count&quot;</span><span class="p">:</span> <span class="n">row</span><span class="o">.</span><span class="n">distinct_products_viewed_count</span><span class="p">,</span>
            <span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">:</span> <span class="n">row</span><span class="o">.</span><span class="n">add_to_cart_count</span>
        <span class="p">}</span>
        <span class="c1"># In Redis, features are often stored in a Hash</span>
        <span class="n">r</span><span class="o">.</span><span class="n">hset</span><span class="p">(</span><span class="n">entity_key</span><span class="p">,</span> <span class="n">mapping</span><span class="o">=</span><span class="n">feature_payload</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished writing batch </span><span class="si">{</span><span class="n">epoch_id</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;StreamingFeatureEngineering&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

    <span class="c1"># Read from Kinesis</span>
    <span class="n">kinesis_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span> \
        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;kinesis&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;streamName&quot;</span><span class="p">,</span> <span class="s2">&quot;processed-events-stream&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;startingPosition&quot;</span><span class="p">,</span> <span class="s2">&quot;latest&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">load</span><span class="p">()</span>

    <span class="c1"># Parse JSON data and apply schema</span>
    <span class="n">json_df</span> <span class="o">=</span> <span class="n">kinesis_df</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&quot;CAST(data AS STRING) as json&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_json</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">),</span> <span class="n">EVENT_SCHEMA</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">))</span> \
        <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;data.*&quot;</span><span class="p">)</span>

    <span class="c1"># Apply the stateful transformation</span>
    <span class="n">features_df</span> <span class="o">=</span> <span class="n">json_df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;session_id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">flatMapGroupsWithState</span><span class="p">(</span>
        <span class="n">outputMode</span><span class="o">=</span><span class="s2">&quot;update&quot;</span><span class="p">,</span>
        <span class="n">stateFormatVersion</span><span class="o">=</span><span class="s2">&quot;2&quot;</span><span class="p">,</span>
        <span class="n">timeoutConf</span><span class="o">=</span><span class="s2">&quot;eventTimeTimeout&quot;</span><span class="p">,</span>
        <span class="n">func</span><span class="o">=</span><span class="n">update_session_state</span>
    <span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s2">&quot;session_id&quot;</span><span class="p">,</span> <span class="s2">&quot;session_duration_seconds&quot;</span><span class="p">,</span> <span class="s2">&quot;distinct_products_viewed_count&quot;</span><span class="p">,</span> <span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">,</span> <span class="s2">&quot;event_timestamp&quot;</span><span class="p">])</span>

    <span class="c1"># --- Write to Sinks ---</span>
    <span class="c1"># Sink 1: Write to Feast Online Store (Redis)</span>
    <span class="n">query_online</span> <span class="o">=</span> <span class="n">features_df</span><span class="o">.</span><span class="n">writeStream</span> \
        <span class="o">.</span><span class="n">foreachBatch</span><span class="p">(</span><span class="n">write_to_feast_online_store</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="s2">&quot;s3://ecom-propensity-checkpoints/online_sink&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="c1"># Sink 2: Write to S3 for Feast Offline Store (Archival)</span>
    <span class="n">query_offline</span> <span class="o">=</span> <span class="n">features_df</span><span class="o">.</span><span class="n">writeStream</span> \
        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">&quot;append&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;path&quot;</span><span class="p">,</span> <span class="s2">&quot;s3://ecom-propensity-gold-features/session_features/&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="s2">&quot;s3://ecom-propensity-checkpoints/offline_sink&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">awaitAnyTermination</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id6">
<h4>Unit Tests<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">MagicMock</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;src/feature_engineering&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">streaming_features</span> <span class="kn">import</span> <span class="n">update_session_state</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">mock_spark_state</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mocks the Spark state object.&quot;&quot;&quot;</span>
    <span class="n">state_store</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="n">mock_state</span> <span class="o">=</span> <span class="n">MagicMock</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">exists_func</span><span class="p">():</span>
        <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">state_store</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">get_func</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">state_store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;state&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_func</span><span class="p">(</span><span class="n">new_state</span><span class="p">):</span>
        <span class="n">state_store</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_state</span>

    <span class="n">mock_state</span><span class="o">.</span><span class="n">exists</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="n">fget</span><span class="o">=</span><span class="n">exists_func</span><span class="p">)</span>
    <span class="n">mock_state</span><span class="o">.</span><span class="n">get</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="n">fget</span><span class="o">=</span><span class="n">get_func</span><span class="p">)</span>
    <span class="n">mock_state</span><span class="o">.</span><span class="n">update</span> <span class="o">=</span> <span class="n">update_func</span>

    <span class="k">return</span> <span class="n">mock_state</span>

<span class="k">def</span> <span class="nf">test_update_session_state_new_session</span><span class="p">(</span><span class="n">mock_spark_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test the initialization and update of a new session.&quot;&quot;&quot;</span>
    
    <span class="c1"># Mock an incoming event</span>
    <span class="n">MockEvent</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="s2">&quot;MockEvent&quot;</span><span class="p">,</span> <span class="p">(),</span> <span class="p">{</span><span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="s2">&quot;prodA&quot;</span><span class="p">,</span> <span class="s2">&quot;event_type&quot;</span><span class="p">:</span> <span class="s2">&quot;page_view&quot;</span><span class="p">})</span>
    <span class="n">new_events</span> <span class="o">=</span> <span class="p">[</span><span class="n">MockEvent</span><span class="p">()]</span>
    
    <span class="c1"># Run the function</span>
    <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">update_session_state</span><span class="p">(</span><span class="s2">&quot;sess1&quot;</span><span class="p">,</span> <span class="nb">iter</span><span class="p">(</span><span class="n">new_events</span><span class="p">),</span> <span class="n">mock_spark_state</span><span class="p">))</span>
    
    <span class="c1"># Assert state was updated</span>
    <span class="k">assert</span> <span class="n">mock_spark_state</span><span class="o">.</span><span class="n">get</span><span class="p">[</span><span class="s1">&#39;add_to_cart_count&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="s2">&quot;prodA&quot;</span> <span class="ow">in</span> <span class="n">mock_spark_state</span><span class="o">.</span><span class="n">get</span><span class="p">[</span><span class="s1">&#39;distinct_products_viewed&#39;</span><span class="p">]</span>
    
    <span class="c1"># Assert correct output was yielded</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="c1"># distinct_products_viewed_count</span>

<span class="k">def</span> <span class="nf">test_update_session_state_existing_session</span><span class="p">(</span><span class="n">mock_spark_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test updating an already existing session state.&quot;&quot;&quot;</span>
    
    <span class="c1"># Pre-populate the state</span>
    <span class="n">initial_state</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;session_start_time&quot;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">(),</span>
        <span class="s2">&quot;distinct_products_viewed&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;prodA&quot;</span><span class="p">},</span>
        <span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">:</span> <span class="mi">0</span>
    <span class="p">}</span>
    <span class="n">mock_spark_state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">initial_state</span><span class="p">)</span>

    <span class="c1"># Mock a new event</span>
    <span class="n">MockEvent</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="s2">&quot;MockEvent&quot;</span><span class="p">,</span> <span class="p">(),</span> <span class="p">{</span><span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="s2">&quot;prodB&quot;</span><span class="p">,</span> <span class="s2">&quot;event_type&quot;</span><span class="p">:</span> <span class="s2">&quot;add_to_cart&quot;</span><span class="p">})</span>
    <span class="n">new_events</span> <span class="o">=</span> <span class="p">[</span><span class="n">MockEvent</span><span class="p">()]</span>

    <span class="c1"># Run the function</span>
    <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">update_session_state</span><span class="p">(</span><span class="s2">&quot;sess1&quot;</span><span class="p">,</span> <span class="nb">iter</span><span class="p">(</span><span class="n">new_events</span><span class="p">),</span> <span class="n">mock_spark_state</span><span class="p">))</span>

    <span class="c1"># Assert state was updated correctly</span>
    <span class="k">assert</span> <span class="n">mock_spark_state</span><span class="o">.</span><span class="n">get</span><span class="p">[</span><span class="s1">&#39;add_to_cart_count&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">mock_spark_state</span><span class="o">.</span><span class="n">get</span><span class="p">[</span><span class="s1">&#39;distinct_products_viewed&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="id7">
<h4>Pipeline (Airflow DAG)<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.providers.amazon.aws.sensors.emr</span> <span class="kn">import</span> <span class="n">EmrClusterSensor</span>
<span class="kn">from</span> <span class="nn">airflow.providers.slack.operators.slack_webhook</span> <span class="kn">import</span> <span class="n">SlackWebhookOperator</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="n">STREAMING_CLUSTER_ID</span> <span class="o">=</span> <span class="s2">&quot;j-STREAMINGCLUSTERID&quot;</span> <span class="c1"># This should come from a Variable or SSM</span>

<span class="k">def</span> <span class="nf">slack_alert_on_failure</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Send a Slack alert if the task fails.&quot;&quot;&quot;</span>
    <span class="n">alert</span> <span class="o">=</span> <span class="n">SlackWebhookOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;slack_alert&#39;</span><span class="p">,</span>
        <span class="n">http_conn_id</span><span class="o">=</span><span class="s1">&#39;slack_connection&#39;</span><span class="p">,</span> <span class="c1"># Airflow connection to Slack webhook</span>
        <span class="n">message</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            :red_circle: High-Priority Alert: EMR Streaming Cluster is DOWN!</span>
<span class="s2">            *DAG*: </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;task_instance&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dag_id</span><span class="si">}</span>
<span class="s2">            *Task*: </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;task_instance&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">task_id</span><span class="si">}</span>
<span class="s2">            *Cluster ID*: </span><span class="si">{</span><span class="n">STREAMING_CLUSTER_ID</span><span class="si">}</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">,</span>
        <span class="n">channel</span><span class="o">=</span><span class="s1">&#39;#mlops-alerts&#39;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">alert</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s1">&#39;streaming_job_monitor&#39;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s1">&#39;*/15 * * * *&#39;</span><span class="p">,</span> <span class="c1"># Run every 15 minutes</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">on_failure_callback</span><span class="o">=</span><span class="n">slack_alert_on_failure</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;monitoring&#39;</span><span class="p">,</span> <span class="s1">&#39;streaming&#39;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    
    <span class="n">check_emr_cluster_health</span> <span class="o">=</span> <span class="n">EmrClusterSensor</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;check_emr_cluster_health&#39;</span><span class="p">,</span>
        <span class="n">job_flow_id</span><span class="o">=</span><span class="n">STREAMING_CLUSTER_ID</span><span class="p">,</span>
        <span class="n">target_states</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;WAITING&#39;</span><span class="p">],</span> <span class="c1"># &#39;WAITING&#39; means the cluster is idle and ready for steps</span>
        <span class="n">aws_conn_id</span><span class="o">=</span><span class="s1">&#39;aws_default&#39;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id8">
<h4>Integration Tests<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">feast</span> <span class="kn">import</span> <span class="n">FeatureStore</span>

<span class="n">TEST_SESSION_ID</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;test-session-</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">TEST_PRODUCT_ID</span> <span class="o">=</span> <span class="s2">&quot;prod-for-stream-test&quot;</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_streaming_pipeline_end_to_end</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pushes a synthetic event to the raw Kinesis stream and validates</span>
<span class="sd">    that the feature eventually appears in the Feast online store.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># --- 1. SETUP ---</span>
    <span class="n">kinesis_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;kinesis&quot;</span><span class="p">,</span> <span class="n">region_name</span><span class="o">=</span><span class="s2">&quot;eu-west-1&quot;</span><span class="p">)</span>
    <span class="n">raw_stream_name</span> <span class="o">=</span> <span class="s2">&quot;raw-events-stream&quot;</span>
    
    <span class="c1"># --- 2. SEND EVENT ---</span>
    <span class="n">event_payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;event_id&quot;</span><span class="p">:</span> <span class="s2">&quot;evt-stream-test&quot;</span><span class="p">,</span>
        <span class="s2">&quot;event_type&quot;</span><span class="p">:</span> <span class="s2">&quot;add_to_cart&quot;</span><span class="p">,</span>
        <span class="s2">&quot;product_id&quot;</span><span class="p">:</span> <span class="n">TEST_PRODUCT_ID</span><span class="p">,</span>
        <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;user-stream-test&quot;</span><span class="p">,</span>
        <span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="n">TEST_SESSION_ID</span><span class="p">,</span>
        <span class="s2">&quot;client_timestamp&quot;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;Z&quot;</span>
    <span class="p">}</span>
    
    <span class="n">kinesis_client</span><span class="o">.</span><span class="n">put_record</span><span class="p">(</span>
        <span class="n">StreamName</span><span class="o">=</span><span class="n">raw_stream_name</span><span class="p">,</span>
        <span class="n">Data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">event_payload</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">),</span>
        <span class="n">PartitionKey</span><span class="o">=</span><span class="n">TEST_SESSION_ID</span>
    <span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sent test event for session </span><span class="si">{</span><span class="n">TEST_SESSION_ID</span><span class="si">}</span><span class="s2"> to Kinesis.&quot;</span><span class="p">)</span>

    <span class="c1"># --- 3. WAIT &amp; POLL ---</span>
    <span class="c1"># Wait for the event to propagate through ingestion lambda and Spark streaming</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">90</span><span class="p">)</span> <span class="c1"># This wait time depends on the streaming trigger interval</span>

    <span class="c1"># --- 4. VALIDATE in FEAST ---</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Polling Feast online store for updated feature...&quot;</span><span class="p">)</span>
    <span class="n">fs</span> <span class="o">=</span> <span class="n">FeatureStore</span><span class="p">(</span><span class="n">repo_path</span><span class="o">=</span><span class="s2">&quot;feature_repo/&quot;</span><span class="p">)</span>
    
    <span class="n">feature_vector</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">get_online_features</span><span class="p">(</span>
        <span class="n">features</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;session_streaming_features:add_to_cart_count&quot;</span><span class="p">],</span>
        <span class="n">entity_rows</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="n">TEST_SESSION_ID</span><span class="p">}],</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Retrieved feature vector: </span><span class="si">{</span><span class="n">feature_vector</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">feature_vector</span><span class="p">[</span><span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> \
        <span class="s2">&quot;Feature was not updated correctly in the online store!&quot;</span>
</pre></div>
</div>
</section>
<section id="id9">
<h4>CI/CD Workflow<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Streaming Feature Engineering Job</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="nv">main</span><span class="w"> </span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/feature_engineering/streaming_features.py&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">deploy_streaming_job</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy and Submit Spark Streaming Job</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">permissions</span><span class="p">:</span>
<span class="w">      </span><span class="nt">id-token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">write</span>
<span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">read</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">role-to-assume</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-deploy-role</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Upload Spark script to S3</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">aws s3 cp src/feature_engineering/streaming_features.py s3://ecom-propensity-airflow-artifacts/scripts/</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Find and Terminate Existing Streaming Step (if any)</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no"># In a production system, you need logic to gracefully update the job.</span>
<span class="w">          </span><span class="no"># A common strategy is to find the old step ID and cancel it before submitting the new one.</span>
<span class="w">          </span><span class="no"># This is complex and depends on naming conventions.</span>
<span class="w">          </span><span class="no">echo &quot;Finding and terminating existing job steps...&quot;</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Submit New Spark Job to EMR Cluster</span>
<span class="w">        </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">submit_job</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">CLUSTER_ID=$(aws emr list-clusters --active --query &quot;Clusters[?Name==&#39;ecom-propensity-streaming-cluster&#39;].Id&quot; --output text)</span>
<span class="w">          </span><span class="no">if [ -z &quot;$CLUSTER_ID&quot; ]; then</span>
<span class="w">            </span><span class="no">echo &quot;::error::Persistent EMR cluster not found!&quot;</span>
<span class="w">            </span><span class="no">exit 1</span>
<span class="w">          </span><span class="no">fi</span>
<span class="w">          </span>
<span class="w">          </span><span class="no">aws emr add-steps --cluster-id $CLUSTER_ID --steps Type=spark,Name=&quot;Realtime Feature Engineering&quot;,ActionOnFailure=CONTINUE,Args=[--deploy-mode,client,s3://ecom-propensity-airflow-artifacts/scripts/streaming_features.py]</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Integration Test</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">success()</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pip install pytest boto3 feast</span>
<span class="w">          </span><span class="no">pytest tests/integration/test_streaming_feature_pipeline.py</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="model-training-pipeline">
<h3>Model Training pipeline<a class="headerlink" href="#model-training-pipeline" title="Permalink to this heading">¶</a></h3>
<section id="id10">
<h4>Architecture<a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_propensity/model_training.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="id11">
<h4>IaC (Terraform)<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h4>
<div class="highlight-terraform notranslate"><div class="highlight"><pre><span></span><span class="c1"># IAM Role that SageMaker Training Jobs will assume</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role&quot;</span><span class="w"> </span><span class="nv">&quot;sagemaker_training_role&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;sagemaker-training-execution-role&quot;</span>
<span class="w">  </span><span class="na">assume_role_policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">jsonencode</span><span class="p">({</span>
<span class="w">    </span><span class="na">Version</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">Statement</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[{</span>
<span class="w">      </span><span class="na">Action</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;sts:AssumeRole&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="na">Effect</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nb">Principal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="na">Service</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;sagemaker.amazonaws.com&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">    </span><span class="p">}],</span>
<span class="w">  </span><span class="p">})</span>
<span class="p">}</span>

<span class="c1"># Policy allowing access to S3, ECR, and CloudWatch Logs</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_policy&quot;</span><span class="w"> </span><span class="nv">&quot;sagemaker_training_policy&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;sagemaker-training-policy&quot;</span>
<span class="w">  </span><span class="na">policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">jsonencode</span><span class="p">({</span>
<span class="w">    </span><span class="na">Version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">Statement</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="na">Effect</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="na">Action</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;s3:GetObject&quot;, &quot;s3:PutObject&quot;, &quot;s3:ListBucket&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="na">Resource</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;arn:aws:s3:::ecom-propensity-*&quot;, &quot;arn:aws:s3:::ecom-propensity-*/*&quot;</span><span class="p">],</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="na">Effect</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="na">Action</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;ecr:GetAuthorizationToken&quot;, &quot;ecr:BatchGetImage&quot;, &quot;ecr:GetDownloadUrlForLayer&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="na">Resource</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;*&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="na">Effect</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="na">Action</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;logs:CreateLogGroup&quot;, &quot;logs:CreateLogStream&quot;, &quot;logs:PutLogEvents&quot;</span><span class="p">],</span>
<span class="w">        </span><span class="na">Resource</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;arn:aws:logs:*:*:*&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">  </span><span class="p">})</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_iam_role_policy_attachment&quot;</span><span class="w"> </span><span class="nv">&quot;sagemaker_training_attach&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">role</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.sagemaker_training_role.name</span>
<span class="w">  </span><span class="na">policy_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_policy.sagemaker_training_policy.arn</span>
<span class="p">}</span>

<span class="c1"># Define an ECR repository to store our training container</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_ecr_repository&quot;</span><span class="w"> </span><span class="nv">&quot;training_repo&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;ecom-propensity/training&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id12">
<h4>Python Scripts<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%(asctime)-15s</span><span class="s2"> </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># --- MLflow setup ---</span>
    <span class="c1"># The MLFLOW_TRACKING_URI is set by the SageMaker operator in Airflow</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MLFLOW_TRACKING_URI&quot;</span><span class="p">])</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="s2">&quot;purchase-intent-training&quot;</span><span class="p">)</span>

    <span class="c1"># --- Parse arguments ---</span>
    <span class="c1"># SageMaker passes hyperparameters and data paths as command-line arguments.</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--n_estimators&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--learning_rate&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="c1"># SageMaker environment variables for data channels</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--train&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SM_CHANNEL_TRAIN&quot;</span><span class="p">))</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># --- Data Loading ---</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loading training data...&quot;</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># --- MLflow Tracking ---</span>
    <span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
        <span class="n">run_id</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Started MLflow Run: </span><span class="si">{</span><span class="n">run_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

        <span class="c1"># --- Model Training ---</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Training LightGBM model...&quot;</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">(</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span>
        <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
            <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)],</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="n">period</span><span class="o">=</span><span class="mi">0</span><span class="p">)]</span>
        <span class="p">)</span>

        <span class="c1"># --- Log artifacts &amp; metrics ---</span>
        <span class="n">val_auc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">best_score_</span><span class="p">[</span><span class="s1">&#39;valid_0&#39;</span><span class="p">][</span><span class="s1">&#39;binary_logloss&#39;</span><span class="p">]</span> <span class="c1"># Example metric</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;validation_auc&quot;</span><span class="p">,</span> <span class="n">val_auc</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">lightgbm</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logged model with Validation AUC: </span><span class="si">{</span><span class="n">val_auc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>This script is the second validation gate. It runs more complex, business-logic-oriented tests to ensure the model behaves as expected in edge cases. It exits with a non-zero status code if any test fails, which will cause the Airflow task to fail.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># /src/model_training/test.py</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%(asctime)-15s</span><span class="s2"> </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">run_sliced_evaluation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">overall_auc</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Evaluates model performance on critical data slices.&quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;--- Running Sliced Evaluation ---&quot;</span><span class="p">)</span>
    
    <span class="n">slices</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;mobile_users&quot;</span><span class="p">:</span> <span class="s2">&quot;device_type_mobile == 1&quot;</span><span class="p">,</span>
        <span class="s2">&quot;desktop_users&quot;</span><span class="p">:</span> <span class="s2">&quot;device_type_desktop == 1&quot;</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">all_slices_passed</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">for</span> <span class="n">slice_name</span><span class="p">,</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">slices</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">slice_idx</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">slice_idx</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Slice &#39;</span><span class="si">{</span><span class="n">slice_name</span><span class="si">}</span><span class="s2">&#39; is empty. Skipping.&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
            
        <span class="n">slice_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">slice_idx</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">slice_idx</span><span class="p">])[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;AUC for slice &#39;</span><span class="si">{</span><span class="n">slice_name</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">slice_auc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Check if performance drops significantly on the slice</span>
        <span class="k">if</span> <span class="n">slice_auc</span> <span class="o">&lt;</span> <span class="n">overall_auc</span> <span class="o">*</span> <span class="mf">0.95</span><span class="p">:</span> <span class="c1"># Allow for a 5% drop</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;FAIL: Performance on slice &#39;</span><span class="si">{</span><span class="n">slice_name</span><span class="si">}</span><span class="s2">&#39; is significantly lower than overall performance.&quot;</span><span class="p">)</span>
            <span class="n">all_slices_passed</span> <span class="o">=</span> <span class="kc">False</span>
    
    <span class="k">if</span> <span class="n">all_slices_passed</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;SUCCESS: All sliced evaluations passed.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_slices_passed</span>

<span class="k">def</span> <span class="nf">run_behavioral_tests</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Runs behavioral tests like invariance and directional expectations.&quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;--- Running Behavioral Tests ---&quot;</span><span class="p">)</span>
    <span class="n">all_tests_passed</span> <span class="o">=</span> <span class="kc">True</span>
    
    <span class="c1"># --- Invariance Test ---</span>
    <span class="c1"># Prediction should not change if we alter a non-predictive ID</span>
    <span class="n">test_record</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">original_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">test_record</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">test_record_modified</span> <span class="o">=</span> <span class="n">test_record</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="c1"># Assuming a feature like &#39;session_uuid&#39; that shouldn&#39;t affect the outcome</span>
    <span class="c1"># If not present, we can skip or use another irrelevant feature</span>
    <span class="k">if</span> <span class="s1">&#39;session_uuid&#39;</span> <span class="ow">in</span> <span class="n">test_record_modified</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">test_record_modified</span><span class="p">[</span><span class="s1">&#39;session_uuid&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12345</span> 
        <span class="n">modified_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">test_record_modified</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">original_pred</span><span class="p">,</span> <span class="n">modified_pred</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;FAIL: Invariance test failed. Prediction changed with session_uuid.&quot;</span><span class="p">)</span>
            <span class="n">all_tests_passed</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;SUCCESS: Invariance test passed.&quot;</span><span class="p">)</span>

    <span class="c1"># --- Directional Expectation Test ---</span>
    <span class="c1"># Adding an &#39;add_to_cart&#39; event should increase the propensity score</span>
    <span class="c1"># Find a record with a moderate number of add_to_cart events</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">record_to_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;add_to_cart_count&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">base_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">record_to_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="n">record_to_test</span><span class="p">[</span><span class="s1">&#39;add_to_cart_count&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">higher_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">record_to_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">higher_pred</span> <span class="o">&lt;=</span> <span class="n">base_pred</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;FAIL: Directional test failed. Increasing add_to_cart_count did not increase score.&quot;</span><span class="p">)</span>
            <span class="n">all_tests_passed</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;SUCCESS: Directional test passed.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Skipping directional test: no suitable test records found.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">all_tests_passed</span>

<span class="k">def</span> <span class="nf">advanced_tests</span><span class="p">(</span><span class="n">run_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">test_data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MLFLOW_TRACKING_URI&quot;</span><span class="p">])</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running advanced tests for model run: </span><span class="si">{</span><span class="n">run_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">lightgbm</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;runs:/</span><span class="si">{</span><span class="n">run_id</span><span class="si">}</span><span class="s2">/model&quot;</span><span class="p">)</span>
    
    <span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">test_data_path</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>

    <span class="c1"># Get overall AUC from the MLflow run to use as a baseline</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">tracking</span><span class="o">.</span><span class="n">MlflowClient</span><span class="p">()</span>
    <span class="n">overall_auc</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_run</span><span class="p">(</span><span class="n">run_id</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;test_auc&quot;</span><span class="p">]</span>

    <span class="n">sliced_passed</span> <span class="o">=</span> <span class="n">run_sliced_evaluation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">overall_auc</span><span class="p">)</span>
    <span class="n">behavioral_passed</span> <span class="o">=</span> <span class="n">run_behavioral_tests</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">sliced_passed</span> <span class="ow">and</span> <span class="n">behavioral_passed</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;One or more advanced tests failed. Halting promotion.&quot;</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Exit with a non-zero code to fail the Airflow task</span>
    
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;All advanced tests passed successfully.&quot;</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--run-id&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--test-data-path&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">advanced_tests</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">run_id</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">test_data_path</span><span class="p">)</span>
</pre></div>
</div>
<p>This is the final step. If all previous gates have passed, this script formally registers the model and promotes it to the “Staging” environment, making it available for the CI/CD deployment pipeline.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># /src/model_training/register.py</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%(asctime)-15s</span><span class="s2"> </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">register_model</span><span class="p">(</span><span class="n">run_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Registers a new model version from an MLflow run and transitions it to Staging.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        run_id (str): The MLflow run ID of the model to register.</span>
<span class="sd">        model_name (str): The name of the model in the MLflow Model Registry.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MLFLOW_TRACKING_URI&quot;</span><span class="p">])</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">tracking</span><span class="o">.</span><span class="n">MlflowClient</span><span class="p">()</span>

    <span class="n">model_uri</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;runs:/</span><span class="si">{</span><span class="n">run_id</span><span class="si">}</span><span class="s2">/model&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Registering model &#39;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&#39; from URI: </span><span class="si">{</span><span class="n">model_uri</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Register the model, which creates a new version</span>
    <span class="n">model_version_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="n">version</span> <span class="o">=</span> <span class="n">model_version_info</span><span class="o">.</span><span class="n">version</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully registered model version: </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Add a description to the model version</span>
    <span class="n">description</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;This model (version </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s2">) was automatically promoted by the &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;training pipeline on </span><span class="si">{</span><span class="n">datetime</span><span class="o">.</span><span class="n">date</span><span class="o">.</span><span class="n">today</span><span class="p">()</span><span class="si">}</span><span class="s2">. &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Source run ID: </span><span class="si">{</span><span class="n">run_id</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">client</span><span class="o">.</span><span class="n">update_model_version</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="n">description</span>
    <span class="p">)</span>

    <span class="c1"># Transition the new model version to the &quot;Staging&quot; stage</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transitioning model version </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s2"> to &#39;Staging&#39;...&quot;</span><span class="p">)</span>
    <span class="n">client</span><span class="o">.</span><span class="n">transition_model_version_stage</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span>
        <span class="n">stage</span><span class="o">=</span><span class="s2">&quot;Staging&quot;</span><span class="p">,</span>
        <span class="n">archive_existing_versions</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># Crucial for production!</span>
    <span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Transition to Staging complete.&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--run-id&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--model-name&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">register_model</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">run_id</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id13">
<h4>Unit Tests<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">patch</span><span class="p">,</span> <span class="n">MagicMock</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;src/model_training&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">train</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">mock_train_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a mock DataFrame for testing.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;feature1&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
        <span class="s1">&#39;feature2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
        <span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">50</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">50</span>
    <span class="p">})</span>

<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;train.mlflow&#39;</span><span class="p">)</span>
<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;train.lgb.LGBMClassifier&#39;</span><span class="p">)</span>
<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;pandas.read_parquet&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_main_training_logic</span><span class="p">(</span><span class="n">mock_read_parquet</span><span class="p">,</span> <span class="n">mock_lgbm</span><span class="p">,</span> <span class="n">mock_mlflow</span><span class="p">,</span> <span class="n">mock_train_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test the main training function&#39;s flow.&quot;&quot;&quot;</span>
    <span class="c1"># Setup mocks</span>
    <span class="n">mock_read_parquet</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="n">mock_train_data</span>
    <span class="n">mock_lgbm_instance</span> <span class="o">=</span> <span class="n">MagicMock</span><span class="p">()</span>
    <span class="n">mock_lgbm</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="n">mock_lgbm_instance</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MLFLOW_TRACKING_URI&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;http://dummy-uri&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SM_CHANNEL_TRAIN&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;/data&#39;</span>

    <span class="c1"># Run the main function</span>
    <span class="n">train</span><span class="o">.</span><span class="n">main</span><span class="p">()</span>
    
    <span class="c1"># Assertions</span>
    <span class="n">mock_mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="o">.</span><span class="n">assert_called_with</span><span class="p">(</span><span class="s2">&quot;purchase-intent-training&quot;</span><span class="p">)</span>
    <span class="n">mock_mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="o">.</span><span class="n">assert_called_once</span><span class="p">()</span>
    <span class="n">mock_lgbm_instance</span><span class="o">.</span><span class="n">fit</span><span class="o">.</span><span class="n">assert_called_once</span><span class="p">()</span>
    <span class="n">mock_mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="o">.</span><span class="n">assert_called</span><span class="p">()</span>
    <span class="n">mock_mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="o">.</span><span class="n">assert_called_with</span><span class="p">(</span><span class="s2">&quot;validation_auc&quot;</span><span class="p">,</span> <span class="n">pytest</span><span class="o">.</span><span class="n">approx</span><span class="p">(</span><span class="mf">0.693</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span> <span class="c1"># Check for some value</span>
    <span class="n">mock_mlflow</span><span class="o">.</span><span class="n">lightgbm</span><span class="o">.</span><span class="n">log_model</span><span class="o">.</span><span class="n">assert_called_once</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id14">
<h4>Pipeline (Airflow DAG)<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span><span class="p">,</span> <span class="n">BranchPythonOperator</span>
<span class="kn">from</span> <span class="nn">airflow.providers.amazon.aws.operators.sagemaker</span> <span class="kn">import</span> <span class="n">SageMakerTrainingOperator</span>
<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span>
<span class="kn">from</span> <span class="nn">sagemaker.estimator</span> <span class="kn">import</span> <span class="n">Estimator</span>

<span class="c1"># --- DAG Definition ---</span>
<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s1">&#39;model_training_pipeline&#39;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s1">&#39;@weekly&#39;</span><span class="p">,</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;training&#39;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>

    <span class="c1"># Task 1: Get data from Feast</span>
    <span class="k">def</span> <span class="nf">get_data_from_feast</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># ... logic from src/model_training/data.py</span>
        <span class="c1"># Saves data to S3 and pushes path via XCom</span>
        <span class="k">pass</span>

    <span class="c1"># Task 2: Validate data with Great Expectations</span>
    <span class="k">def</span> <span class="nf">validate_data</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># ... logic from src/model_training/validate.py</span>
        <span class="k">pass</span>

    <span class="c1"># Task 3: Launch SageMaker Training Job</span>
    <span class="c1"># Define the estimator here for clarity</span>
    <span class="n">sagemaker_estimator</span> <span class="o">=</span> <span class="n">Estimator</span><span class="p">(</span>
        <span class="n">image_uri</span><span class="o">=</span><span class="s2">&quot;&lt;aws_account_id&gt;.dkr.ecr.eu-west-1.amazonaws.com/ecom-propensity/training:latest&quot;</span><span class="p">,</span>
        <span class="n">role</span><span class="o">=</span><span class="s2">&quot;arn:aws:iam::&lt;aws_account_id&gt;:role/sagemaker-training-execution-role&quot;</span><span class="p">,</span>
        <span class="n">instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">instance_type</span><span class="o">=</span><span class="s1">&#39;ml.m5.large&#39;</span><span class="p">,</span>
        <span class="c1"># Pass hyperparameters</span>
        <span class="n">hyperparameters</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="mi">250</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">},</span>
        <span class="c1"># Pass environment variables</span>
        <span class="n">environment</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;MLFLOW_TRACKING_URI&#39;</span><span class="p">:</span> <span class="s1">&#39;http://your-mlflow-server:5000&#39;</span><span class="p">}</span>
    <span class="p">)</span>
    
    <span class="n">train_model</span> <span class="o">=</span> <span class="n">SageMakerTrainingOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;train_model&#39;</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;TrainingJobName&quot;</span><span class="p">:</span> <span class="s2">&quot;propensity-model-{{ ds_nodash }}&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AlgorithmSpecification&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;TrainingImage&quot;</span><span class="p">:</span> <span class="n">sagemaker_estimator</span><span class="o">.</span><span class="n">image_uri</span><span class="p">,</span>
                <span class="s2">&quot;TrainingInputMode&quot;</span><span class="p">:</span> <span class="s2">&quot;File&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;RoleArn&quot;</span><span class="p">:</span> <span class="n">sagemaker_estimator</span><span class="o">.</span><span class="n">role</span><span class="p">,</span>
            <span class="c1"># ... other SageMaker configs</span>
        <span class="p">},</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="s1">&#39;{{ ti.xcom_pull(task_ids=&quot;get_data_task&quot;)[&quot;s3_path&quot;] }}&#39;</span><span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># Task 4: Evaluate model and decide whether to proceed</span>
    <span class="k">def</span> <span class="nf">evaluate_and_decide</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># ... logic from src/model_training/evaluate.py</span>
        <span class="c1"># Compares new model run to prod model in MLflow Registry</span>
        <span class="c1"># if is_better:</span>
        <span class="c1">#    return &#39;run_advanced_tests&#39;</span>
        <span class="c1"># else:</span>
        <span class="c1">#    return &#39;end_pipeline&#39;</span>
        <span class="k">pass</span>
    
    <span class="n">branch_on_evaluation</span> <span class="o">=</span> <span class="n">BranchPythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;check_evaluation&#39;</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">evaluate_and_decide</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Task 5: Run advanced behavioral and fairness tests</span>
    <span class="k">def</span> <span class="nf">run_advanced_tests</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># ... logic from src/model_training/test.py</span>
        <span class="k">pass</span>

    <span class="c1"># Task 6: Register model in MLflow Staging</span>
    <span class="k">def</span> <span class="nf">register_model</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># ... logic from src/model_training/register.py</span>
        <span class="k">pass</span>

    <span class="c1"># Define dependencies</span>
    <span class="c1"># get_data_task &gt;&gt; validate_data &gt;&gt; train_model &gt;&gt; branch_on_evaluation</span>
    <span class="c1"># branch_on_evaluation &gt;&gt; [run_advanced_tests_task, end_pipeline_task]</span>
    <span class="c1"># run_advanced_tests_task &gt;&gt; register_model_task</span>
</pre></div>
</div>
</section>
<section id="id15">
<h4>Integration Tests<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">airflow_client.client</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># --- Test Configuration ---</span>
<span class="n">DAG_ID</span> <span class="o">=</span> <span class="s2">&quot;model_training_pipeline&quot;</span>
<span class="n">MLFLOW_TRACKING_URI</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;STAGING_MLFLOW_URI&quot;</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_training_pipeline_end_to_end</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Triggers the training DAG and validates that a new model version is created in Staging.&quot;&quot;&quot;</span>
    
    <span class="c1"># --- 1. SETUP: Get current latest version ---</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">MLFLOW_TRACKING_URI</span><span class="p">)</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">tracking</span><span class="o">.</span><span class="n">MlflowClient</span><span class="p">()</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;PurchasePropensityModel&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">initial_versions</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_latest_versions</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">stages</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Staging&quot;</span><span class="p">])</span>
        <span class="n">initial_version_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial_versions</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RestException</span><span class="p">:</span>
        <span class="n">initial_version_count</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Model doesn&#39;t exist yet</span>

    <span class="c1"># --- 2. TRIGGER: Start a new DAG Run ---</span>
    <span class="c1"># (Code to trigger DAG via Airflow API, similar to feature pipeline test)</span>
    <span class="c1"># ...</span>
    
    <span class="c1"># --- 3. POLL: Wait for the DAG Run to complete ---</span>
    <span class="c1"># (Code to poll for DAG run completion)</span>
    <span class="c1"># ...</span>
    
    <span class="c1"># --- 4. VALIDATE: Check MLflow Model Registry ---</span>
    <span class="n">final_versions</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_latest_versions</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">stages</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Staging&quot;</span><span class="p">])</span>
    <span class="n">final_version_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">final_versions</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">final_version_count</span> <span class="o">&gt;</span> <span class="n">initial_version_count</span><span class="p">,</span> \
        <span class="s2">&quot;Integration test failed: No new model version was moved to Staging.&quot;</span>
</pre></div>
</div>
</section>
<section id="id16">
<h4>CI/CD Workflow<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CI/CD for Model Training Pipeline</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="nv">main</span><span class="w"> </span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/model_training/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;pipelines/dags/model_training_dag.py&#39;</span>
<span class="w">  </span><span class="nt">pull_request</span><span class="p">:</span>
<span class="w">    </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;src/model_training/**&#39;</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;pipelines/dags/model_training_dag.py&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">build-and-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Build Container &amp; Run Tests</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Unit Tests</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest tests/unit/test_training.py</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS credentials</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">github.ref == &#39;refs/heads/main&#39;</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="c1"># ... credentials config ...</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Login to Amazon ECR</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">github.ref == &#39;refs/heads/main&#39;</span>
<span class="w">        </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">login-ecr</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/amazon-ecr-login@v1</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Build, tag, and push image to Amazon ECR</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">github.ref == &#39;refs/heads/main&#39;</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">ECR_REGISTRY</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ steps.login-ecr.outputs.registry }}</span>
<span class="w">          </span><span class="nt">ECR_REPOSITORY</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ecom-propensity/training</span>
<span class="w">          </span><span class="nt">IMAGE_TAG</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">latest</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG -f src/model_training/Dockerfile .</span>
<span class="w">          </span><span class="no">docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG</span>

<span class="w">  </span><span class="nt">deploy-dag</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Airflow DAG</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">build-and-test</span>
<span class="w">    </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">github.ref == &#39;refs/heads/main&#39;</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="c1"># ... credentials config ...</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy DAG to S3</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">aws s3 cp pipelines/dags/model_training_dag.py s3://ecom-propensity-airflow-artifacts/dags/</span>
</pre></div>
</div>
</section>
</section>
<section id="id17">
<h3>Feature Engineering: Batch<a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h3>
<section id="id18">
<h4>Architecture<a class="headerlink" href="#id18" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_propensity/inference_sequence_diagram.svg" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="id19">
<h4>Python Scripts<a class="headerlink" href="#id19" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>
<span class="kn">from</span> <span class="nn">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">Request</span><span class="p">,</span> <span class="n">HTTPException</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">feast</span> <span class="kn">import</span> <span class="n">FeatureStore</span>

<span class="c1"># --- Logging Setup ---</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># --- FastAPI App Initialization ---</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">()</span>

<span class="c1"># --- Model &amp; Feature Store Loading ---</span>
<span class="c1"># This happens once at container startup</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="s2">&quot;/opt/ml/model/model.joblib&quot;</span> <span class="c1"># SageMaker&#39;s default model path</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Successfully loaded model from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">MODEL_PATH</span><span class="p">)</span>
    
    <span class="c1"># Initialize Feast feature store. Assumes feature_repo is packaged with the app.</span>
    <span class="c1"># The registry.db path needs to be accessible. For production, a shared path is better.</span>
    <span class="n">fs</span> <span class="o">=</span> <span class="n">FeatureStore</span><span class="p">(</span><span class="n">repo_path</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">)</span> 
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Successfully initialized Feast Feature Store.&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">critical</span><span class="p">(</span><span class="s2">&quot;Failed to load model or feature store: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">fs</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># --- Pydantic Schemas for Request &amp; Response ---</span>
<span class="k">class</span> <span class="nc">PredictionRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">user_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">session_id</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span> <span class="nc">PredictionResponse</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">propensity</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">model_version</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;MODEL_VERSION&quot;</span><span class="p">,</span> <span class="s2">&quot;v0.0.0&quot;</span><span class="p">)</span>

<span class="c1"># --- API Endpoints ---</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;/health&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">health_check</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Health check endpoint for SageMaker to ping.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">fs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="n">status_code</span><span class="o">=</span><span class="mi">503</span><span class="p">,</span> <span class="n">detail</span><span class="o">=</span><span class="s2">&quot;Model or Feature Store not loaded&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;ok&quot;</span><span class="p">}</span>

<span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/predict&quot;</span><span class="p">,</span> <span class="n">response_model</span><span class="o">=</span><span class="n">PredictionResponse</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Main prediction endpoint.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">fs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="n">status_code</span><span class="o">=</span><span class="mi">503</span><span class="p">,</span> <span class="n">detail</span><span class="o">=</span><span class="s2">&quot;Model is not ready&quot;</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># 1. Fetch online features from Feast (Redis)</span>
        <span class="n">feature_vector</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">get_online_features</span><span class="p">(</span>
            <span class="n">features</span><span class="o">=</span><span class="p">[</span>
                <span class="s2">&quot;user_historical_features:lifetime_purchase_count&quot;</span><span class="p">,</span>
                <span class="s2">&quot;user_historical_features:avg_order_value_90d&quot;</span><span class="p">,</span>
                <span class="s2">&quot;session_streaming_features:add_to_cart_count&quot;</span><span class="p">,</span>
                <span class="c1"># ... add all other features here</span>
            <span class="p">],</span>
            <span class="n">entity_rows</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="n">request</span><span class="o">.</span><span class="n">user_id</span><span class="p">,</span> <span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="n">request</span><span class="o">.</span><span class="n">session_id</span><span class="p">}],</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_df</span><span class="p">()</span>
        
        <span class="c1"># 2. Prepare features for the model (drop IDs, ensure order)</span>
        <span class="n">feature_df</span> <span class="o">=</span> <span class="n">feature_vector</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;user_id&quot;</span><span class="p">,</span> <span class="s2">&quot;session_id&quot;</span><span class="p">,</span> <span class="s2">&quot;event_timestamp&quot;</span><span class="p">])</span>

        <span class="c1"># 3. Get prediction</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">feature_df</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Prediction successful for session </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">session_id</span><span class="p">)</span>
        
        <span class="c1"># 4. Return response</span>
        <span class="k">return</span> <span class="n">PredictionResponse</span><span class="p">(</span><span class="n">propensity</span><span class="o">=</span><span class="n">prediction</span><span class="p">)</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Error during prediction: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="n">status_code</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">detail</span><span class="o">=</span><span class="s2">&quot;Internal server error during prediction.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-docker notranslate"><div class="highlight"><pre><span></span><span class="c"># Use an official lightweight Python image.</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">python:3.9-slim</span>

<span class="c"># Set the working directory in the container</span>
<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>

<span class="c"># Set environment variables for SageMaker</span>
<span class="k">ENV</span><span class="w"> </span>SAGEMAKER_PROGRAM<span class="w"> </span>app.py

<span class="c"># Copy the requirements file and install dependencies</span>
<span class="k">COPY</span><span class="w"> </span>requirements.txt<span class="w"> </span>.
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>-r<span class="w"> </span>requirements.txt

<span class="c"># Copy the feature repository and the application code</span>
<span class="k">COPY</span><span class="w"> </span>./feature_repo<span class="w"> </span>./feature_repo
<span class="k">COPY</span><span class="w"> </span>.<span class="w"> </span>.

<span class="c"># SageMaker will mount the model artifacts to /opt/ml/model</span>
<span class="c"># The CMD is not needed, as SageMaker&#39;s entrypoint will run the app</span>

<span class="c"># If running locally (not on SageMaker):</span>
<span class="c"># CMD [&quot;gunicorn&quot;, &quot;-w&quot;, &quot;4&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;-b&quot;, &quot;0.0.0.0:8080&quot;, &quot;app:app&quot;]</span>
</pre></div>
</div>
</section>
<section id="id20">
<h4>Unit Tests<a class="headerlink" href="#id20" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">patch</span><span class="p">,</span> <span class="n">MagicMock</span>
<span class="kn">from</span> <span class="nn">fastapi.testclient</span> <span class="kn">import</span> <span class="n">TestClient</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pytest</span>

<span class="c1"># Add src to path to allow imports</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;src/serving&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">app</span> <span class="kn">import</span> <span class="n">app</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">TestClient</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>

<span class="nd">@patch</span><span class="p">(</span><span class="s1">&#39;app.fs&#39;</span><span class="p">)</span> <span class="c1"># Mock the Feast FeatureStore object</span>
<span class="k">def</span> <span class="nf">test_predict_success</span><span class="p">(</span><span class="n">mock_fs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test the happy path for the /predict endpoint.&quot;&quot;&quot;</span>
    <span class="c1"># Mock the return value of get_online_features</span>
    <span class="n">mock_feature_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;user123&quot;</span><span class="p">],</span>
        <span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;sess456&quot;</span><span class="p">],</span>
        <span class="s2">&quot;lifetime_purchase_count&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span>
        <span class="s2">&quot;avg_order_value_90d&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">99.5</span><span class="p">],</span>
        <span class="s2">&quot;add_to_cart_count&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="s2">&quot;event_timestamp&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="o">.</span><span class="n">now</span><span class="p">()]</span>
    <span class="p">})</span>
    <span class="n">mock_fs</span><span class="o">.</span><span class="n">get_online_features</span><span class="o">.</span><span class="n">return_value</span><span class="o">.</span><span class="n">to_df</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="n">mock_feature_df</span>

    <span class="c1"># Mock the model object</span>
    <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s1">&#39;app.model&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">mock_model</span><span class="p">:</span>
        <span class="n">mock_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]]</span> <span class="c1"># Mock output</span>
        
        <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="s2">&quot;/predict&quot;</span><span class="p">,</span>
            <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;user123&quot;</span><span class="p">,</span> <span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="s2">&quot;sess456&quot;</span><span class="p">}</span>
        <span class="p">)</span>
        
        <span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
        <span class="k">assert</span> <span class="s2">&quot;propensity&quot;</span> <span class="ow">in</span> <span class="n">data</span>
        <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;propensity&quot;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">1.0</span>
        <span class="k">assert</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;propensity&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">0.8</span>

<span class="k">def</span> <span class="nf">test_health_check</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test the /health endpoint.&quot;&quot;&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;/health&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span>
    <span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span> <span class="o">==</span> <span class="p">{</span><span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;ok&quot;</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id21">
<h4>IaC (Terraform)<a class="headerlink" href="#id21" title="Permalink to this heading">¶</a></h4>
<div class="highlight-terraform notranslate"><div class="highlight"><pre><span></span><span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_ecr_repository&quot;</span><span class="w"> </span><span class="nv">&quot;serving_repo&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;ecom-propensity/serving&quot;</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sagemaker_model&quot;</span><span class="w"> </span><span class="nv">&quot;propensity_model&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">               </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;propensity-model-${var.model_version}&quot;</span><span class="c1"> # Versioned model</span>
<span class="w">  </span><span class="na">execution_role_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.sagemaker_training_role.arn</span><span class="c1"> # Reuse role for simplicity</span>
<span class="w">  </span>
<span class="w">  </span><span class="nb">primary_container</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;${aws_ecr_repository.serving_repo.repository_url}:${var.image_tag}&quot;</span>
<span class="w">    </span><span class="nb">environment</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="s2">&quot;MODEL_VERSION&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">var.model_version</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sagemaker_endpoint_configuration&quot;</span><span class="w"> </span><span class="nv">&quot;propensity_endpoint_config&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;propensity-endpoint-config-${var.model_version}&quot;</span>

<span class="w">  </span><span class="nb">production_variants</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">variant_name</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;v1&quot;</span><span class="c1"> # This would be the old variant in a canary</span>
<span class="w">    </span><span class="na">model_name</span><span class="w">             </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_sagemaker_model.propensity_model.name</span>
<span class="w">    </span><span class="na">instance_type</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;ml.c5.xlarge&quot;</span>
<span class="w">    </span><span class="na">initial_instance_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span>
<span class="w">    </span><span class="na">initial_variant_weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">1.0</span><span class="c1"> # In a canary, this would be updated</span>
<span class="w">  </span><span class="p">}</span>
<span class="c1">  </span>
<span class="c1">  # When doing a canary, you would add a second production_variant block</span>
<span class="c1">  # for the new model and adjust the initial_variant_weight.</span>
<span class="p">}</span>

<span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sagemaker_endpoint&quot;</span><span class="w"> </span><span class="nv">&quot;propensity_endpoint&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">                 </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;propensity-endpoint&quot;</span>
<span class="w">  </span><span class="na">endpoint_config_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_sagemaker_endpoint_configuration.propensity_endpoint_config.name</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id22">
<h4>Integration Tests<a class="headerlink" href="#id22" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>import os
import json
import boto3
import pytest

SAGEMAKER_ENDPOINT_NAME = os.environ.get(&quot;STAGING_SAGEMAKER_ENDPOINT&quot;)

@pytest.mark.integration
def test_sagemaker_endpoint_invocation():
    &quot;&quot;&quot;Sends a real request to the deployed SageMaker endpoint.&quot;&quot;&quot;
    assert SAGEMAKER_ENDPOINT_NAME is not None, &quot;STAGING_SAGEMAKER_ENDPOINT env var not set&quot;

    sagemaker_runtime = boto3.client(&quot;sagemaker-runtime&quot;, region_name=&quot;eu-west-1&quot;)
    
    payload = {
        &quot;user_id&quot;: &quot;integration-test-user&quot;,
        &quot;session_id&quot;: &quot;integration-test-session&quot;
    }
    
    response = sagemaker_runtime.invoke_endpoint(
        EndpointName=SAGEMAKER_ENDPOINT_NAME,
        ContentType=&#39;application/json&#39;,
        Body=json.dumps(payload)
    )
    
    assert response[&#39;ResponseMetadata&#39;][&#39;HTTPStatusCode&#39;] == 200
    
    result = json.loads(response[&#39;Body&#39;].read().decode())
    assert &#39;propensity&#39; in result
    assert isinstance(result[&#39;propensity&#39;], float)```

</pre></div>
</div>
</section>
<section id="api-contract-smoke-test">
<h4>API Contract &amp; Smoke Test<a class="headerlink" href="#api-contract-smoke-test" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">ValidationError</span>

<span class="c1"># --- Test Configuration ---</span>
<span class="n">SAGEMAKER_ENDPOINT_NAME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;STAGING_SAGEMAKER_ENDPOINT&quot;</span><span class="p">)</span>
<span class="n">AWS_REGION</span> <span class="o">=</span> <span class="s2">&quot;eu-west-1&quot;</span>

<span class="c1"># --- Pydantic Model for Response Validation ---</span>
<span class="c1"># This defines the &quot;API Contract&quot; we expect.</span>
<span class="k">class</span> <span class="nc">ExpectedResponse</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">propensity</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">model_version</span><span class="p">:</span> <span class="nb">str</span>

<span class="c1"># --- Pytest Fixtures ---</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;module&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sagemaker_client</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Boto3 client for SageMaker service.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;sagemaker&quot;</span><span class="p">,</span> <span class="n">region_name</span><span class="o">=</span><span class="n">AWS_REGION</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;module&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sagemaker_runtime_client</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Boto3 client for SageMaker runtime (invocations).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;sagemaker-runtime&quot;</span><span class="p">,</span> <span class="n">region_name</span><span class="o">=</span><span class="n">AWS_REGION</span><span class="p">)</span>

<span class="c1"># --- Test Cases ---</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">smoke</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_endpoint_is_in_service</span><span class="p">(</span><span class="n">sagemaker_client</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Smoke Test 1: Checks that the endpoint is deployed and healthy.</span>
<span class="sd">    This is our proxy for the /health check.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">SAGEMAKER_ENDPOINT_NAME</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;STAGING_SAGEMAKER_ENDPOINT env var not set&quot;</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">sagemaker_client</span><span class="o">.</span><span class="n">describe_endpoint</span><span class="p">(</span><span class="n">EndpointName</span><span class="o">=</span><span class="n">SAGEMAKER_ENDPOINT_NAME</span><span class="p">)</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;EndpointStatus&quot;</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;InService&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Endpoint is not InService. Current status: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># We can add a wait loop here if needed</span>
        <span class="c1"># ...</span>
        
    <span class="k">except</span> <span class="n">sagemaker_client</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">ClientError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not describe SageMaker endpoint. Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">smoke</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_api_contract_and_schema</span><span class="p">(</span><span class="n">sagemaker_runtime_client</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Smoke Test 2: Invokes the endpoint and validates the response schema (API Contract).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="s2">&quot;contract-test-user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="s2">&quot;contract-test-session&quot;</span>
    <span class="p">}</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">sagemaker_runtime_client</span><span class="o">.</span><span class="n">invoke_endpoint</span><span class="p">(</span>
        <span class="n">EndpointName</span><span class="o">=</span><span class="n">SAGEMAKER_ENDPOINT_NAME</span><span class="p">,</span>
        <span class="n">ContentType</span><span class="o">=</span><span class="s1">&#39;application/json&#39;</span><span class="p">,</span>
        <span class="n">Body</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">)</span>
    <span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;ResponseMetadata&#39;</span><span class="p">][</span><span class="s1">&#39;HTTPStatusCode&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">200</span>
    
    <span class="c1"># Validate the response body against our Pydantic schema</span>
    <span class="n">response_body</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;Body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">())</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">validated_response</span> <span class="o">=</span> <span class="n">ExpectedResponse</span><span class="p">(</span><span class="o">**</span><span class="n">response_body</span><span class="p">)</span>
        <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">validated_response</span><span class="o">.</span><span class="n">propensity</span> <span class="o">&lt;=</span> <span class="mf">1.0</span>
    <span class="k">except</span> <span class="n">ValidationError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;API contract validation failed. Response did not match schema: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id23">
<h4>CI/CD Workflow<a class="headerlink" href="#id23" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Model to SageMaker</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">workflow_dispatch</span><span class="p">:</span>
<span class="w">    </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">      </span><span class="nt">model_version</span><span class="p">:</span>
<span class="w">        </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;MLflow</span><span class="nv"> </span><span class="s">Model</span><span class="nv"> </span><span class="s">Version</span><span class="nv"> </span><span class="s">(e.g.,</span><span class="nv"> </span><span class="s">5)&#39;</span>
<span class="w">        </span><span class="nt">required</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">      </span><span class="nt">image_tag</span><span class="p">:</span>
<span class="w">        </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Docker</span><span class="nv"> </span><span class="s">image</span><span class="nv"> </span><span class="s">tag</span><span class="nv"> </span><span class="s">(e.g.,</span><span class="nv"> </span><span class="s">v1.2.0)&#39;</span>
<span class="w">        </span><span class="nt">required</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">deploy-to-staging</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Build &amp; Deploy to Staging</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">permissions</span><span class="p">:</span>
<span class="w">      </span><span class="nt">id-token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">write</span>
<span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">read</span>

<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">role-to-assume</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-deploy-role</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Download model artifact from MLflow</span>
<span class="w">        </span><span class="c1"># This step requires a script that uses the MLflow client</span>
<span class="w">        </span><span class="c1"># to download the model artifact for the specified version.</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">mkdir -p /tmp/model</span>
<span class="w">          </span><span class="no">python scripts/download_model.py --model-name PurchasePropensity --version ${{ github.event.inputs.model_version }} --output-path /tmp/model/model.joblib</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Build and push Docker image to ECR</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no"># ... (logic to build /src/serving/Dockerfile, including the downloaded model, and push to ECR with the image_tag) ...</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Setup Terraform</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hashicorp/setup-terraform@v2</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Terraform Apply to Staging</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd infrastructure</span>
<span class="w">          </span><span class="no">terraform init</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve \</span>
<span class="w">            </span><span class="no">-var=&quot;model_version=${{ github.event.inputs.model_version }}&quot; \</span>
<span class="w">            </span><span class="no">-var=&quot;image_tag=${{ github.event.inputs.image_tag }}&quot; \</span>
<span class="w">            </span><span class="no">-var-file=&quot;staging.tfvars&quot; # Use a workspace for staging</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Integration Test on Staging Endpoint</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_SAGEMAKER_ENDPOINT</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;propensity-endpoint&quot;</span><span class="w"> </span><span class="c1"># This should be an output from Terraform</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pip install -r tests/requirements.txt</span>
<span class="w">          </span><span class="no">pytest tests/integration/test_inference_pipeline.py</span>
<span class="w">  </span>
<span class="w">  </span><span class="nt">smoke-test-staging</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run API Contract &amp; Smoke Test</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deploy-to-staging</span>
<span class="w">    </span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">role-to-assume</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-deploy-role</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install Test Dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install pytest boto3 pydantic</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Execute Smoke &amp; Contract Tests</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="c1"># This endpoint name should be an output from the &#39;deploy-to-staging&#39; job</span>
<span class="w">          </span><span class="nt">STAGING_SAGEMAKER_ENDPOINT</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ needs.deploy-to-staging.outputs.endpoint_name }}</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest tests/integration/test_api_contract.py</span>
</pre></div>
</div>
</section>
</section>
<section id="performance-load-testing">
<h3>Performance &amp; Load Testing<a class="headerlink" href="#performance-load-testing" title="Permalink to this heading">¶</a></h3>
<p>The load test is a separate, longer-running workflow, often triggered manually or on a nightly schedule, not on every deployment.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">from</span> <span class="nn">locust</span> <span class="kn">import</span> <span class="n">User</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">between</span>
<span class="kn">from</span> <span class="nn">botocore.config</span> <span class="kn">import</span> <span class="n">Config</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># --- Locust Configuration ---</span>
<span class="n">AWS_REGION</span> <span class="o">=</span> <span class="s2">&quot;eu-west-1&quot;</span>
<span class="n">SAGEMAKER_ENDPOINT_NAME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;STAGING_SAGEMAKER_ENDPOINT&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Boto3Client</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A wrapper for the Boto3 SageMaker client to be used in Locust.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
        <span class="c1"># Increase retries and timeouts for a production load test</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">(</span>
            <span class="n">retries</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;max_attempts&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;standard&#39;</span><span class="p">},</span>
            <span class="n">connect_timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">read_timeout</span><span class="o">=</span><span class="mi">10</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sagemaker_runtime</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span>
            <span class="s2">&quot;sagemaker-runtime&quot;</span><span class="p">,</span>
            <span class="n">region_name</span><span class="o">=</span><span class="n">AWS_REGION</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">invoke_endpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">payload</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Invoke endpoint and record the result in Locust.&quot;&quot;&quot;</span>
        <span class="n">request_meta</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;request_type&quot;</span><span class="p">:</span> <span class="s2">&quot;sagemaker&quot;</span><span class="p">,</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;invoke_endpoint&quot;</span><span class="p">,</span>
            <span class="s2">&quot;start_time&quot;</span><span class="p">:</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">(),</span>
            <span class="s2">&quot;response_length&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{},</span>
            <span class="s2">&quot;exception&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">start_perf_counter</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sagemaker_runtime</span><span class="o">.</span><span class="n">invoke_endpoint</span><span class="p">(</span>
                <span class="n">EndpointName</span><span class="o">=</span><span class="n">SAGEMAKER_ENDPOINT_NAME</span><span class="p">,</span>
                <span class="n">ContentType</span><span class="o">=</span><span class="s1">&#39;application/json&#39;</span><span class="p">,</span>
                <span class="n">Body</span><span class="o">=</span><span class="n">payload</span>
            <span class="p">)</span>
            <span class="n">response_body</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;Body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="n">request_meta</span><span class="p">[</span><span class="s2">&quot;response_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">response_body</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">request_meta</span><span class="p">[</span><span class="s2">&quot;exception&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">e</span>
        
        <span class="n">request_meta</span><span class="p">[</span><span class="s2">&quot;response_time&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_perf_counter</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
        <span class="c1"># This is where we fire the event for Locust to record</span>
        <span class="n">events</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">fire</span><span class="p">(</span><span class="o">**</span><span class="n">request_meta</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SageMakerUser</span><span class="p">(</span><span class="n">User</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulates a user making prediction requests to the SageMaker endpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">wait_time</span> <span class="o">=</span> <span class="n">between</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="c1"># Wait 100-500ms between requests</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">environment</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">environment</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">SAGEMAKER_ENDPOINT_NAME</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;STAGING_SAGEMAKER_ENDPOINT env var must be set for Locust test.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">Boto3Client</span><span class="p">()</span>

    <span class="nd">@task</span>
    <span class="k">def</span> <span class="nf">make_prediction</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Generate a random payload to simulate different users</span>
        <span class="n">user_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;load-test-user-</span><span class="si">{</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">10000</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">session_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;load-test-session-</span><span class="si">{</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">50000</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="n">payload</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span>
            <span class="s2">&quot;user_id&quot;</span><span class="p">:</span> <span class="n">user_id</span><span class="p">,</span>
            <span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="n">session_id</span>
        <span class="p">})</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">invoke_endpoint</span><span class="p">(</span><span class="n">payload</span><span class="p">)</span>

<span class="c1"># Locust needs to be imported here for the event hook to work</span>
<span class="kn">from</span> <span class="nn">locust</span> <span class="kn">import</span> <span class="n">events</span>
</pre></div>
</div>
<p><strong>How to run this test:</strong>
This command simulates 100 users (-u 100), with each user starting 20 tasks per second (-r 20), for a total duration of 5 minutes (–run-time 5m), and saves an HTML report.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Locust</span>
pip<span class="w"> </span>install<span class="w"> </span>locust

<span class="c1"># Set the environment variable</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">STAGING_SAGEMAKER_ENDPOINT</span><span class="o">=</span><span class="s2">&quot;propensity-endpoint&quot;</span>

<span class="c1"># Run Locust from the command line</span>
locust<span class="w"> </span>-f<span class="w"> </span>tests/performance/locustfile.py<span class="w"> </span>--headless<span class="w"> </span>-u<span class="w"> </span><span class="m">100</span><span class="w"> </span>-r<span class="w"> </span><span class="m">20</span><span class="w"> </span>--run-time<span class="w"> </span>5m<span class="w"> </span>--html<span class="w"> </span>report.html
</pre></div>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Performance Load Test on Staging</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">workflow_dispatch</span><span class="p">:</span><span class="w"> </span><span class="c1"># Allows manual triggering</span>
<span class="w">  </span><span class="nt">schedule</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">cron</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;0</span><span class="nv"> </span><span class="s">2</span><span class="nv"> </span><span class="s">*</span><span class="nv"> </span><span class="s">*</span><span class="nv"> </span><span class="s">*&#39;</span><span class="w"> </span><span class="c1"># Runs every night at 2 AM UTC</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">run-load-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Execute Locust Load Test</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">permissions</span><span class="p">:</span>
<span class="w">      </span><span class="nt">id-token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">write</span>
<span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">read</span>

<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">role-to-assume</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-deploy-role</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install Dependencies</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install locust boto3</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Locust Test</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="w">          </span><span class="nt">STAGING_SAGEMAKER_ENDPOINT</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ secrets.STAGING_ENDPOINT_NAME }}</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">locust -f tests/performance/locustfile.py --headless \</span>
<span class="w">            </span><span class="no">-u 200 -r 50 --run-time 10m \</span>
<span class="w">            </span><span class="no">--csv report \</span>
<span class="w">            </span><span class="no">--exit-code-on-error 1</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Upload Locust Report</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/upload-artifact@v3</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">locust-report</span>
<span class="w">          </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">report*</span>
</pre></div>
</div>
</section>
<section id="a-b-testing">
<h3>A/B Testing<a class="headerlink" href="#a-b-testing" title="Permalink to this heading">¶</a></h3>
<section id="id24">
<h4>Architecture<a class="headerlink" href="#id24" title="Permalink to this heading">¶</a></h4>
<img src="../_static/past_experiences/ecom_propensity/ab_testing.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="iac">
<h4>IaC<a class="headerlink" href="#iac" title="Permalink to this heading">¶</a></h4>
<div class="highlight-terraform notranslate"><div class="highlight"><pre><span></span>variable &quot;champion_model_name&quot; {
  description = &quot;The name of the currently deployed champion model.&quot;
  type        = string
}

variable &quot;challenger_model_name&quot; {
  description = &quot;The name of the new challenger model for the A/B test. Can be empty.&quot;
  type        = string
  default     = &quot;&quot;
}

variable &quot;challenger_weight&quot; {
  description = &quot;The percentage of traffic (0-100) to route to the challenger model.&quot;
  type        = number
  default     = 0
}

resource &quot;aws_sagemaker_endpoint_configuration&quot; &quot;propensity_endpoint_config&quot; {
  name = &quot;propensity-endpoint-config-ab-test&quot;
  # This lifecycle block prevents Terraform from destroying the old config before the new one is active
  lifecycle {
    create_before_destroy = true
  }

  # --- Champion Model Variant ---
  production_variants {
    variant_name           = &quot;champion&quot;
    model_name             = var.champion_model_name
    instance_type          = &quot;ml.c5.xlarge&quot;
    initial_instance_count = 2
    initial_variant_weight = 100 - var.challenger_weight
  }

  # --- Challenger Model Variant (Created Conditionally) ---
  dynamic &quot;production_variants&quot; {
    # This block is only created if a challenger_model_name is provided
    for_each = var.challenger_model_name != &quot;&quot; ? [1] : []
    content {
      variant_name           = &quot;challenger&quot;
      model_name             = var.challenger_model_name
      instance_type          = &quot;ml.c5.xlarge&quot;
      initial_instance_count = 2 # Start with same capacity for fair performance test
      initial_variant_weight = var.challenger_weight
    }
  }
}

resource &quot;aws_sagemaker_endpoint&quot; &quot;propensity_endpoint&quot; {
  name                 = &quot;propensity-endpoint&quot;
  endpoint_config_name = aws_sagemaker_endpoint_configuration.propensity_endpoint_config.name
}
</pre></div>
</div>
</section>
<section id="analysis-script">
<h4>Analysis Script<a class="headerlink" href="#analysis-script" title="Permalink to this heading">¶</a></h4>
<p>This script performs the statistical analysis of the experiment results. It would be run by an Airflow DAG after the experiment concludes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span> <span class="nf">analyze_experiment_results</span><span class="p">(</span><span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loads experiment data and performs a T-test to determine the winner.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        data_path (str): Path to the Parquet file containing experiment results.</span>
<span class="sd">                         Expected columns: `user_id`, `variant_name`, `converted` (0 or 1).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    
    <span class="n">control_group</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;variant_name&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;champion&#39;</span><span class="p">]</span>
    <span class="n">treatment_group</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;variant_name&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;challenger&#39;</span><span class="p">]</span>
    
    <span class="c1"># --- Calculate metrics ---</span>
    <span class="n">control_conversion_rate</span> <span class="o">=</span> <span class="n">control_group</span><span class="p">[</span><span class="s1">&#39;converted&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">treatment_conversion_rate</span> <span class="o">=</span> <span class="n">treatment_group</span><span class="p">[</span><span class="s1">&#39;converted&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">lift</span> <span class="o">=</span> <span class="p">(</span><span class="n">treatment_conversion_rate</span> <span class="o">-</span> <span class="n">control_conversion_rate</span><span class="p">)</span> <span class="o">/</span> <span class="n">control_conversion_rate</span>
    
    <span class="c1"># --- Perform Welch&#39;s T-test ---</span>
    <span class="c1"># More robust than standard T-test as it doesn&#39;t assume equal variance</span>
    <span class="n">t_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span>
        <span class="n">treatment_group</span><span class="p">[</span><span class="s1">&#39;converted&#39;</span><span class="p">],</span> 
        <span class="n">control_group</span><span class="p">[</span><span class="s1">&#39;converted&#39;</span><span class="p">],</span> 
        <span class="n">equal_var</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    
    <span class="c1"># --- Print Results ---</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- A/B Test Analysis Report ---&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Control (Champion) Conversion Rate: </span><span class="si">{</span><span class="n">control_conversion_rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Treatment (Challenger) Conversion Rate: </span><span class="si">{</span><span class="n">treatment_conversion_rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Relative Lift: </span><span class="si">{</span><span class="n">lift</span><span class="si">:</span><span class="s2">+.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">P-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span> <span class="c1"># Using alpha = 0.05</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Result: Statistically Significant. The challenger model is the winner.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Result: Not Statistically Significant. We cannot conclude the challenger is better.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--data-path&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">analyze_experiment_results</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id25">
<h4>Pipeline: Airflow DAG<a class="headerlink" href="#id25" title="Permalink to this heading">¶</a></h4>
<p>This DAG is for analyzing the results after the A/B test has run for its designated period.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s1">&#39;ab_test_analysis&#39;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">schedule_interval</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># Triggered manually</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;analysis&#39;</span><span class="p">,</span> <span class="s1">&#39;ab-testing&#39;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    
    <span class="n">run_analysis</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;run_statistical_analysis&#39;</span><span class="p">,</span>
        <span class="n">bash_command</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;python /opt/airflow/src/analysis/run_ab_test_analysis.py &quot;</span>
            <span class="s2">&quot;--data-path s3://ecom-propensity-analytics/ab-test-results/experiment_XYZ.parquet&quot;</span>
        <span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="ci-cd-github-actions-workflow">
<h4>CI/CD GitHub Actions Workflow<a class="headerlink" href="#ci-cd-github-actions-workflow" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Start A/B Test on SageMaker</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">workflow_dispatch</span><span class="p">:</span>
<span class="w">    </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">      </span><span class="nt">champion_model_name</span><span class="p">:</span>
<span class="w">        </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Name</span><span class="nv"> </span><span class="s">of</span><span class="nv"> </span><span class="s">the</span><span class="nv"> </span><span class="s">production</span><span class="nv"> </span><span class="s">(champion)</span><span class="nv"> </span><span class="s">SageMaker</span><span class="nv"> </span><span class="s">model&#39;</span>
<span class="w">        </span><span class="nt">required</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">      </span><span class="nt">challenger_model_name</span><span class="p">:</span>
<span class="w">        </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Name</span><span class="nv"> </span><span class="s">of</span><span class="nv"> </span><span class="s">the</span><span class="nv"> </span><span class="s">new</span><span class="nv"> </span><span class="s">(challenger)</span><span class="nv"> </span><span class="s">SageMaker</span><span class="nv"> </span><span class="s">model&#39;</span>
<span class="w">        </span><span class="nt">required</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">      </span><span class="nt">challenger_traffic_split</span><span class="p">:</span>
<span class="w">        </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Traffic</span><span class="nv"> </span><span class="s">%</span><span class="nv"> </span><span class="s">for</span><span class="nv"> </span><span class="s">challenger</span><span class="nv"> </span><span class="s">(e.g.,</span><span class="nv"> </span><span class="s">50</span><span class="nv"> </span><span class="s">for</span><span class="nv"> </span><span class="s">a</span><span class="nv"> </span><span class="s">50/50</span><span class="nv"> </span><span class="s">split)&#39;</span>
<span class="w">        </span><span class="nt">required</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">        </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;50&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">deploy-ab-test</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deploy Challenger Variant for A/B Test</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">permissions</span><span class="p">:</span>
<span class="w">      </span><span class="nt">id-token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">write</span>
<span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">read</span>

<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Checkout code</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Configure AWS credentials</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">role-to-assume</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-deploy-role</span>
<span class="w">          </span><span class="nt">aws-region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">eu-west-1</span>
<span class="w">      </span>
<span class="w">      </span><span class="c1"># NOTE: Assumes the challenger model&#39;s container is already built and pushed to ECR</span>
<span class="w">      </span><span class="c1"># This workflow only handles the infrastructure update to start the test.</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Setup Terraform</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hashicorp/setup-terraform@v2</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Terraform Apply to Start A/B Test</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd infrastructure</span>
<span class="w">          </span><span class="no">terraform init</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve \</span>
<span class="w">            </span><span class="no">-var=&quot;champion_model_name=${{ github.event.inputs.champion_model_name }}&quot; \</span>
<span class="w">            </span><span class="no">-var=&quot;challenger_model_name=${{ github.event.inputs.challenger_model_name }}&quot; \</span>
<span class="w">            </span><span class="no">-var=&quot;challenger_weight=${{ github.event.inputs.challenger_traffic_split }}&quot;</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run Smoke Test on BOTH Variants</span>
<span class="w">        </span><span class="c1"># This is a more advanced integration test that would need to be written.</span>
<span class="w">        </span><span class="c1"># It would invoke the endpoint multiple times, checking the headers</span>
<span class="w">        </span><span class="c1"># to confirm it gets responses from both the &#39;champion&#39; and &#39;challenger&#39; variants.</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">echo &quot;SMOKE TEST</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Verifying both variants are healthy...&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="the-continual-learning-monitoring-loop">
<h3>The Continual Learning &amp; Monitoring Loop<a class="headerlink" href="#the-continual-learning-monitoring-loop" title="Permalink to this heading">¶</a></h3>
<section id="id26">
<h4>Architecture<a class="headerlink" href="#id26" title="Permalink to this heading">¶</a></h4>
<p>This architecture illustrates the closed-loop system. A daily monitoring job checks for drift. If drift is detected, it triggers an automated retraining workflow. This workflow produces a new model candidate, which is then safely deployed via a multi-stage canary release process, with automated analysis and rollback capabilities.</p>
<img src="../_static/past_experiences/ecom_propensity/continual_learning.png" width="100%" style="background-color: #FCF1EF;"/>
</section>
<section id="daily-monitoring-and-drift-detection-artifacts">
<h4>Daily Monitoring and Drift Detection Artifacts<a class="headerlink" href="#daily-monitoring-and-drift-detection-artifacts" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">evidently.report</span> <span class="kn">import</span> <span class="n">Report</span>
<span class="kn">from</span> <span class="nn">evidently.metric_preset</span> <span class="kn">import</span> <span class="n">DataDriftPreset</span><span class="p">,</span> <span class="n">TargetDriftPreset</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%(asctime)-15s</span><span class="s2"> </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">run_drift_analysis</span><span class="p">(</span><span class="n">ref_data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">prod_data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">report_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compares production data to reference data to detect drift.</span>
<span class="sd">    Exits with a non-zero status code if drift is detected.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loading reference data from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ref_data_path</span><span class="p">)</span>
    <span class="n">ref_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">ref_data_path</span><span class="p">)</span>
    
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loading production data from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">prod_data_path</span><span class="p">)</span>
    <span class="n">prod_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">prod_data_path</span><span class="p">)</span>

    <span class="c1"># For this example, let&#39;s assume prediction is &#39;propensity&#39; and target is &#39;converted&#39;</span>
    <span class="n">ref_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="s1">&#39;converted&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Generating data drift report...&quot;</span><span class="p">)</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">Report</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span>
        <span class="n">DataDriftPreset</span><span class="p">(),</span>
        <span class="n">TargetDriftPreset</span><span class="p">(),</span>
    <span class="p">])</span>
    <span class="n">report</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">reference_data</span><span class="o">=</span><span class="n">ref_df</span><span class="p">,</span> <span class="n">current_data</span><span class="o">=</span><span class="n">prod_df</span><span class="p">)</span>
    
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Saving drift report to </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">report_path</span><span class="p">)</span>
    <span class="n">report</span><span class="o">.</span><span class="n">save_html</span><span class="p">(</span><span class="n">report_path</span><span class="p">)</span>

    <span class="n">drift_report</span> <span class="o">=</span> <span class="n">report</span><span class="o">.</span><span class="n">as_dict</span><span class="p">()</span>
    <span class="n">is_drift_detected</span> <span class="o">=</span> <span class="n">drift_report</span><span class="p">[</span><span class="s1">&#39;metrics&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;result&#39;</span><span class="p">][</span><span class="s1">&#39;dataset_drift&#39;</span><span class="p">]</span>
    
    <span class="k">if</span> <span class="n">is_drift_detected</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Data drift detected! The production data distribution has shifted significantly.&quot;</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Fail the task if drift is found</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No significant data drift detected.&quot;</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--ref-data&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--prod-data&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--report-path&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">run_drift_analysis</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ref_data</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">prod_data</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">report_path</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="monitoring-dag">
<h4>Monitoring DAG<a class="headerlink" href="#monitoring-dag" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span>
<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span>

<span class="k">def</span> <span class="nf">on_drift_detection_failure</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called when the drift check fails.</span>
<span class="sd">    It triggers the GitHub Actions workflow for retraining.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># In a real system, you&#39;d use a more robust method like calling the GitHub API</span>
    <span class="c1"># from a PythonOperator. For clarity, a curl command is shown.</span>
    <span class="c1"># The GITHUB_TOKEN would be stored as an Airflow secret.</span>
    <span class="n">trigger_workflow_command</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    curl -X POST </span><span class="se">\</span>
<span class="s2">      -H &quot;Accept: application/vnd.github.v3+json&quot; </span><span class="se">\</span>
<span class="s2">      -H &quot;Authorization: token {{ conn.github_pat.password }}&quot; </span><span class="se">\</span>
<span class="s2">      https://api.github.com/repos/your-org/ecom-propensity/actions/workflows/retrain_and_deploy.yml/dispatches </span><span class="se">\</span>
<span class="s2">      -d &#39;{&quot;ref&quot;:&quot;main&quot;, &quot;inputs&quot;:{&quot;trigger_reason&quot;:&quot;data_drift&quot;}}&#39;</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">BashOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;trigger_retraining_workflow&#39;</span><span class="p">,</span>
        <span class="n">bash_command</span><span class="o">=</span><span class="n">trigger_workflow_command</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s1">&#39;daily_monitoring_and_drift_check&#39;</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s1">&#39;@daily&#39;</span><span class="p">,</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;monitoring&#39;</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    
    <span class="n">run_drift_check</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;run_drift_check&#39;</span><span class="p">,</span>
        <span class="n">bash_command</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;python /opt/airflow/src/monitoring/run_drift_check.py &quot;</span>
            <span class="s2">&quot;--ref-data s3://ecom-propensity-gold-features/training_data_profile.parquet &quot;</span>
            <span class="s2">&quot;--prod-data s3://ecom-propensity-monitoring-logs/latest/ &quot;</span>
            <span class="s2">&quot;--report-path /tmp/drift_report_{{ ds }}.html&quot;</span>
        <span class="p">),</span>
        <span class="n">on_failure_callback</span><span class="o">=</span><span class="n">on_drift_detection_failure</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="automated-retraining-and-canary-release-artifacts">
<h4>Automated Retraining and Canary Release Artifacts<a class="headerlink" href="#automated-retraining-and-canary-release-artifacts" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>
<span class="kn">import</span> <span class="nn">boto3</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%(asctime)-15s</span><span class="s2"> </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>

<span class="c1"># Define acceptable performance thresholds for the challenger</span>
<span class="n">MAX_LATENCY_INCREASE_FACTOR</span> <span class="o">=</span> <span class="mf">1.10</span>  <span class="c1"># Allow 10% higher latency</span>
<span class="n">MAX_ERROR_RATE_ABSOLUTE</span> <span class="o">=</span> <span class="mf">0.01</span>      <span class="c1"># Max 1% error rate</span>

<span class="k">def</span> <span class="nf">analyze_canary_metrics</span><span class="p">(</span><span class="n">endpoint_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">bake_time_mins</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Queries CloudWatch metrics for champion and challenger variants and compares them.</span>
<span class="sd">    Returns True if the challenger is healthy, False otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;cloudwatch&quot;</span><span class="p">,</span> <span class="n">region_name</span><span class="o">=</span><span class="s2">&quot;eu-west-1&quot;</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="n">bake_time_mins</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_metric</span><span class="p">(</span><span class="n">variant_name</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">):</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_metric_data</span><span class="p">(</span>
            <span class="n">MetricDataQueries</span><span class="o">=</span><span class="p">[{</span>
                <span class="s1">&#39;Id&#39;</span><span class="p">:</span> <span class="s1">&#39;m1&#39;</span><span class="p">,</span>
                <span class="s1">&#39;MetricStat&#39;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s1">&#39;Metric&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;Namespace&#39;</span><span class="p">:</span> <span class="s1">&#39;AWS/SageMaker&#39;</span><span class="p">,</span> <span class="s1">&#39;MetricName&#39;</span><span class="p">:</span> <span class="n">metric_name</span><span class="p">,</span>
                               <span class="s1">&#39;Dimensions&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;Name&#39;</span><span class="p">:</span> <span class="s1">&#39;EndpointName&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">:</span> <span class="n">endpoint_name</span><span class="p">},</span>
                                              <span class="p">{</span><span class="s1">&#39;Name&#39;</span><span class="p">:</span> <span class="s1">&#39;VariantName&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">:</span> <span class="n">variant_name</span><span class="p">}]},</span>
                    <span class="s1">&#39;Period&#39;</span><span class="p">:</span> <span class="mi">60</span><span class="p">,</span> <span class="s1">&#39;Stat&#39;</span><span class="p">:</span> <span class="s1">&#39;Average&#39;</span>
                <span class="p">},</span> <span class="s1">&#39;ReturnData&#39;</span><span class="p">:</span> <span class="kc">True</span>
            <span class="p">}],</span>
            <span class="n">StartTime</span><span class="o">=</span><span class="n">start_time</span><span class="p">,</span> <span class="n">EndTime</span><span class="o">=</span><span class="n">end_time</span>
        <span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;MetricDataResults&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;Values&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">values</span> <span class="k">else</span> <span class="mf">0.0</span>

    <span class="c1"># --- Fetch metrics for both variants ---</span>
    <span class="n">champion_latency</span> <span class="o">=</span> <span class="n">get_metric</span><span class="p">(</span><span class="s2">&quot;champion&quot;</span><span class="p">,</span> <span class="s2">&quot;ModelLatency&quot;</span><span class="p">)</span>
    <span class="n">challenger_latency</span> <span class="o">=</span> <span class="n">get_metric</span><span class="p">(</span><span class="s2">&quot;challenger&quot;</span><span class="p">,</span> <span class="s2">&quot;ModelLatency&quot;</span><span class="p">)</span>
    <span class="n">challenger_errors</span> <span class="o">=</span> <span class="n">get_metric</span><span class="p">(</span><span class="s2">&quot;challenger&quot;</span><span class="p">,</span> <span class="s2">&quot;Invocation5XXErrors&quot;</span><span class="p">)</span>
    
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Champion Latency: </span><span class="si">{</span><span class="n">champion_latency</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Challenger Latency: </span><span class="si">{</span><span class="n">challenger_latency</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Challenger Errors (avg per min): </span><span class="si">{</span><span class="n">challenger_errors</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># --- Compare against thresholds ---</span>
    <span class="n">healthy</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">challenger_latency</span> <span class="o">&gt;</span> <span class="n">champion_latency</span> <span class="o">*</span> <span class="n">MAX_LATENCY_INCREASE_FACTOR</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;FAIL: Challenger latency is unacceptably high.&quot;</span><span class="p">)</span>
        <span class="n">healthy</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">challenger_errors</span> <span class="o">&gt;</span> <span class="n">MAX_ERROR_RATE_ABSOLUTE</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;FAIL: Challenger error rate is unacceptably high.&quot;</span><span class="p">)</span>
        <span class="n">healthy</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">healthy</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;SUCCESS: Challenger performance is within acceptable limits.&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">healthy</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--endpoint-name&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--bake-time-mins&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">analyze_canary_metrics</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">endpoint_name</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">bake_time_mins</span><span class="p">):</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="retraining-and-canary-deployment-workflow">
<h4>Retraining and Canary Deployment Workflow<a class="headerlink" href="#retraining-and-canary-deployment-workflow" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Triggered Retraining and Canary Deployment</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">workflow_dispatch</span><span class="p">:</span>
<span class="w">    </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">      </span><span class="nt">trigger_reason</span><span class="p">:</span>
<span class="w">        </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Reason</span><span class="nv"> </span><span class="s">for</span><span class="nv"> </span><span class="s">triggering</span><span class="nv"> </span><span class="s">the</span><span class="nv"> </span><span class="s">run</span><span class="nv"> </span><span class="s">(e.g.,</span><span class="nv"> </span><span class="s">data_drift,</span><span class="nv"> </span><span class="s">scheduled)&#39;</span>
<span class="w">        </span><span class="nt">required</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">        </span><span class="nt">default</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;manual&#39;</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">start-training-pipeline</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Start Model Retraining</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">outputs</span><span class="p">:</span>
<span class="w">      </span><span class="nt">new_model_version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ steps.check_mlflow.outputs.version }}</span>

<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Trigger Airflow Training DAG</span>
<span class="w">        </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">trigger_dag</span>
<span class="w">        </span><span class="c1"># ... (logic to call Airflow API to start &#39;model_training_pipeline&#39; DAG) ...</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">echo &quot;DAG run started...&quot;</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Wait for Training DAG to Complete</span>
<span class="w">        </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wait_dag</span>
<span class="w">        </span><span class="c1"># ... (logic to poll Airflow API until DAG run is successful) ...</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">echo &quot;DAG run finished successfully.&quot;</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Check MLflow for New Staging Model</span>
<span class="w">        </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">check_mlflow</span>
<span class="w">        </span><span class="c1"># ... (Python script to check MLflow Registry for a new model in &#39;Staging&#39;) ...</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no"># This script would output the new version number</span>
<span class="w">          </span><span class="no">echo &quot;version=6&quot; &gt;&gt; $GITHUB_OUTPUT</span>

<span class="w">  </span><span class="nt">canary-release</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Canary Release to Production</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">needs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">start-training-pipeline</span>
<span class="w">    </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">needs.start-training-pipeline.outputs.new_model_version != &#39;&#39;</span>

<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v3</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws-actions/configure-aws-credentials@v2</span>
<span class="w">        </span><span class="c1"># ... (credentials config) ...</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hashicorp/setup-terraform@v2</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Step</span><span class="nv"> </span><span class="s">1:</span><span class="nv"> </span><span class="s">Canary</span><span class="nv"> </span><span class="s">Deploy</span><span class="nv"> </span><span class="s">(10%</span><span class="nv"> </span><span class="s">Traffic)&#39;</span>
<span class="w">        </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">canary_deploy</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">cd infrastructure</span>
<span class="w">          </span><span class="no">terraform init</span>
<span class="w">          </span><span class="no"># This assumes a script that gets champion/challenger names from MLflow</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;challenger_weight=10&quot; ...</span>
<span class="w">      </span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Step</span><span class="nv"> </span><span class="s">2:</span><span class="nv"> </span><span class="s">Bake</span><span class="nv"> </span><span class="s">Period&#39;</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sleep 900</span><span class="w"> </span><span class="c1"># Wait 15 minutes</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Step</span><span class="nv"> </span><span class="s">3:</span><span class="nv"> </span><span class="s">Automated</span><span class="nv"> </span><span class="s">Canary</span><span class="nv"> </span><span class="s">Analysis&#39;</span>
<span class="w">        </span><span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">analysis</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">pip install boto3</span>
<span class="w">          </span><span class="no">python src/deployment/canary_analysis.py --endpoint-name propensity-endpoint</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Step</span><span class="nv"> </span><span class="s">4:</span><span class="nv"> </span><span class="s">Promote</span><span class="nv"> </span><span class="s">(if</span><span class="nv"> </span><span class="s">analysis</span><span class="nv"> </span><span class="s">passed)&#39;</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">steps.analysis.outcome == &#39;success&#39;</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">echo &quot;Canary analysis passed. Promoting to 100% traffic.&quot;</span>
<span class="w">          </span><span class="no">cd infrastructure</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;challenger_weight=100&quot; ...</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;Step</span><span class="nv"> </span><span class="s">5:</span><span class="nv"> </span><span class="s">Rollback</span><span class="nv"> </span><span class="s">(if</span><span class="nv"> </span><span class="s">analysis</span><span class="nv"> </span><span class="s">failed)&#39;</span>
<span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">steps.analysis.outcome != &#39;success&#39;</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">          </span><span class="no">echo &quot;::error::Canary analysis failed! Rolling back.&quot;</span>
<span class="w">          </span><span class="no">cd infrastructure</span>
<span class="w">          </span><span class="no">terraform apply -auto-approve -var=&quot;challenger_weight=0&quot; ...</span>
<span class="w">          </span><span class="no">exit 1</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="ecom_summarisation.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Reviews Summarisation</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="ecom_cltv.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Customer Lifetime Value</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, Deepak Karkala
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Real-Time Purchase Intent Scoring</a><ul>
<li><a class="reference internal" href="#id1"></a><ul>
<li><a class="reference internal" href="#tldr-a-production-grade-mlops-system-for-real-time-purchase-intent-scoring"><strong>TLDR: A Production-Grade MLOps System for Real-Time Purchase Intent Scoring</strong></a></li>
<li><a class="reference internal" href="#business-challenge-from-anonymous-clicks-to-intent-driven-conversions"><strong>1. Business Challenge: From Anonymous Clicks to Intent-Driven Conversions</strong></a></li>
<li><a class="reference internal" href="#ml-problem-framing">2. ML Problem Framing</a><ul>
<li><a class="reference internal" href="#setting-the-business-objectives">2.1 Setting the Business Objectives</a></li>
<li><a class="reference internal" href="#is-machine-learning-the-right-approach">2.2 Is Machine Learning the Right Approach?</a></li>
<li><a class="reference internal" href="#defining-the-ml-problem">2.3 Defining the ML Problem</a></li>
<li><a class="reference internal" href="#assessing-feasibility-risks">2.4 Assessing Feasibility &amp; Risks</a></li>
<li><a class="reference internal" href="#defining-success-metrics">2.5 Defining Success Metrics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mlops-project-planning-and-operational-strategy">3. MLOps Project Planning and Operational Strategy</a><ul>
<li><a class="reference internal" href="#the-mlops-first-mindset-building-a-system-not-just-a-model">3.1 The MLOps-First Mindset: Building a System, Not Just a Model</a></li>
<li><a class="reference internal" href="#the-architectural-triad-balancing-offline-nearline-and-online-computation">3.2 The Architectural Triad: Balancing Offline, Nearline, and Online Computation</a></li>
<li><a class="reference internal" href="#the-mlops-stack-canvas-technology-stack-selection">3.3 The MLOps Stack Canvas: Technology Stack Selection</a></li>
<li><a class="reference internal" href="#core-mlops-pipelines-and-workflows">3.4 Core MLOps Pipelines and Workflows</a></li>
<li><a class="reference internal" href="#project-stages-and-timeline">3.5.1 Project Stages and Timeline</a></li>
<li><a class="reference internal" href="#cross-functional-team-roles">3.5.2 Cross-Functional Team &amp; Roles</a></li>
<li><a class="reference internal" href="#versioning-and-governance-strategy">3.5.3 Versioning and Governance Strategy</a></li>
<li><a class="reference internal" href="#a-comprehensive-ml-testing-strategy-the-mlops-crucible">3.6 A Comprehensive ML Testing Strategy: The MLOps Crucible</a></li>
</ul>
</li>
<li><a class="reference internal" href="#data-sourcing-discovery-and-characteristics">4. Data Sourcing, Discovery, and Characteristics</a><ul>
<li><a class="reference internal" href="#data-sourcing-discovery-plan">4.1 Data Sourcing &amp; Discovery Plan</a></li>
<li><a class="reference internal" href="#key-technical-considerations-for-implementation">4.3 Key Technical Considerations for Implementation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#data-engineering-and-pipelines">5. Data Engineering and Pipelines</a><ul>
<li><a class="reference internal" href="#the-data-engineering-lifecycle-from-raw-data-to-ml-ready-features">5.1 The Data Engineering Lifecycle: From Raw Data to ML-Ready Features</a></li>
<li><a class="reference internal" href="#real-time-streaming-pipeline-design-architecture">5.2 Real-Time Streaming Pipeline: Design &amp; Architecture</a><ul>
<li><a class="reference internal" href="#core-architecture">5.2.1 Core Architecture</a></li>
<li><a class="reference internal" href="#key-challenges-and-solutions-for-real-time-feature-engineering">5.2.2 Key Challenges and Solutions for Real-Time Feature Engineering</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-do-we-choose-the-optimal-trigger-interval-for-our-spark-structured-streaming-job">5.3 How do we choose the optimal Trigger Interval for our Spark Structured Streaming job?</a><ul>
<li><a class="reference internal" href="#factors-influencing-the-trigger-interval-choice">Factors Influencing the Trigger Interval Choice</a></li>
<li><a class="reference internal" href="#summary-of-trade-offs">Summary of Trade-offs</a></li>
<li><a class="reference internal" href="#recommendation-for-our-project">Recommendation for Our Project</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#feature-engineering-and-pipelines-crafting-the-predictive-signals">6. Feature Engineering and Pipelines: Crafting the Predictive Signals</a><ul>
<li><a class="reference internal" href="#feature-engineering-lifecycle-and-strategy">6.1 Feature Engineering Lifecycle and Strategy</a></li>
<li><a class="reference internal" href="#a-lexicon-of-features-for-purchase-intent">6.2 A Lexicon of Features for Purchase Intent</a></li>
<li><a class="reference internal" href="#architecting-the-feature-engineering-pipelines">6.3 Architecting the Feature Engineering Pipelines</a><ul>
<li><a class="reference internal" href="#the-daily-batch-feature-pipeline">6.3.1 The Daily Batch Feature Pipeline</a></li>
<li><a class="reference internal" href="#the-real-time-streaming-feature-pipeline">6.3.2 The Real-Time Streaming Feature Pipeline</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#model-development-iteration">7. Model Development &amp; Iteration</a><ul>
<li><a class="reference internal" href="#foundations-for-success-the-modeling-blueprint">7.1 Foundations for Success: The Modeling Blueprint</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ml-training-pipelines">8. ML Training Pipelines</a><ul>
<li><a class="reference internal" href="#training-pipeline-design-and-architecture">8.1 Training Pipeline Design and Architecture</a><ul>
<li><a class="reference internal" href="#architecture-diagram"><strong>Architecture Diagram</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pipeline-components-and-implementation-plan">8.2 Pipeline Components and Implementation Plan</a></li>
<li><a class="reference internal" href="#artifacts-to-be-implemented">8.3 Artifacts to be Implemented</a></li>
</ul>
</li>
<li><a class="reference internal" href="#deployment-serving-and-inference">9. Deployment, Serving, and Inference</a><ul>
<li><a class="reference internal" href="#overarching-deployment-and-serving-strategy">9.1 Overarching Deployment and Serving Strategy</a></li>
<li><a class="reference internal" href="#pre-deployment-preparations-packaging-the-model-for-serving">9.2 Pre-Deployment Preparations: Packaging the Model for Serving</a></li>
<li><a class="reference internal" href="#the-real-time-inference-pipeline">9.3 The Real-Time Inference Pipeline</a><ul>
<li><a class="reference internal" href="#id2"><strong>Architecture Diagram</strong></a></li>
<li><a class="reference internal" href="#latency-budget-p99-100ms"><strong>Latency Budget (p99 &lt; 100ms)</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-plan-for-the-inference-system-artifacts">9.4 Implementation Plan for the Inference System Artifacts</a></li>
</ul>
</li>
<li><a class="reference internal" href="#monitoring-observability-and-model-evolution">10. Monitoring, Observability, and Model Evolution</a><ul>
<li><a class="reference internal" href="#monitoring-and-observability-plan">10.1 Monitoring and Observability Plan</a></li>
</ul>
</li>
<li><a class="reference internal" href="#continual-learning-production-testing-evolving-the-model">11. Continual Learning &amp; Production Testing: Evolving the Model</a><ul>
<li><a class="reference internal" href="#the-imperative-to-evolve-triggers-for-model-updates">11.1 The Imperative to Evolve: Triggers for Model Updates</a></li>
<li><a class="reference internal" href="#retraining-and-data-curation-strategy">11.2 Retraining and Data Curation Strategy</a></li>
<li><a class="reference internal" href="#production-testing-the-a-b-testing-framework">11.3 Production Testing: The A/B Testing Framework</a></li>
<li><a class="reference internal" href="#addressing-advanced-challenges">11.4 Addressing Advanced Challenges</a></li>
</ul>
</li>
<li><a class="reference internal" href="#governance-ethics-the-human-element">12. Governance, Ethics &amp; The Human Element</a><ul>
<li><a class="reference internal" href="#comprehensive-model-governance-plan">12.1 Comprehensive Model Governance Plan</a></li>
<li><a class="reference internal" href="#responsible-ai-rai-by-design">12.2 Responsible AI (RAI) by Design</a></li>
<li><a class="reference internal" href="#holistic-testing-a-production-readiness-assessment">12.3 Holistic Testing: A Production Readiness Assessment</a></li>
<li><a class="reference internal" href="#the-human-element-team-user-experience">12.4 The Human Element: Team &amp; User Experience</a></li>
</ul>
</li>
<li><a class="reference internal" href="#overall-system-architecture">13. Overall System Architecture</a><ul>
<li><a class="reference internal" href="#a-unified-architectural-blueprint">13.1 A Unified Architectural Blueprint</a></li>
<li><a class="reference internal" href="#real-time-inference-sequence-diagram">13.2 Real-Time Inference Sequence Diagram</a></li>
<li><a class="reference internal" href="#potential-bottlenecks-and-performance-optimizations">13.3 Potential Bottlenecks and Performance Optimizations</a></li>
<li><a class="reference internal" href="#estimated-monthly-costs">13.4 Estimated Monthly Costs</a></li>
<li><a class="reference internal" href="#deep-dive-calculating-inference-instance-requirements">13.5 Deep Dive: Calculating Inference Instance Requirements</a><ul>
<li><a class="reference internal" href="#the-core-factors-performance-equation">13.5.1 The Core Factors &amp; Performance Equation</a></li>
<li><a class="reference internal" href="#performance-optimization-strategies">13.5.2 Performance Optimization Strategies</a></li>
<li><a class="reference internal" href="#bottom-up-calculation-throughput-of-a-single-instance">13.5.3 Bottom-Up Calculation: Throughput of a Single Instance</a></li>
<li><a class="reference internal" href="#scaling-analysis-instances-and-costs-by-request-volume">13.5.4 Scaling Analysis: Instances and Costs by Request Volume</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#code-implementation">Code Implementation</a><ul>
<li><a class="reference internal" href="#data-ingestion-pipeline">Data Ingestion Pipeline</a><ul>
<li><a class="reference internal" href="#architecture">Architecture</a></li>
</ul>
</li>
<li><a class="reference internal" href="#feature-engineering-batch">Feature Engineering: Batch</a><ul>
<li><a class="reference internal" href="#id3">Architecture</a></li>
<li><a class="reference internal" href="#iac-terraform">IaC (Terraform)</a></li>
<li><a class="reference internal" href="#python-scripts">Python Scripts</a></li>
<li><a class="reference internal" href="#unit-tests">Unit Tests</a></li>
<li><a class="reference internal" href="#pipeline-airflow-dag">Pipeline (Airflow DAG)</a></li>
<li><a class="reference internal" href="#integration-tests">Integration Tests</a></li>
<li><a class="reference internal" href="#how-and-when-the-test-is-run"><strong>1. How and When the Test is Run</strong></a></li>
<li><a class="reference internal" href="#required-setup-prerequisites"><strong>2. Required Setup (Prerequisites)</strong></a></li>
<li><a class="reference internal" href="#ci-cd-workflow">CI/CD Workflow</a></li>
<li><a class="reference internal" href="#the-two-emr-cluster-patterns">The Two EMR Cluster Patterns</a></li>
</ul>
</li>
<li><a class="reference internal" href="#what-is-the-emr-tf-file-really-for">What is the <code class="docutils literal notranslate"><span class="pre">emr.tf</span></code> file <em>really</em> for?</a></li>
<li><a class="reference internal" href="#summary-table">Summary Table</a></li>
<li><a class="reference internal" href="#feature-engineering-streaming-pipeline">Feature Engineering: Streaming Pipeline</a><ul>
<li><a class="reference internal" href="#id4">Architecture</a></li>
<li><a class="reference internal" href="#id5">Python Scripts</a></li>
<li><a class="reference internal" href="#id6">Unit Tests</a></li>
<li><a class="reference internal" href="#id7">Pipeline (Airflow DAG)</a></li>
<li><a class="reference internal" href="#id8">Integration Tests</a></li>
<li><a class="reference internal" href="#id9">CI/CD Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-training-pipeline">Model Training pipeline</a><ul>
<li><a class="reference internal" href="#id10">Architecture</a></li>
<li><a class="reference internal" href="#id11">IaC (Terraform)</a></li>
<li><a class="reference internal" href="#id12">Python Scripts</a></li>
<li><a class="reference internal" href="#id13">Unit Tests</a></li>
<li><a class="reference internal" href="#id14">Pipeline (Airflow DAG)</a></li>
<li><a class="reference internal" href="#id15">Integration Tests</a></li>
<li><a class="reference internal" href="#id16">CI/CD Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id17">Feature Engineering: Batch</a><ul>
<li><a class="reference internal" href="#id18">Architecture</a></li>
<li><a class="reference internal" href="#id19">Python Scripts</a></li>
<li><a class="reference internal" href="#id20">Unit Tests</a></li>
<li><a class="reference internal" href="#id21">IaC (Terraform)</a></li>
<li><a class="reference internal" href="#id22">Integration Tests</a></li>
<li><a class="reference internal" href="#api-contract-smoke-test">API Contract &amp; Smoke Test</a></li>
<li><a class="reference internal" href="#id23">CI/CD Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-load-testing">Performance &amp; Load Testing</a></li>
<li><a class="reference internal" href="#a-b-testing">A/B Testing</a><ul>
<li><a class="reference internal" href="#id24">Architecture</a></li>
<li><a class="reference internal" href="#iac">IaC</a></li>
<li><a class="reference internal" href="#analysis-script">Analysis Script</a></li>
<li><a class="reference internal" href="#id25">Pipeline: Airflow DAG</a></li>
<li><a class="reference internal" href="#ci-cd-github-actions-workflow">CI/CD GitHub Actions Workflow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-continual-learning-monitoring-loop">The Continual Learning &amp; Monitoring Loop</a><ul>
<li><a class="reference internal" href="#id26">Architecture</a></li>
<li><a class="reference internal" href="#daily-monitoring-and-drift-detection-artifacts">Daily Monitoring and Drift Detection Artifacts</a></li>
<li><a class="reference internal" href="#monitoring-dag">Monitoring DAG</a></li>
<li><a class="reference internal" href="#automated-retraining-and-canary-release-artifacts">Automated Retraining and Canary Release Artifacts</a></li>
<li><a class="reference internal" href="#retraining-and-canary-deployment-workflow">Retraining and Canary Deployment Workflow</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/scripts/furo.js?v=4e2eecee"></script>
    </body>
</html>