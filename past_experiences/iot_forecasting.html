<!doctype html>
<html class="no-js" lang="en" data-content_root="">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="ADAS: Data Engine" href="adas_engine/index.html" /><link rel="prev" title="Anomaly Detection in Time Series IoT Data" href="iot_anomaly.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><!-- Generated with Sphinx 7.1.2 and Furo 2024.05.06 -->
        <title>Energy Demand Forecasting in Time Series IoT Data - Home</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=387cc868" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/style.css?v=8a7ff5ee" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" /
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Home</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">Home</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Past Experiences</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Past Experiences</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="iot_anomaly.html">Anomaly Detection in Time Series IoT Data</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Energy Demand Forecasting in Time Series IoT Data</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="adas_engine/index.html">ADAS: Data Engine</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of ADAS: Data Engine</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch0_business_challenge.html">Business Challenge and Goals</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch1_ml_problem_framing.html">ML Problem Framing</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch2_operational_strategy.html">Planning, Operational Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch3_pipelines_workflows.html">Workflows, Team, Roles</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch4_testing_strategy.html">Testing Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch6_data_ingestion_workflows.html">Data Ingestion Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch7_scene_understanding_data_mining.html">Scene Understanding &amp; Data Mining</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch8_model_training.html">Model Training &amp; Experimentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch9_packaging_promotion.html">Packaging, Evaluation &amp; Promotion Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch10_deployment_serving.html">Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch11_monitoring_continual_learning.html">Monitoring &amp; Continual Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch12_cost_lifecycle_compliance.html">Cost, Lifecycle, Compliance</a></li>
<li class="toctree-l3"><a class="reference internal" href="adas_engine/ch13_reliability_capacity_maps.html">Reliability, Capacity, Maps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ecom_cltv.html">Customer Lifetime Value</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_propensity.html">Real-Time Purchase Intent Scoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_summarisation.html">Reviews Summarisation</a></li>
<li class="toctree-l2"><a class="reference internal" href="ecom_rag.html">RAG-Based Product Discovery</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../projects/index.html">Projects</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Projects</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/nlp/index.html">Natural Language Processing</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Natural Language Processing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/airbnb_alternate_search/about/index.html">Airbnb Listing description based Semantic Search</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/cv/index.html">Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Computer Vision</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/ecommerce_image_segmentation/about/index.html">Image Segmentation for Ecommerce Products</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../projects/ml/index.html">Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Machine Learning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.deepakkarkala.com/docs/articles/machine_learning/airbnb_price_modeling/about/index.html">Predictive Price Modeling for Airbnb listings</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications/index.html">Patents, Papers, Thesis</a></li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../agents/index.html">AI Agents: A Lead Engineer’s Handbook</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of AI Agents: A Lead Engineer’s Handbook</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch1_intro.html">Agent Fundamentals: What, Why, and When?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch2_patterns.html">Agentic Patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch5_context_engineering.html">Context Engineering for AI Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch6_case_studies.html">The State of the Industry: Insights from the Field</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch7_conclusion.html"><strong>Conclusion: The Lead Engineer’s Mental Model for Building Agents</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_cost.html">Cost Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_data.html">Data Management and Knowledge Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_deploy.html">Deployment and Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_guardrails.html">Guardrails</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_hitl.html">Human-in-the-Loop (HITL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_latency.html">Latency Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_llm.html">LLM – Prompts, Goals, and Persona</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_memory.html">Managing Agent Memory (Short-Term and Long-Term)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_monitor.html">Monitoring and Observability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_orchestration.html">Orchestration and Task Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_prod.html">Production Challenges and Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_security.html">Securing AI Agents and Preventing Abuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_tool.html">Tool Use and Integration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/ch_trust.html">Building Trustworthy and Ethical AI Agents</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../mlops/index.html">MLOps</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of MLOps</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch1_problem_framing.html">ML Problem framing</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of ML Problem framing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/ch2_blueprint_operational_strategy.html">The MLOps Blueprint &amp; Operational Strategy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch2a_platform/index.html">ML Platforms</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of ML Platforms</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/ml_platforms.html">ML Platforms: How to</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/uber.html">Uber Michelangelo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/linkedin.html">LinkedIn DARWIN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/netflix.html">Netflix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/shopify.html">Shopify Merlin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/zomato.html">Zomato: Real-time ML</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/coveo.html">Coveo: MLOPs at reasonable scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/monzo.html">Monzo ML Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch2a_platform/didact.html">Didact AI</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch3_project_planning/index.html">Project Planning</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of Project Planning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/prd.html">Project Requirements Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/tech_stack.html">Tech Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/config_management.html">Config Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/pipeline_design.html">Pipeline Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/environment_strategy.html">Environment Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/cicd_branching_model.html">CI/CD Strategy and Branching Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/directory_structure.html">Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/env_branchind_cicd_deployment.html">Environments, Branching, CI/CD, and Deployments Explained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch3_project_planning/project_management.html">Project Management for MLOps</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch4_data_discovery/index.html">Data Sourcing, Discovery</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of Data Sourcing, Discovery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/data_sourcing_discovery.html">Data Sourcing, Discovery &amp; Understanding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/ch4_project.html">Project-Trending Now: Implementing Web Scraping, Ingestion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/industry_case_studies.html">Data Discovery Platforms: Industry Case Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/facebook_nemo.html">Facebook: Nemo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/netflix_metacat.html">Netflix Metacat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/uber_databook.html">Uber Databook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch4_data_discovery/linkedin_datahub.html">LinkedIn Datahub</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch5_data_pipelines/index.html">Data Engineering, Pipelines</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of Data Engineering, Pipelines</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/data_pipelines.html">Data Engineering for Reliable ML Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/data_engineering_pipelines.html">Data Engineering &amp; Pipelines: A Lead’s Compendium</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/streaming_pipelines.html">Real-Time &amp; Streaming Data Pipelines: Challenges, Solutions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/netflix_keystone.html">Netflix Keystone</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch5_data_pipelines/doordash_riviera.html">Doordash Riviera</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch6_feature_engg/index.html">Feature Engineering, Feature Stores</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of Feature Engineering, Feature Stores</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch6_feature_engg/feature_engineering_store.html">Feature Engineering and Feature Stores</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch6_feature_engg/feature_engineering.html">Feature Engineering for MLOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch6_feature_engg/explained_feature_stores.html">Feature Stores for MLOps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch6_feature_engg/point_in_time.html">Point-in-Time Correctness &amp; Time Travel in ML Data Pipelines</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/index.html">Feast Feature Store</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of Feast Feature Store</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_architecture.html">Feast Architecture: A Technical Deep Dive for MLOps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_concepts.html">Feast Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_components.html">Feast Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_usecases.html">Feast Use Cases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/feast_aws.html">Running Feast with AWS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/run_in_prod.html">Running Feast in Production</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/validate_historical_with_gx.html">Validating Historical Features with Great Expectations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mlops/ch6_feature_engg/feast/add_reuse_tests.html">Adding or Reusing Tests in Feast</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch7_model_development/index.html">Model Development, Tuning, Selection, Ensembles, Calibration</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of Model Development, Tuning, Selection, Ensembles, Calibration</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/ch7_model_development.html">Chapter 7: Model Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/dl_training_playbook.html">How to train DL Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/development.html">Model Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/industry_lessons.html">Model Development: Lessons from production systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/ensembles.html"><strong>Model Ensembles</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/selection.html">Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/tuning_hypopt.html">Hyperparameter Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/expt_tracking.html">ML Expt tracking, Data Lineage, Model Registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch7_model_development/calibration.html">Model Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/ch8_ml_pipelines.html">ML Training Pipelines</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch9_ml_testing/index.html">Testing in ML Systems</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of Testing in ML Systems</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch9_ml_testing/ch9_ml_testing.html">Testing in ML Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch9_ml_testing/data_testing_validation.html">Data Testing &amp; Validation in Production</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch9_ml_testing/ml_testing.html">Testing ML Systems: Ensuring Reliability from Code to Production</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch10_deployment_serving/index.html">Model Deployment &amp; Serving</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of Model Deployment &amp; Serving</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/ch10_deployment_serving.html">Chapter 10: Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/guide_deployment_serving.html">Guide: Model Deployment &amp; Serving</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch10_deployment_serving/guide_inference_stack.html">Deep Dive: Inference Stack</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/index.html">Monitoring, Observability, Drift, Interpretability</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle navigation of Monitoring, Observability, Drift, Interpretability</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/ch11_monitor_observe_drift.html">Chapter 11: Monitoring, Observability, Drifts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_monitor_observe_drift.html">Guide: ML System Failures, Data Distribution Shifts, Monitoring, and Observability</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_interpretability_shap_lime.html">Interpretability, SHAP, LIME</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch11_monitor_observe_drift/guide_stack.html">Prometheus + Grafana and ELK Stacks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/index.html">Continual learning, Retraining, A/B Testing</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle navigation of Continual learning, Retraining, A/B Testing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/ch12_continual_learning_prod_testing.html">Chapter 12: Continual Learning &amp; Production Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_continual_learning.html">Continual Learning &amp; Model Retraining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_ab_testing.html">A/B Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_ab_testing_industry_lessons.html">A/B Testing &amp; Experimentation: Industry lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/guide_prod_testing_expt.html">Guide: Production Testing &amp; Experimentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mlops/ch12_retrain_online_testing/dr_prod_testing_expt.html">Deep Research: Production Testing &amp; Experimentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mlops/ch13_governance_ethics_human.html">Governance, Ethics &amp; The Human Element</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/index.html">PyTorch</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><div class="visually-hidden">Toggle navigation of PyTorch</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/general.html">General</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/state_dict.html">state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/distributed_data_parallel.html">Distributed Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/ddp_under_the_hood.html">DDP: Under the Hood</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/dp_ddp.html">DP vs DDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/fsdp.html">FSDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/tensor_parallelism.html">Tensor parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/pipeline_parallelism.html">Pipeline Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/device_mesh.html">Device Mesh</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lld/index.html">Low Level Design</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><div class="visually-hidden">Toggle navigation of Low Level Design</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lld/parking_lot.html">Parking Lot</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../visualization/index.html">Data Visualization Projects</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/past_experiences/iot_forecasting.md.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="energy-demand-forecasting-in-time-series-iot-data">
<h1>Energy Demand Forecasting in Time Series IoT Data<a class="headerlink" href="#energy-demand-forecasting-in-time-series-iot-data" title="Permalink to this heading">¶</a></h1>
<section id="id1">
<h2><a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<hr class="docutils" />
<section id="tl-dr-ml-powered-energy-demand-forecasting-for-smart-buildings">
<h3>TL;DR: ML-Powered Energy Demand Forecasting for Smart Buildings<a class="headerlink" href="#tl-dr-ml-powered-energy-demand-forecasting-for-smart-buildings" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Challenge:</strong> The company needed to forecast building-level energy demand 24-72 hours in advance to provide valuable data to energy suppliers for grid balancing and to enable a new resident-facing feature for optimizing solar energy self-consumption. The system had to be accurate, reliable, and capable of handling complex factors like weather and holidays.</p></li>
<li><p><strong>My Role &amp; Solution:</strong> I led the development and operationalization of the forecasting system, serving as the Data Scientist and MLOps Engineer. My key contributions were:</p>
<ul>
<li><p><strong>Strategic Approach:</strong> I designed a model development strategy that began with strong baselines (like SARIMAX and Prophet) and iteratively progressed to a high-performance XGBoost model. I established that forecasting at the building-level was the most effective initial approach, balancing accuracy with computational feasibility.</p></li>
<li><p><strong>Feature Engineering:</strong> I engineered a rich feature set crucial for forecast accuracy. This involved creating time-series features (lags, rolling windows), incorporating calendar events (holidays), and building a robust pipeline to process and align <em>future</em> weather forecast data with historical consumption patterns.</p></li>
<li><p><strong>MLOps Infrastructure:</strong> Using Terraform, I built a fully automated, serverless MLOps pipeline on AWS. The architecture included a distinct training workflow orchestrated by Step Functions for monthly model retraining, and a separate daily inference workflow that generates and stores forecasts.</p></li>
<li><p><strong>Production Lifecycle Management:</strong> I implemented the end-to-end system, including a containerized (Docker/ECR) forecasting model, versioning and governance via SageMaker Model Registry, and a CI/CD pipeline in Bitbucket for automated testing and deployment. The solution included a scalable serving layer using Amazon Timestream, API Gateway, and Lambda to deliver forecasts to both B2B and B2C consumers.</p></li>
</ul>
</li>
<li><p><strong>Impact:</strong> The forecasting system became a key data product for the company, unlocking new value for both business partners and residents.</p>
<ul>
<li><p>Achieved a <strong>&lt;10% Mean Absolute Percentage Error (MAPE)</strong> on 24-hour ahead building-level forecasts, providing reliable data to energy suppliers.</p></li>
<li><p>Enabled the launch of the “Smart Energy Advisor” feature in the resident app, leading to a <strong>15% increase in user engagement</strong> with energy management tools. This, in turn, drove a measured ~10% increase in the building’s solar self-consumption rate by empowering residents to align appliance usage with peak solar generation periods.</p></li>
</ul>
</li>
<li><p><strong>System Architecture:</strong> I designed and implemented the complete AWS solution, from data ingestion to API serving, ensuring scalability and automation.</p></li>
</ul>
<p><img alt="" src="_static/past_experiences/iot_forecasting/contributions.png" /></p>
</section>
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h3>
<section id="purpose">
<h4>Purpose<a class="headerlink" href="#purpose" title="Permalink to this heading">¶</a></h4>
<p>This document provides detailed technical information about the Machine Learning (ML) based Energy Demand Forecasting (EDF) system developed. It serves as a guide for developers, MLOps engineers, operations teams.</p>
</section>
<section id="business-goal">
<h4>Business Goal<a class="headerlink" href="#business-goal" title="Permalink to this heading">¶</a></h4>
<p>The primary goals of the EDF system are to:</p>
<ul class="simple">
<li><p>Provide accurate short-term (e.g., 24-72 hours) aggregated energy demand forecasts at the building level to external energy suppliers and Distribution System Operators (DSOs) for improved grid balancing, energy procurement, and network management.</p></li>
<li><p>Empower residents by providing insights into predicted building energy consumption versus predicted solar generation, enabling them to optimize appliance usage for increased solar self-consumption and potential cost savings.</p></li>
</ul>
</section>
<section id="scope">
<h4>Scope<a class="headerlink" href="#scope" title="Permalink to this heading">¶</a></h4>
<p>This documentation details the end-to-end pipelines for data processing, model training, model evaluation, model registration, batch inference (forecasting), forecast storage, and the conceptual API serving layer. It assumes the existence of a separate data ingestion pipeline providing the necessary raw data feeds.</p>
</section>
<section id="key-technologies">
<h4>Key Technologies<a class="headerlink" href="#key-technologies" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><strong>Cloud Platform:</strong> AWS (Amazon Web Services)</p></li>
<li><p><strong>Data Lake:</strong> Amazon S3</p></li>
<li><p><strong>Data Processing/ETL:</strong> AWS Glue (PySpark), AWS Lambda (Python), SageMaker Processing Jobs (PySpark/Python)</p></li>
<li><p><strong>Feature Management:</strong> Shared Code Libraries (Primary), Amazon SageMaker Feature Store (Optional for historical features)</p></li>
<li><p><strong>Model Training:</strong> Amazon SageMaker Training Jobs (using custom Docker containers with Prophet, XGBoost, etc.)</p></li>
<li><p><strong>Model Forecasting:</strong> Amazon SageMaker Processing Jobs (running forecast generation scripts)</p></li>
<li><p><strong>Model Registry:</strong> Amazon SageMaker Model Registry</p></li>
<li><p><strong>Orchestration:</strong> AWS Step Functions</p></li>
<li><p><strong>Scheduling:</strong> Amazon EventBridge Scheduler</p></li>
<li><p><strong>Forecast Storage:</strong> Amazon Timestream (Primary Example), Amazon RDS, Amazon DynamoDB</p></li>
<li><p><strong>API Layer:</strong> Amazon API Gateway, AWS Lambda</p></li>
<li><p><strong>Infrastructure as Code:</strong> Terraform</p></li>
<li><p><strong>CI/CD:</strong> Bitbucket Pipelines</p></li>
<li><p><strong>Containerization:</strong> Docker</p></li>
<li><p><strong>Core Libraries:</strong> PySpark, Pandas, Scikit-learn, Boto3, PyYAML, Prophet, XGBoost, AWS Data Wrangler.</p></li>
</ul>
<p><strong>Table of Contents</strong></p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#introduction"><span class="xref myst">Introduction</span></a></p>
<ul class="simple">
<li><p><a class="reference internal" href="#purpose"><span class="xref myst">Purpose</span></a></p></li>
<li><p><a class="reference internal" href="#business-goal"><span class="xref myst">Business Goal</span></a></p></li>
<li><p><a class="reference internal" href="#scope"><span class="xref myst">Scope</span></a></p></li>
<li><p><a class="reference internal" href="#key-technologies"><span class="xref myst">Key Technologies</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#discovery-and-scoping"><span class="xref myst">Discovery and Scoping</span></a></p>
<ul class="simple">
<li><p><a class="reference internal" href="#use-case-evaluation"><span class="xref myst">Use Case Evaluation</span></a></p></li>
<li><p><a class="reference internal" href="#product-strategies"><span class="xref myst">Product Strategies</span></a></p></li>
<li><p><a class="reference internal" href="#features"><span class="xref myst">Features</span></a></p></li>
<li><p><a class="reference internal" href="#product-requirements-document"><span class="xref myst">Product Requirements Document</span></a></p></li>
<li><p><a class="reference internal" href="#milestones-and-timelines"><span class="xref myst">Milestones and Timelines</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#system-architecture"><span class="xref myst">System Architecture</span></a></p>
<ul class="simple">
<li><p><a class="reference internal" href="#overall-data-flow"><span class="xref myst">Overall Data Flow</span></a></p></li>
<li><p><a class="reference internal" href="#training-workflow-diagram"><span class="xref myst">Training Workflow Diagram</span></a></p></li>
<li><p><a class="reference internal" href="#inference-workflow-diagram"><span class="xref myst">Inference Workflow Diagram</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#configuration-management"><span class="xref myst">Configuration Management</span></a></p></li>
<li><p><a class="reference internal" href="#infrastructure-as-code-terraform"><span class="xref myst">Infrastructure as Code (Terraform)</span></a></p>
<ul class="simple">
<li><p><a class="reference internal" href="#stacks-overview"><span class="xref myst">Stacks Overview</span></a></p></li>
<li><p><a class="reference internal" href="#key-resources"><span class="xref myst">Key Resources</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#cicd-pipeline-bitbucket"><span class="xref myst">CI/CD Pipeline (Bitbucket)</span></a></p>
<ul class="simple">
<li><p><a class="reference internal" href="#ci-workflow"><span class="xref myst">CI Workflow</span></a></p></li>
<li><p><a class="reference internal" href="#training-cd-workflow"><span class="xref myst">Training CD Workflow</span></a></p></li>
<li><p><a class="reference internal" href="#inference-cd-workflow"><span class="xref myst">Inference CD Workflow</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#deployment--execution"><span class="xref myst">Deployment &amp; Execution</span></a></p>
<ul class="simple">
<li><p><a class="reference internal" href="#prerequisites"><span class="xref myst">Prerequisites</span></a></p></li>
<li><p><a class="reference internal" href="#initial-deployment"><span class="xref myst">Initial Deployment</span></a></p></li>
<li><p><a class="reference internal" href="#running-training"><span class="xref myst">Running Training</span></a></p></li>
<li><p><a class="reference internal" href="#running-inference"><span class="xref myst">Running Inference</span></a></p></li>
<li><p><a class="reference internal" href="#model-approval"><span class="xref myst">Model Approval</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#monitoring--alerting"><span class="xref myst">Monitoring &amp; Alerting</span></a></p></li>
<li><p><a class="reference internal" href="#troubleshooting-guide"><span class="xref myst">Troubleshooting Guide</span></a></p></li>
<li><p><a class="reference internal" href="#security-considerations"><span class="xref myst">Security Considerations</span></a></p></li>
<li><p><a class="reference internal" href="#roadmap--future-enhancements"><span class="xref myst">Roadmap &amp; Future Enhancements</span></a></p></li>
<li><p><a class="reference internal" href="#appendices"><span class="xref myst">Appendices</span></a></p>
<ul class="simple">
<li><p><a class="reference internal" href="#configuration-file-example"><span class="xref myst">Configuration File Example</span></a></p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="discovery-and-scoping">
<h3>Discovery and Scoping<a class="headerlink" href="#discovery-and-scoping" title="Permalink to this heading">¶</a></h3>
<section id="use-case-evaluation">
<h4>Use Case Evaluation<a class="headerlink" href="#use-case-evaluation" title="Permalink to this heading">¶</a></h4>
<p><img alt="" src="_static/past_experiences/iot_forecasting/use_case.png" /></p>
</section>
<section id="product-strategies">
<h4>Product Strategies<a class="headerlink" href="#product-strategies" title="Permalink to this heading">¶</a></h4>
<p><img alt="" src="_static/past_experiences/iot_forecasting/strategy.png" /></p>
</section>
<section id="features">
<h4>Features<a class="headerlink" href="#features" title="Permalink to this heading">¶</a></h4>
<p><img alt="" src="_static/past_experiences/iot_forecasting/features.png" /></p>
</section>
<section id="product-requirements-document">
<h4>Product Requirements Document<a class="headerlink" href="#product-requirements-document" title="Permalink to this heading">¶</a></h4>
<p><img alt="" src="_static/past_experiences/iot_forecasting/prd.png" /></p>
</section>
<section id="development-stages">
<h4>Development Stages<a class="headerlink" href="#development-stages" title="Permalink to this heading">¶</a></h4>
<!--![](../_static/past_experiences/iot_forecasting/stages.png)-->
<p align="center">
    <img src="../_static/past_experiences/iot_forecasting/stages.png" width="75%"> 
</p>
</section>
</section>
<section id="system-architecture">
<h3>System Architecture<a class="headerlink" href="#system-architecture" title="Permalink to this heading">¶</a></h3>
<p><img alt="" src="_static/past_experiences/iot_forecasting/arch.png" /></p>
<!--
#### Data Characteristics

*   **Buildings:** 10 dozen = **120 buildings**.
*   **Apartments:** **3,000 apartments**.
*   **Sensors/Readings per Apartment:** ~8 (1 Energy, 1 Hot Water, 3 rooms * [1 Temp + 1 Setpoint]). Total sensors/readings ~24,000.
*   **Base Frequency:** Potential reading every 15 minutes.
*   **Storage Optimization:** Data is only recorded on change. We'll assume a **change rate of 10-20%** on average for sensor readings per interval, as temperatures, setpoints, and consumption are not constantly fluctuating dramatically. This is a crucial factor for volume estimation.
*   **Weather Data:** Fetched per building or per small geographical cluster of buildings.


| Data Type | Daily Volume (Average) | Data Velocity | Data Profile & Governance Notes |
| :--- | :--- | :--- | :--- |
| **IoT Sensor Data (Time Series)** | **150 - 300 MB per day.** <br>This is the highest volume data. Calculated from ~3,000 apartments with ~8 readings each, potentially every 15 mins. Volume is significantly reduced by only storing changed values (assuming 10-20% avg change rate per interval). Spikes in volume can occur during coordinated events (e.g., building-wide heating adjustments, morning rush hour). | **Near Real-Time Events, Processed in Batch.** <br>Individual sensor readings are generated continuously, collected by in-apartment tablets, and sent periodically to the central database. Data is ingested into the analytical platform via **daily or hourly batch jobs** (e.g., using AWS Glue). | **Semi-Structured (JSON/DB Rows).** <br>Each record contains `apartment_id`, `building_id`, `timestamp`, `sensor_type` (e.g., `heating_energy_kwh`, `room_temp_c`, `setpoint_temp_c`, `hot_water_litres`), and `value`. <br>**Governance:** Data is linked to a specific apartment/building, making it sensitive. While not direct PII, it reveals resident behavior patterns. Governed by **FADP/GDPR**. Anonymization or pseudonymization is required for non-operational analysis. Data quality monitoring for sensor drift/failure is critical. |
| **Aggregated Consumption Data** | **< 10 MB per day.** <br>This data is derived from the raw IoT sensor data. Volume is low as it contains one aggregated record per building per hour. | **Batch-Generated.** <br>Created by a **daily AWS Glue ETL job** that processes the previous day's IoT sensor data. Not a direct stream from a source. | **Highly Structured (Parquet).** <br>Consists of a table with a well-defined schema: `building_id`, `timestamp_hour`, `total_consumption_kwh`, `total_solar_kwh`, etc. <br>**Governance:** Aggregation provides a layer of privacy compared to raw sensor data. Serves as the primary source of truth for building-level forecasting. Data lineage from raw sensor data must be maintained. |
| **External Weather Data** | **< 5 MB per day.** <br>Data is fetched for ~120 building locations (or geographical clusters). Volume depends on API response format and fetch frequency (hourly for history, more frequently for forecasts). | **Scheduled API Polling (Batch).** <br>Historical data is fetched via a **daily AWS Lambda job**. Future forecast data is fetched via a **Lambda job running several times a day** to ensure freshness. | **Semi-Structured (JSON from API).** <br>Contains `location` (lat/lon or city), `timestamp`, and various weather metrics (`temperature_c`, `humidity`, `cloud_cover`, `solar_irradiance_ghi_forecast`, etc.). <br>**Governance:** Dependent on a third-party provider (e.g., MeteoSwiss). Requires monitoring for API availability, changes, and rate limits. Caching strategies may be needed to manage costs and dependencies. |
| **Building & Apartment Topology** | **< 1 MB per day (on average).** <br>This is very low volume, consisting primarily of updates when new buildings/apartments are commissioned or structural changes are made. | **Low / On-Change.** <br>Synchronized via **daily or triggered batch jobs** from the main operational database or commissioning system. | **Highly Structured.** <br>Contains `building_id`, `apartment_id`, and potentially metadata like `building_size_sqm`, `num_rooms`, `building_age`, `insulation_type`. <br>**Governance:** The source of truth for the physical hierarchy. Data consistency is critical for correct aggregation and for potentially creating building "archetypes" for transfer learning (cold starts). |
| **System & User Feedback** | **< 1 MB per day.** <br>Generated when maintenance technicians label an anomaly alert (True/False Positive) or when residents report issues. | **Low / Event-Driven.** <br>Captured via the internal maintenance dashboard or resident support application. Ingested via API calls or **event-driven Lambda functions**. | **Highly Structured & High Value.** <br>Contains `alert_id`, `technician_id`, `feedback_label` (`True Positive`, `False Positive`, `Root Cause`), `timestamp`. <br>**Governance:** The primary source of labeled data for model evaluation and future supervised retraining. The quality and consistency of this feedback are critical for the ML lifecycle. |

-->
<section id="overall-data-flow">
<h4>Overall Data Flow<a class="headerlink" href="#overall-data-flow" title="Permalink to this heading">¶</a></h4>
<p>The EDF system utilizes distinct, automated pipelines for training forecasting models and generating daily forecasts. It relies on processed historical data and external weather forecasts. Forecasts are stored in a time-series database and made available via APIs.</p>
<ol class="arabic simple">
<li><p><strong>Raw Data:</strong> Aggregated Consumption, Solar Generation, Historical Weather, <em>Future Weather Forecasts</em>, Calendar/Topology data land in S3 Raw Zone.</p></li>
<li><p><strong>Processed Data:</strong> Ingestion pipeline processes <em>historical</em> data into <code class="docutils literal notranslate"><span class="pre">processed_edf_data</span></code> in S3 Processed Zone / Glue Catalog.</p></li>
<li><p><strong>Features (Training):</strong> Training Feature Engineering step reads <code class="docutils literal notranslate"><span class="pre">processed_edf_data</span></code>, calculates historical time-series features (lags, windows, time components), splits train/eval sets, and writes them to S3.</p></li>
<li><p><strong>Features (Inference):</strong> Inference Feature Engineering step reads recent <code class="docutils literal notranslate"><span class="pre">processed_edf_data</span></code> and <em>future raw weather forecasts</em>, calculates features for the forecast horizon using shared logic, and writes them to S3.</p></li>
<li><p><strong>Model Artifacts:</strong> Training jobs save serialized forecast models (Prophet JSON, XGBoost UBJ) to S3.</p></li>
<li><p><strong>Evaluation Reports:</strong> Evaluation jobs save forecast metrics (MAPE, RMSE) to S3.</p></li>
<li><p><strong>Model Packages:</strong> Approved forecast models are registered in the EDF Model Package Group.</p></li>
<li><p><strong>Raw Forecasts:</strong> Inference forecast generation step writes raw forecast outputs (timestamp, building, yhat, yhat_lower, yhat_upper) to S3.</p></li>
<li><p><strong>Stored Forecasts:</strong> Load Forecasts step reads raw forecasts from S3 and writes them into the target database (e.g., Timestream).</p></li>
<li><p><strong>Served Forecasts:</strong> API Gateway and Lambda query the forecast database to serve B2B and B2C clients.</p></li>
</ol>
<p><strong>Forecasting Pipeline Description:</strong></p>
<ol class="arabic simple">
<li><p><strong>Ingestion:</strong> Aggregated Consumption, Solar Generation, Topology/Calendar, and crucially, <em>Forecasted</em> Weather data are ingested into the Raw S3 Zone.</p></li>
<li><p><strong>Processing:</strong> A daily Glue ETL job aggregates data to the building level (if needed), joins it with weather forecasts and calendar info, and engineers features (lags, time features, weather interactions). Processed features are stored in S3 (and potentially SageMaker Feature Store for easier reuse/serving). The Glue Data Catalog is updated.</p></li>
<li><p><strong>Model Training:</strong> A Step Functions workflow orchestrates training, similar to AD, using appropriate forecasting models (ARIMA, Prophet, Gradient Boosting, etc.). Models are registered.</p></li>
<li><p><strong>Batch Inference:</strong> A daily Step Functions workflow prepares features for the forecast horizon (using actual latest data <em>and future weather forecasts</em>), runs SageMaker Batch Transform, and stores the resulting forecasts (e.g., hourly demand for next 72h) in S3.</p></li>
<li><p><strong>Results Storage:</strong> Raw forecasts are stored in S3. A subsequent job loads the forecasts into a database optimized for time-series queries or fast lookups (DynamoDB, RDS, or potentially Amazon Timestream).</p></li>
<li><p><strong>Serving:</strong></p>
<ul class="simple">
<li><p><strong>B2B:</strong> A dedicated API Gateway endpoint uses Lambda to query the Forecast Database, perform necessary aggregation/anonymization, and return forecasts to authorized suppliers/DSOs.</p></li>
<li><p><strong>B2C:</strong> The existing App/Tablet backend queries the Forecast Database (likely via another API Gateway endpoint) to get simplified data for resident visualization (comparing building demand vs. solar).</p></li>
<li><p><strong>Analysts:</strong> Use Athena for ad-hoc analysis.</p></li>
</ul>
</li>
</ol>
</section>
<section id="training-workflow">
<h4>Training Workflow<a class="headerlink" href="#training-workflow" title="Permalink to this heading">¶</a></h4>
<p><strong>Summary:</strong> Triggered by Schedule/Manual -&gt; Validates Schema -&gt; Engineers Historical Features (Train/Eval Sets) -&gt; Trains Model (Prophet/XGBoost) -&gt; Evaluates Model (MAPE, RMSE) -&gt; Conditionally Registers Model (Pending Approval).</p>
<p><strong>Step Function State Machine</strong></p>
<ol class="arabic simple">
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">ValidateSchema</span></code> (Optional)</p>
<ul class="simple">
<li><p><strong>Service:</strong> SM Processing / Glue Shell</p></li>
<li><p><strong>Action:</strong> Validates schema of <code class="docutils literal notranslate"><span class="pre">processed_edf_data</span></code>.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">FeatureEngineeringTrainingEDF</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> SM Processing Job (Spark) / Glue ETL</p></li>
<li><p><strong>Action:</strong> Reads <code class="docutils literal notranslate"><span class="pre">processed_edf_data</span></code> for training range. Creates time features, lags (consumption, weather), rolling windows. Splits into train/eval feature sets (Parquet) written to S3.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">ModelTrainingEDF</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> SM Training Job</p></li>
<li><p><strong>Action:</strong> Reads training features from S3. Instantiates/fits chosen model (Prophet/XGBoost) based on input strategy and hyperparameters. Saves model artifact (<code class="docutils literal notranslate"><span class="pre">prophet_model.json</span></code> / <code class="docutils literal notranslate"><span class="pre">xgboost_model.ubj</span></code> + <code class="docutils literal notranslate"><span class="pre">model_metadata.json</span></code>) within <code class="docutils literal notranslate"><span class="pre">model.tar.gz</span></code> to S3.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">ModelEvaluationEDF</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> SM Processing Job (Python + libs)</p></li>
<li><p><strong>Action:</strong> Loads model artifact. Reads evaluation features from S3. Generates forecasts for eval period. Calculates MAPE, RMSE, MAE against actuals (from eval features). Writes <code class="docutils literal notranslate"><span class="pre">evaluation_report_edf.json</span></code> to S3.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">CheckEvaluationEDF</span></code> (Choice)</p>
<ul class="simple">
<li><p><strong>Action:</strong> Compares metrics (e.g., MAPE, RMSE) against thresholds from config.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">RegisterModelEDF</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> Lambda</p></li>
<li><p><strong>Action:</strong> Creates Model Package in EDF group (<code class="docutils literal notranslate"><span class="pre">PendingManualApproval</span></code>), embedding metadata and evaluation results.</p></li>
</ul>
</li>
<li><p><strong>Terminal States:</strong> <code class="docutils literal notranslate"><span class="pre">WorkflowSucceeded</span></code>, <code class="docutils literal notranslate"><span class="pre">EvaluationFailedEDF</span></code>, <code class="docutils literal notranslate"><span class="pre">WorkflowFailed</span></code>.</p></li>
</ol>
</section>
<section id="inference-workflow">
<h4>Inference Workflow<a class="headerlink" href="#inference-workflow" title="Permalink to this heading">¶</a></h4>
<p><strong>Summary:</strong> Triggered Daily by Scheduler -&gt; Gets Latest Approved Model -&gt; Creates SM Model Resource -&gt; Engineers Inference Features -&gt; Generates Forecasts (SM Processing) -&gt; Loads Forecasts into DB (Timestream/RDS).</p>
<ol class="arabic simple">
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">GetInferenceDate</span></code> (Optional Lambda / Step Functions Context)</p>
<ul class="simple">
<li><p><strong>Action:</strong> Determines target forecast start date (e.g., “today” or “tomorrow” relative to execution time) and forecast end date based on horizon.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">GetApprovedEDFModelPackage</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> Lambda</p></li>
<li><p><strong>Action:</strong> Gets latest <code class="docutils literal notranslate"><span class="pre">Approved</span></code> Model Package ARN from EDF group.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">CreateEDFSageMakerModel</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> Lambda</p></li>
<li><p><strong>Action:</strong> Creates SM <code class="docutils literal notranslate"><span class="pre">Model</span></code> resource from the approved package ARN.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">FeatureEngineeringInferenceEDF</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> SM Processing Job (Spark) / Glue ETL</p></li>
<li><p><strong>Action:</strong> Reads recent historical <code class="docutils literal notranslate"><span class="pre">processed_edf_data</span></code> (for lags) AND future raw <code class="docutils literal notranslate"><span class="pre">weather-forecast</span></code> data. Creates feature set covering the forecast horizon (including future dates and weather). Writes inference features (Parquet) to S3.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">GenerateForecastsEDF</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> SM Processing Job (Python + libs)</p></li>
<li><p><strong>Action:</strong> Loads model artifact (mounted via SM Model resource). Reads inference features from S3. Calls model’s <code class="docutils literal notranslate"><span class="pre">predict</span></code> method to generate forecasts (<code class="docutils literal notranslate"><span class="pre">yhat</span></code>, <code class="docutils literal notranslate"><span class="pre">yhat_lower</span></code>, <code class="docutils literal notranslate"><span class="pre">yhat_upper</span></code>). Writes forecast results (Parquet/CSV) to S3.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> <code class="docutils literal notranslate"><span class="pre">LoadForecastsToDB</span></code></p>
<ul class="simple">
<li><p><strong>Service:</strong> Lambda / Glue Python Shell</p></li>
<li><p><strong>Action:</strong> Reads forecast results from S3. Formats data for target DB (Timestream example). Writes records to the database using batch operations.</p></li>
</ul>
</li>
<li><p><strong>Terminal States:</strong> <code class="docutils literal notranslate"><span class="pre">WorkflowSucceeded</span></code>, <code class="docutils literal notranslate"><span class="pre">WorkflowFailed</span></code>.</p></li>
</ol>
</section>
<section id="forecast-serving">
<h4>Forecast Serving<a class="headerlink" href="#forecast-serving" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><strong>B2B API:</strong> API Gateway endpoint proxying to a Lambda function. Lambda queries the Forecast DB (e.g., Timestream) based on requested <code class="docutils literal notranslate"><span class="pre">building_id</span></code> and <code class="docutils literal notranslate"><span class="pre">time_range</span></code>. Requires authentication/authorization (e.g., API Keys, Cognito Authorizer).</p></li>
<li><p><strong>B2C API:</strong> Separate API Gateway endpoint/Lambda. Queries Forecast DB, potentially performs simple comparison with Solar forecast (if available), returns simplified data structure for UI visualization. Requires app user authentication.</p></li>
</ul>
</section>
</section>
<section id="model-development-iteration">
<h3>Model Development &amp; Iteration<a class="headerlink" href="#model-development-iteration" title="Permalink to this heading">¶</a></h3>
<p><strong>Models for Time Series Forecasting</strong>
<img alt="" src="_static/past_experiences/iot_forecasting/models1.png" />
<img alt="" src="_static/past_experiences/iot_forecasting/models2.png" /></p>
<p><img alt="" src="_static/past_experiences/iot_forecasting/model_development.png" /></p>
<!--

**Context:** The goal is to produce accurate, building-level, short-term (24-72h) energy demand forecasts. The model must leverage time-series patterns, holiday effects, and crucially, future weather forecasts. Success is measured by the accuracy of point forecasts.

**Evaluation Metric:**
*   **Primary Metric:** **Mean Absolute Percentage Error (MAPE)**. It's a standard, intuitive metric for forecasting that expresses error as a percentage of the actual value. A MAPE of 10% means the forecast is, on average, 10% off the actual value.
*   **Secondary Metric:** **Root Mean Squared Error (RMSE)**. This metric is in the same units as the target (kWh) and penalizes large errors more heavily than MAPE, which is important for avoiding significant misses during peak consumption periods.

**Evaluation Method:**
All experiments are evaluated using a consistent backtesting methodology. The model is trained on a rolling window of historical data (e.g., 1 year) and then used to forecast the next period (e.g., 1 week). This process is repeated over multiple splits of the test set to get a robust estimate of out-of-sample performance.

---

Here is a list of the experiments performed:

| Experiment / Iteration | Model/Technique | Features Used | Evaluation Metric & Hypothetical Result | Key Learnings & Rationale |
| :--- | :--- | :--- | :--- | :--- |
| **Phase 1: Baselines & Classical Time Series Models** |
| **1. Baseline Model** | Seasonal Naive | - `consumption_kwh` (target) | **MAPE:** 28%<br>**RMSE:** 12.5 kWh | **Rationale:** Establish the absolute minimum performance benchmark. The model predicts the consumption from the same hour on the same day of the previous week. Its poor performance highlights the strong influence of factors other than simple weekly seasonality, especially weather. |
| **2. Classical Univariate Model** | SARIMA | - `consumption_kwh` (target) | **MAPE:** 21%<br>**RMSE:** 9.8 kWh<br>*(Improvement: ~25% vs. Baseline)* | **Rationale:** Test a standard statistical time-series model. `auto_arima` was used to find optimal `(p,d,q)(P,D,Q)` parameters. SARIMA successfully captured trend and autocorrelation, significantly outperforming the naive baseline. However, its errors clearly correlated with unexpected weather changes, proving the need for exogenous variables. |
| **3. Multivariate Statistical Model** | SARIMAX | - Target: `consumption_kwh`<br>- **Exogenous:** `temperature_c`, `is_holiday_flag` | **MAPE:** 16%<br>**RMSE:** 7.2 kWh<br>*(Improvement: ~24% vs. SARIMA)* | **Rationale:** Introduce external factors that are known in advance. Adding historical weather and holiday flags as exogenous variables provided a major boost in accuracy. This confirmed that weather is a primary driver of energy consumption. Parameter tuning remained complex. |
| **Phase 2: Modern & Machine Learning Models** |
| **4. Decomposable Model** | Prophet | - Target: `consumption_kwh`<br>- **Regressors:** `temperature_c`, `solar_irradiance_ghi`, `is_holiday_flag` | **MAPE:** 14%<br>**RMSE:** 6.5 kWh<br>*(Improvement: ~12.5% vs. SARIMAX)* | **Rationale:** Test a more modern, flexible time-series model. Prophet's ability to handle multiple seasonalities (daily, weekly) and its robust holiday modeling, combined with weather regressors, gave it an edge over SARIMAX. It was also significantly faster to train and easier to configure. |
| **5. ML Regression Model (Basic Features)** | XGBoost | - Target: `consumption_kwh`<br>- **Basic Time Features:** `hour_of_day`, `day_of_week`<br>- **Exogenous:** `temperature_c`, `solar_irradiance_ghi`, `is_holiday_flag` | **MAPE:** 13.5%<br>**RMSE:** 6.3 kWh<br>*(Improvement: ~3.5% vs. Prophet)* | **Rationale:** Treat forecasting as a standard regression problem. Even with basic time features, XGBoost's ability to capture non-linear relationships (e.g., the effect of temperature is not linear) allowed it to slightly outperform Prophet. This demonstrated the potential of tree-based models. |
| **6. ML Regression Model (Advanced Features)** | XGBoost | All previous features +<br>- **Lag features** (e.g., consumption 24h & 168h ago)<br>- **Rolling window features** (e.g., 24h avg consumption/temp)<br>- **Interaction features** (e.g., temp * hour) | **MAPE:** 9.5%<br>**RMSE:** 4.1 kWh<br>*(Improvement: ~30% vs. XGBoost Basic)* | **Rationale:** Provide the model with deep temporal context. This was the most significant leap in performance. Lag and rolling window features allowed the model to understand recent consumption patterns and trends, which are highly predictive. **This established XGBoost with rich features as the champion model.** |
| **7. Hyperparameter Optimization** | XGBoost (Tuned) | Same as Exp #6 | **MAPE:** 8.2%<br>**RMSE:** 3.6 kWh<br>*(Improvement: ~14% vs. XGBoost Advanced)* | **Rationale:** Fine-tune the champion model. Using SageMaker's Automatic Model Tuning to optimize XGBoost's key hyperparameters (`n_estimators`, `max_depth`, `learning_rate`, etc.) squeezed out a significant, final improvement in accuracy, bringing the model to production-ready performance. |
| **Phase 3: Deep Learning & Global Models** |
| **8. Global Model Exploration** | DeepAR | - **Target:** `consumption_kwh`<br>- **Covariates:** Weather forecasts, time features<br>- **Static Features:** `building_id` | **MAPE:** 8.5%<br>**RMSE:** 3.8 kWh | **Rationale:** Test if a global model trained across all buildings could outperform the building-specific XGBoost models. DeepAR performed very well, nearly matching the tuned XGBoost, and its key advantage was providing a probabilistic forecast (prediction intervals). However, its complexity and slightly lower point-forecast accuracy meant it did not replace XGBoost as the champion for this specific goal. It is retained as a strong candidate for future probabilistic forecasting features. |


### Model development Iteration: Takeaways

The model development for Energy Demand Forecasting followed a path of progressively adding complexity and context, leading to significant accuracy gains at each major step:

1.  **Exogenous Variables are Critical:** The first major improvement came from moving beyond purely univariate models (like SARIMA) to models that incorporate external drivers like weather and holidays (SARIMAX, Prophet, ML models).
2.  **ML Models Outperform Classical for Non-Linearity:** Machine learning models, particularly **XGBoost**, demonstrated an advantage over classical statistical models like SARIMAX by being able to capture complex, non-linear relationships between features (especially temperature) and energy consumption.
3.  **Advanced Feature Engineering is the Key Driver:** The most substantial performance gain was achieved by creating rich temporal features (lags and rolling windows). This highlights that for regression-based forecasting, feature engineering is often more important than the initial choice of algorithm.
4.  **A Tuned ML Model is the Champion:** A well-featured and hyperparameter-tuned **XGBoost** model provided the best point-forecast accuracy, balancing performance with reasonable training times and a well-understood implementation.
5.  **Deep Learning Shows Promise:** **DeepAR** proved to be a powerful alternative, especially for its ability to learn across buildings and provide probabilistic forecasts, marking it as a valuable tool for future enhancements rather than an immediate replacement for the champion model.

-->
</section>
<section id="configuration-management">
<h3>Configuration Management<a class="headerlink" href="#configuration-management" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Uses <code class="docutils literal notranslate"><span class="pre">config/edf_config.yaml</span></code> version-controlled in Git.</p></li>
<li><p>File uploaded to S3 by CI/CD.</p></li>
<li><p>Scripts load config from S3 URI passed via environment variable/argument.</p></li>
<li><p>Includes sections for <code class="docutils literal notranslate"><span class="pre">feature_engineering</span></code>, <code class="docutils literal notranslate"><span class="pre">training</span></code> (with nested model hyperparameters), <code class="docutils literal notranslate"><span class="pre">evaluation</span></code> (thresholds), <code class="docutils literal notranslate"><span class="pre">inference</span></code> (schedule, instance types, DB config).</p></li>
<li><p>Secrets managed via AWS Secrets Manager / SSM Parameter Store.</p></li>
</ul>
</section>
<section id="infrastructure-as-code-terraform">
<h3>Infrastructure as Code (Terraform)<a class="headerlink" href="#infrastructure-as-code-terraform" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Manages all AWS resources for EDF pipelines.</p></li>
<li><p><strong>Stacks:</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">training_edf</span></code>: ECR Repo (<code class="docutils literal notranslate"><span class="pre">edf-training-container</span></code>), Model Package Group (<code class="docutils literal notranslate"><span class="pre">EDFBuildingDemandForecaster</span></code>), Lambda (<code class="docutils literal notranslate"><span class="pre">RegisterEDFModelFunction</span></code>), Step Function (<code class="docutils literal notranslate"><span class="pre">EDFTrainingWorkflow</span></code>), associated IAM roles. Optionally Feature Group (<code class="docutils literal notranslate"><span class="pre">edf-building-features</span></code>). Requires outputs from <code class="docutils literal notranslate"><span class="pre">ingestion</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inference_edf</span></code>: Timestream DB/Table (or RDS/DDB), Lambdas (GetModel, CreateModel - potentially reused; LoadForecasts), Step Function (<code class="docutils literal notranslate"><span class="pre">EDFInferenceWorkflow</span></code>), EventBridge Scheduler, API Gateway Endpoints/Lambdas (for serving), associated IAM roles. Requires outputs from <code class="docutils literal notranslate"><span class="pre">ingestion</span></code> and <code class="docutils literal notranslate"><span class="pre">training_edf</span></code>.</p></li>
</ul>
</li>
<li><p><strong>State:</strong> Remote backend (S3/DynamoDB) configured.</p></li>
</ul>
</section>
<section id="ci-cd-pipeline-bitbucket">
<h3>CI/CD Pipeline (Bitbucket)<a class="headerlink" href="#ci-cd-pipeline-bitbucket" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Defined in <code class="docutils literal notranslate"><span class="pre">bitbucket-pipelines.yml</span></code>.</p></li>
<li><p><strong>CI (Branches/PRs):</strong> Lints, runs ALL unit tests, builds EDF container, pushes to ECR (<code class="docutils literal notranslate"><span class="pre">edf-training-container</span></code> repo), validates ALL Terraform stacks.</p></li>
<li><p><strong>EDF Training CD (<code class="docutils literal notranslate"><span class="pre">custom:deploy-and-test-edf-training</span></code>):</strong> Manual trigger. Deploys <code class="docutils literal notranslate"><span class="pre">training_edf</span></code> stack. Runs EDF training integration tests.</p></li>
<li><p><strong>EDF Inference CD (<code class="docutils literal notranslate"><span class="pre">custom:deploy-and-test-edf-inference</span></code>):</strong> Manual trigger. Deploys <code class="docutils literal notranslate"><span class="pre">inference_edf</span></code> stack. Runs EDF inference integration tests (requires approved model).</p></li>
</ul>
</section>
<section id="deployment-execution">
<h3>Deployment &amp; Execution<a class="headerlink" href="#deployment-execution" title="Permalink to this heading">¶</a></h3>
<p><strong>8.1 Prerequisites:</strong> Base AWS setup, Terraform, Docker, Python, Git, Bitbucket config, deployed <code class="docutils literal notranslate"><span class="pre">ingestion</span></code> stack.</p>
<p><strong>8.2 Initial Deployment:</strong> Deploy Terraform stacks (<code class="docutils literal notranslate"><span class="pre">training_edf</span></code>, <code class="docutils literal notranslate"><span class="pre">inference_edf</span></code>) after <code class="docutils literal notranslate"><span class="pre">ingestion</span></code>. Build/push EDF container. Upload initial <code class="docutils literal notranslate"><span class="pre">edf_config.yaml</span></code>. Ensure processed EDF data exists.</p>
<p><strong>8.3 Running Training:</strong> Trigger <code class="docutils literal notranslate"><span class="pre">EDFTrainingWorkflow</span></code> Step Function (schedule/manual) with appropriate input JSON (date range, model strategy, hyperparameters, image URI).</p>
<p><strong>8.4 Running Inference:</strong> <code class="docutils literal notranslate"><span class="pre">EDFInferenceWorkflow</span></code> runs automatically via EventBridge Scheduler. Ensure prerequisite data (processed history, weather forecasts) is available daily.</p>
<p><strong>8.5 Model Approval:</strong> Manually review <code class="docutils literal notranslate"><span class="pre">PendingManualApproval</span></code> packages in the <code class="docutils literal notranslate"><span class="pre">EDFBuildingDemandForecaster</span></code> group and promote to <code class="docutils literal notranslate"><span class="pre">Approved</span></code> based on evaluation metrics.</p>
</section>
<section id="monitoring-alerting">
<h3>Monitoring &amp; Alerting<a class="headerlink" href="#monitoring-alerting" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>CloudWatch Logs:</strong> Monitor logs for EDF Step Functions, Lambdas, SageMaker Processing Jobs.</p></li>
<li><p><strong>CloudWatch Metrics:</strong> Monitor SFN <code class="docutils literal notranslate"><span class="pre">ExecutionsFailed</span></code>, Lambda <code class="docutils literal notranslate"><span class="pre">Errors</span></code>/<code class="docutils literal notranslate"><span class="pre">Duration</span></code>, Processing Job <code class="docutils literal notranslate"><span class="pre">CPU/Memory</span></code>, Timestream/RDS metrics (if applicable), API Gateway <code class="docutils literal notranslate"><span class="pre">Latency</span></code>/<code class="docutils literal notranslate"><span class="pre">4XX/5XX</span> <span class="pre">Errors</span></code>.</p></li>
<li><p><strong>Forecast Accuracy Tracking:</strong> <strong>CRITICAL:</strong> Implement a separate process (e.g., scheduled Lambda/Glue job) to periodically compare stored forecasts against actual consumption data (loaded later) and calculate ongoing MAPE/RMSE. Log these metrics to CloudWatch or a dedicated monitoring dashboard.</p></li>
<li><p><strong>Data Pipeline Monitoring:</strong> Monitor success/failure of ingestion jobs providing raw data and the <code class="docutils literal notranslate"><span class="pre">process_edf_data</span></code> Glue job.</p></li>
<li><p><strong>Weather Forecast API:</strong> Monitor availability and error rates of the external weather forecast API.</p></li>
<li><p><strong>CloudWatch Alarms:</strong> Set alarms on: SFN Failures, Lambda Errors, Forecast Accuracy Degradation (MAPE/RMSE exceeding threshold), Weather API Fetch Failures, Target DB Write Errors.</p></li>
</ul>
</section>
<section id="estimated-monthly-costs">
<h3>Estimated Monthly Costs<a class="headerlink" href="#estimated-monthly-costs" title="Permalink to this heading">¶</a></h3>
<p><strong>Assumptions:</strong></p>
<ul class="simple">
<li><p><strong>Environment:</strong> AWS <code class="docutils literal notranslate"><span class="pre">eu-central-1</span></code> (Frankfurt) region.</p></li>
<li><p><strong>Buildings:</strong> 120.</p></li>
<li><p><strong>Data Volume:</strong> Building-level aggregation results in smaller processed data sizes compared to the raw sensor data for AD.</p>
<ul>
<li><p>Daily Processed EDF Data: ~10 MB.</p></li>
<li><p>Total Monthly Processed EDF Data: 10 MB/day * 30 days = <strong>~300 MB</strong>.</p></li>
</ul>
</li>
<li><p><strong>Model Training Frequency:</strong> 1 time per month.</p></li>
<li><p><strong>Batch Inference Frequency:</strong> 30 times per month (daily).</p></li>
<li><p><strong>API Serving Layer:</strong></p>
<ul>
<li><p>B2B API (Suppliers): Low volume, high value. Assume <strong>10,000 requests/month</strong>.</p></li>
<li><p>B2C API (Residents): Higher volume. Assume 3,000 apartments * 1 request/day * 30 days = <strong>90,000 requests/month</strong>.</p></li>
<li><p>Total API Requests: <strong>~100,000 per month</strong>.</p></li>
</ul>
</li>
</ul>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Pipeline Component</p></th>
<th class="head text-left"><p>AWS Service(s)</p></th>
<th class="head text-left"><p>Detailed Cost Calculation &amp; Rationale</p></th>
<th class="head text-left"><p>Estimated Cost (USD)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Data Ingestion &amp; Processing</strong></p></td>
<td class="text-left"><p><strong>AWS Glue</strong><br><strong>AWS Lambda</strong><br><strong>S3</strong></p></td>
<td class="text-left"><p><strong>AWS Glue ETL (Processing EDF Data):</strong> Daily job to aggregate raw data and join with weather/calendar. Less data than AD raw processing.<br>- 1 job/day * 30 days * 0.15 hours/job * 4 DPUs * ~$0.44/DPU-hr = <strong>~$7.92</strong><br><br><strong>AWS Lambda (Ingesting Raw Data):</strong> Lambdas for fetching weather forecasts, historical weather, etc.<br>- Assume 5 functions, ~100k total invocations/month, avg 10 sec duration, 256MB. All usage is well within the Lambda free tier. = <strong>~$0.00</strong><br><br><strong>S3 (PUT Requests):</strong> Lower volume than AD due to aggregation.<br>- ~1000 PUT req/day * 30 days * ~$0.005/1k req = <strong>~$0.15</strong></p></td>
<td class="text-left"><p><strong>$8 - $15</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Feature Engineering</strong></p></td>
<td class="text-left"><p><strong>SageMaker Processing</strong></p></td>
<td class="text-left"><p><strong>SageMaker Processing Jobs:</strong> Priced per instance-second. Covers feature engineering for both the monthly training and daily inference pipelines.<br>- Training (monthly): 1 run/month * 1.0 hour/run * 1 <code class="docutils literal notranslate"><span class="pre">ml.m5.large</span></code> instance * ~$0.11/hr = <strong>~$0.11</strong><br>- Inference (daily): 30 runs/month * 0.25 hours/run * 1 <code class="docutils literal notranslate"><span class="pre">ml.m5.large</span></code> instance * ~$0.11/hr = <strong>~$0.83</strong></p></td>
<td class="text-left"><p><strong>$1 - $3</strong></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Model Training</strong></p></td>
<td class="text-left"><p><strong>SageMaker Training</strong><br><strong>Step Functions</strong></p></td>
<td class="text-left"><p><strong>SageMaker Training Jobs:</strong> Priced per instance-second. Assuming monthly retraining on a standard instance.<br>- 1 run/month * 1.5 hours/run * 1 <code class="docutils literal notranslate"><span class="pre">ml.m5.large</span></code> instance * ~$0.11/hr = <strong>~$0.17</strong><br><br><strong>Step Functions:</strong> The training workflow runs infrequently.<br>- ~10 transitions/run * 1 run/month = 10 transitions. Well within free tier. = <strong>~$0.00</strong></p></td>
<td class="text-left"><p><strong>$1 - $2</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Model Inference</strong></p></td>
<td class="text-left"><p><strong>SageMaker Processing</strong><br><strong>Step Functions</strong></p></td>
<td class="text-left"><p><strong>SageMaker Processing Job (Forecast Generation):</strong> We use a Processing Job for flexibility. This is the daily forecasting job.<br>- 30 runs/month * 0.5 hours/run * 1 <code class="docutils literal notranslate"><span class="pre">ml.m5.large</span></code> instance * ~$0.11/hr = <strong>~$1.65</strong><br><br><strong>Step Functions:</strong> The inference workflow runs daily.<br>- ~8 transitions/run * 30 runs/month = 240 transitions. Well within free tier. = <strong>~$0.00</strong></p></td>
<td class="text-left"><p><strong>$2 - $4</strong></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Forecast Storage &amp; Serving</strong></p></td>
<td class="text-left"><p><strong>API Gateway</strong><br><strong>AWS Lambda</strong><br><strong>Amazon Timestream</strong></p></td>
<td class="text-left"><p><strong>API Gateway:</strong> Priced per million requests.<br>- 100,000 requests/month is well within the 1M free tier requests (for REST APIs). = <strong>~$0.00</strong><br><br><strong>AWS Lambda (Serving):</strong> Lambdas for serving B2B/B2C requests.<br>- 100k invocations/month * avg 150ms duration * 256MB memory. All usage well within free tier. = <strong>~$0.00</strong><br><br><strong>Amazon Timestream:</strong> Priced per GB of ingestion, storage, and queries.<br>- Ingestion: 30 days * 120 bldgs * 72 hr fcst * ~1KB/record = ~260 MB ingest/month. Free tier is 1GB. = <strong>~$0.00</strong><br>- Memory Store: Small, rolling window of recent data. ~1 GB * ~$0.036/GB-hr * 720 hrs = <strong>~$25.92</strong> (This can be high). <strong>Let’s assume we reduce memory retention to 1 day = ~$3.60</strong><br>- Magnetic Store: ~3 GB/year. ~0.3 GB * ~$0.03/GB-month = <strong>~$0.01</strong><br>- Queries: 100k requests * ~10MB scanned/query (estimate) = ~1TB queries. $0.01/GB * 1024 GB = <strong>~$10.24</strong></p></td>
<td class="text-left"><p><strong>$15 - $25</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Storage &amp; Logging</strong></p></td>
<td class="text-left"><p><strong>S3</strong><br><strong>ECR</strong><br><strong>CloudWatch</strong></p></td>
<td class="text-left"><p><strong>S3:</strong> Priced per GB-month. Lower processed/feature data volume than AD, but still need to store raw data.<br>- Assume total storage for Raw, Processed, Features, Artifacts = ~380 GB<br>- 380 GB * ~$0.023/GB-month = <strong>~$8.74</strong><br>- Add ~$2 for GET/LIST requests.<br><br><strong>ECR:</strong> Priced per GB-month. For the EDF container image.<br>- Assume a separate 10 GB storage for EDF images. = <strong>~$1.00</strong><br><br><strong>CloudWatch:</strong> Logs from all jobs and Lambdas.<br>- Assume ~10 GB log ingestion * ~$0.50/GB = <strong>~$5.00</strong></p></td>
<td class="text-left"><p><strong>$15 - $25</strong></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Total Estimated Monthly Cost</strong></p></td>
<td class="text-left"><p><strong>-</strong></p></td>
<td class="text-left"><p><strong>-</strong></p></td>
<td class="text-left"><p><strong>$42 - $74</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p>This detailed breakdown reveals that the highest operational cost for the forecasting system is the <strong>Forecast Storage &amp; Serving</strong> layer, specifically the <strong>Timestream</strong> memory store and query costs. The batch ML compute costs for feature engineering, training, and inference remain very low. This highlights the importance of optimizing the database configuration (e.g., memory vs. magnetic retention) and query patterns for the serving APIs to manage costs effectively.</p>
</section>
<section id="challenges-and-learnings">
<h3>Challenges and learnings<a class="headerlink" href="#challenges-and-learnings" title="Permalink to this heading">¶</a></h3>
<!--

---

### Challenge 1: The "Sunny Winter Day" Problem - Model Inability to Generalize to Unseen Conditions

**Chronological Events:**

1.  **Initial Model Success (First 6 Months):** The team develops a champion XGBoost model (Exp #7) for forecasting building-level energy demand. It's trained on one full year of historical data and performs well in backtesting, achieving a respectable ~8% MAPE. The model's feature importance, analyzed with SHAP, shows that `temperature_c`, `hour_of_day`, and `consumption_lag_24h` are the dominant drivers, as expected. The model is deployed, and the daily inference pipeline begins providing forecasts to a pilot group of energy suppliers.

2.  **The First Catastrophic Failure (First Winter in Production):** In late November, a weather pattern emerges that was rare in the single year of training data: a week of unusually sunny but extremely cold days. The daily forecast accuracy plummets. The model consistently and significantly *under-predicts* energy demand for the entire week, with the MAPE spiking to over 30%.

3.  **High-Stakes Business Impact:** The pilot energy supplier is furious. Relying on the forecast, they procured insufficient energy on the spot market and had to buy expensive peak-load power to cover the shortfall, incurring significant financial losses. They threaten to pull out of the pilot. The internal product and marketing teams are under immense pressure to explain the failure.

4.  **Initial Debugging - No Obvious Errors:** The MLOps team investigates. The inference pipeline ran successfully with no errors. The input weather forecast data from the API was accurate—it correctly predicted sunny and cold conditions. The model health checks passed. Tracing the Step Function execution shows everything worked as designed. The problem wasn't a technical failure; it was a **model failure**.

5.  **Deep Dive with SHAP and Feature Analysis (Week 2 of Failure):** The ML team pulls the inference feature data and the model's predictions for the failure period. They run SHAP analysis on the specific predictions and discover the root cause:
    *   **Feature Interaction Problem:** In the training data, the feature `solar_irradiance_ghi` was highly correlated with `temperature_c` (sunny days were generally warm). The model learned a strong, simple interaction: `if solar_irradiance is HIGH -> then energy_demand is LOW` (because less heating is needed and residents might be out).
    *   **Failure to Generalize:** When faced with a new, unseen combination of `HIGH solar_irradiance` and `LOW temperature_c` (a sunny winter day), the model incorrectly gave too much weight to the `solar_irradiance` feature. It predicted low energy demand because it was sunny, failing to account for the overriding effect of the extreme cold. It had learned a correlation, not the true causal relationship.

6.  **Exploring Solutions (Week 3):** The team brainstorms how to fix this fundamental generalization issue:
    *   **Option A (More Data):** Wait several years to collect more examples of "sunny winter days". This is unacceptable from a business perspective.
    *   **Option B (Better Features):** Engineer features that are more robust to these interactions. The team experiments with creating interaction terms explicitly (e.g., `temperature * solar_irradiance`) and non-linear transformations of temperature (e.g., polynomial features) to help the model learn that the effect of temperature is not linear and dominates at cold extremes.
    *   **Option C (Different Model):** Re-evaluate models that might handle these relationships differently. Could a simpler model like Prophet (with seasonality and weather regressors) be less prone to this specific error? Could an LSTM capture the temporal context better?

7.  **Resolution: Hybrid Approach (Week 4-5):**
    *   **Feature Engineering (Primary Fix):** The team implements **Heating Degree Days (HDD)** as a new feature. HDD is a non-linear transformation of temperature specifically designed to measure heating requirements (`max(0, 18°C - outdoor_temp)`). This single feature is much more causally linked to heating demand than raw temperature.
    *   **Model Retraining:** They retrain the XGBoost model with the new HDD feature and other interaction terms. In offline testing on the problematic week, the new model's MAPE drops from 30% to ~12%, a massive improvement.
    *   **Monitoring Enhancement:** They add a new monitoring check to the inference pipeline: an "out-of-distribution" (OOD) alert. This check compares the joint distribution of key input features for the upcoming forecast (e.g., `temperature_c` vs. `solar_irradiance_ghi`) against the distributions seen in the training data. If a highly unusual combination is detected, it flags the forecast with a "low confidence" warning, even if the model produces a prediction. This manages stakeholder expectations.

**Lessons & Retrospective:**

*   **"What were the challenges?":** "Our biggest challenge was a catastrophic model failure in production due to the model's inability to generalize to weather conditions that were rare in our training data. The model learned a spurious correlation between sunshine and warmth, which led to significant under-prediction during a 'sunny but cold' week, causing direct financial impact for our partner. The incident was hard to debug because it wasn't a code error but a fundamental data science problem."
*   **"What would you change?":** "I would place a much stronger emphasis on **stress-testing the model against plausible but unseen feature combinations** during the evaluation phase, before ever deploying to production. Instead of just relying on a historical backtest split, we should have synthesized adversarial data points (like high sun/low temp, low sun/high temp) to check for logical consistency in the model's predictions. Furthermore, I would implement input data monitoring for out-of-distribution detection from day one."

---

### Challenge 2: The Holiday Effect Mismatch - Aligning Model Features with Business Reality

**Chronological Events:**

1.  **Initial Implementation:** The team uses a standard Python library to generate a list of Swiss national and cantonal holidays. This is added as a binary `is_holiday_flag` feature to the Prophet and XGBoost models.
2.  **Observed Errors:** During the first year of operation, the team notices recurring large forecast errors around specific holidays.
    *   **Christmas Week:** The model accurately predicts the dip on Christmas Day (Dec 25th) and the 26th, but it massively *over-predicts* demand for the 24th and the 27th-30th.
    *   **"Bridge" Days:** On days falling between a weekend and a public holiday (e.g., a Monday before a Tuesday holiday), the model predicts normal weekday consumption, but actual consumption is much lower.
    *   **School Holidays:** The model completely misses the sustained, slightly-higher-than-normal consumption patterns during regional school holiday weeks, which are not official public holidays.
3.  **Business Impact:** While not as catastrophic as the winter failure, these errors reduce trust with suppliers, who need to plan for multi-day events, not just single holidays. Internally, the "smart energy advice" feature gives poor recommendations to residents during these periods.
4.  **Problem Diagnosis:** The ML team realizes that a simple `is_holiday` flag is a naive representation of human behavior. The "holiday effect" is not a single-day event.
    *   For Christmas, the effect is a *period* of lower commercial/industrial demand but potentially different residential patterns, starting before the day itself and lasting until the New Year.
    *   "Bridge" days act like semi-holidays.
    *   School holidays change the daytime occupancy patterns of families.
5.  **Reframing the Feature:** The problem is reframed from "Is this day a holiday?" to "What *type* of day is this from a socio-economic behavior perspective?".
6.  **Solution: Advanced Calendar Features & Custom Holiday Modeling (Iteration):**
    *   **Data Enrichment:** The team works with the product manager to identify and source data for all major Swiss school holiday periods by canton.
    *   **Feature Engineering:** They replace the single `is_holiday_flag` with a richer set of features:
        *   `days_until_next_holiday`: A continuous feature counting down to the next major holiday.
        *   `days_since_last_holiday`: A continuous feature counting up from the last major holiday.
        *   `is_bridge_day`: A binary flag for "bridge" days.
        *   `is_school_holiday`: A binary flag.
        *   `holiday_name`: A categorical feature (e.g., 'Christmas', 'Easter', 'NationalDay').
    *   **Model-Specific Implementation:**
        *   **For Prophet:** They create a custom `holidays` dataframe that includes not just the holiday itself but also the surrounding days with different names (e.g., `Christmas_Eve`, `Christmas_Day`, `Boxing_Day`, `Christmas_Week`). Prophet can learn a separate effect for each of these "holidays".
        *   **For XGBoost:** They add the new engineered features (`days_until...`, `is_bridge_day`, etc.) directly to the feature set.
7.  **Results & Validation:** After retraining the models with these new, richer calendar features, the forecast accuracy around holiday periods improves dramatically. The MAPE for the Christmas week drops from over 25% to around 10%. The models now correctly anticipate the pre-holiday slowdown and the "bridge day" effect.

**Lessons & Retrospective:**

*   **"What were some of the challenges?":** "A key challenge was accurately modeling human behavior around holidays. Our initial approach of using a simple binary holiday flag was too simplistic and failed to capture the complex, multi-day nature of events like Christmas or the subtle effects of school holidays. This led to significant and predictable forecast errors, damaging stakeholder trust."
*   **"If you were to start over...":** "I would collaborate with a product manager or domain expert from the very beginning to build a 'behavioral calendar' instead of just using a standard holiday library. We should have brainstormed all the event types that affect energy consumption—public holidays, school holidays, bridge days, major local events—and designed features to capture them explicitly. It's a lesson in not just using off-the-shelf features but thinking deeply about how to translate real-world context into a format the model can understand."

---

### Challenge 3: The API Latency Crisis - Mismatch Between Batch Model & Real-Time Needs

**Chronological Events:**

1.  **Initial Design:** The inference pipeline is designed as a daily batch process. It runs overnight, generating a 72-hour forecast for all buildings, which is then loaded into an Amazon Timestream database. An API Gateway + Lambda is built to serve these stored forecasts to the B2C mobile app.
2.  **New Feature Request:** The Product team, excited by the forecasts, designs a new "real-time cost optimization" feature for the mobile app. The idea is that a user can see the building's current demand and the next few hours' forecast to decide whether to run their washing machine *now* or in an hour. This requires the API to be fast and reflect the latest data.
3.  **The Latency Problem:** When the feature is prototyped, the UX is poor. The API call to the Lambda function, which then queries Timestream, sometimes takes 2-3 seconds. The main issue is the query itself: to construct a user-friendly chart, the Lambda needs to pull 24-48 hourly data points for a specific building, which can be a moderately heavy query for Timestream when invoked frequently by thousands of users simultaneously.
4.  **Initial Solution - Caching:** The team's first reaction is to add a caching layer. They implement an ElastiCache (Redis) cluster in front of the Timestream database. The Lambda now first checks Redis for the forecast; if it's a miss, it queries Timestream and then populates the cache. This improves latency for repeated requests for the same building but introduces cache invalidation complexity and adds significant infrastructure cost (24/7 Redis cluster).
5.  **The "Stale Forecast" Problem:** A bigger issue arises. The batch forecast is generated only once per day (at 5 AM). If a major, un-forecasted weather event occurs at 2 PM (e.g., a sudden thunderstorm darkens the sky), the solar generation plummets and building consumption spikes. The forecast shown in the app is now completely wrong and won't be updated until the next day. Users complain that the "smart" feature is not very smart.
6.  **Re-evaluating the Architecture:** The team realizes they have an architectural mismatch. They have a **batch inference system** trying to serve a **near real-time use case**. The 24-hour cycle is too slow.
7.  **The Final Solution - A Hybrid "Serve & Refresh" Pattern:**
    *   **Keep the Batch System:** The daily, full 72-hour forecast generated by the complex XGBoost/Prophet model is still valuable for the B2B use case (suppliers) and for setting the daily baseline. It continues to load into Timestream.
    *   **Create a Lightweight "Refresher" Model:** The ML team develops a much simpler, faster model (e.g., a simple Linear Regression or a very lightweight Prophet model). Its only job is to provide a *short-term correction* to the main forecast based on the *most recent* (e.g., last 1-2 hours) consumption data and updated near-term weather forecasts.
    *   **New "Refresh" Pipeline:** They create a lightweight, serverless pipeline (e.g., an EventBridge rule triggering a Lambda function every 15-30 minutes). This Lambda:
        1.  Fetches the last hour of actual consumption.
        2.  Calculates the recent forecast error (Actual - Baseline Forecast).
        3.  Uses the simple "refresher" model to predict the next 1-3 hours' correction.
        4.  Writes this *updated* short-term forecast (Baseline + Correction) to a **different, low-latency storage: DynamoDB**, which is optimized for fast key-value lookups.
    *   **Update the API:** The B2C API Lambda is changed. It now queries DynamoDB first to get the latest, frequently refreshed forecast for the next few hours. For the longer-term forecast (e.g., hours 4-24), it falls back to querying Timestream (or the Redis cache).

8.  **Result:** The user-facing feature becomes much more responsive and accurate. API latency for the critical next-few-hours forecast drops to <100ms. The system can now react to intra-day deviations from the baseline forecast. This hybrid architecture successfully serves both the long-term B2B planning needs and the short-term B2C real-time needs without a complete, costly overhaul to a fully real-time inference system.

**Lessons & Retrospective:**

*   **"Production bugs that made you regret decisions?":** "My regret was not clarifying the specific latency and freshness requirements for *all* potential use cases at the beginning. We designed a perfect batch system, but the business wanted a real-time feature. This led to a series of costly patches (like adding ElastiCache) before we finally addressed the core architectural mismatch. We tried to solve a data freshness problem with a caching solution, which was the wrong approach."
*   **"What would you change?":** "I would have a dedicated discussion about the 'Time-to-Live' and 'Time-to-Action' for our forecasts. For the B2B use case, a 24-hour TTL is fine. For the B2C feature, the TTL was less than an hour. Recognizing this difference upfront would have led us to design the hybrid 'Serve & Refresh' architecture from the start, saving us the time and expense of the intermediate caching solution and avoiding stakeholder frustration."



### Deep Dive: The "New Commissioning" Cold Start Challenge

**The Core Problem:**

The most effective models developed (both for Anomaly Detection and Forecasting) are data-hungry. They rely on a sufficient history of consumption data for a specific building or apartment to learn its unique "normal" patterns, including its thermal response to weather, seasonal cycles, and typical resident behavior. A newly commissioned building has no such history. This creates a high-stakes paradox: the period when predictive monitoring is *most valuable* (identifying early-life failures, incorrect installations, or commissioning errors) is precisely when the primary data-driven models are at their *weakest*.

**Related Challenges Stemming from the Cold Start Problem:**

1.  **Inability to Establish a Baseline:**
    *   **Anomaly Detection:** Models like LOF, Isolation Forest, and forecasting-based methods (Prophet, XGBoost) have no concept of a "normal" baseline for the new building. Any initial behavior could be misinterpreted. A low-consumption building might be flagged as anomalous when compared to the global average, even if its behavior is perfectly normal for its high-efficiency design.
    *   **Forecasting:** Models cannot learn the building's specific thermal inertia, response to solar gain, or the typical daily/weekly rhythm of its residents. Forecasts will be highly inaccurate.

2.  **High Rate of "False Alarms":**
    *   **Initial Occupancy Flux:** In the first few weeks or months, apartments are gradually occupied. The building's aggregate consumption pattern is unstable and constantly changing as more residents move in. A model trying to find a stable pattern will constantly be "surprised," leading to a storm of false positive anomalies.
    *   **System "Burn-in":** The heating and ventilation systems themselves might be undergoing initial calibration or "burn-in," exhibiting patterns that will not be present later in its operational life. An ML model will flag these transient but normal behaviors as anomalous.

3.  **Risk of Masking True Anomalies:**
    *   The high noise and instability of the initial data can mask a true, critical anomaly. If everything looks anomalous, then nothing stands out. A genuinely faulty valve causing a leak might be lost in the noise of alerts generated by residents moving in and adjusting their thermostats for the first time.

4.  **Ineffective Transfer of Global Knowledge:**
    *   As we saw in the "Multi-Building Dilemma" challenge, a single global model performs poorly on buildings that deviate from the average. Simply applying a global model to a new building is a recipe for failure, especially if the new building is of a different archetype (e.g., a new high-efficiency design).

---

### Potential Solutions: A Tiered, Multi-faceted Approach

A robust solution to the cold start problem is not a single model but a **graceful, evolving strategy** that provides the best possible insight at each stage of data availability.

**Tier 1: The Zero-Data & Commissioning Phase (First ~4 Weeks)**

At this stage, we have no historical consumption data. The focus is on physics-based and rule-based checks.

*   **Solution: Physics-Informed Heuristics & Commissioning Rules:**
    *   **How it Works:** Instead of relying on learned patterns, we use simple, engineering-driven rules based on the physical constraints of the system. This is a form of expert system.
    *   **Implementation:** A separate, simpler rules engine (which could be a Lambda function or a specific configuration for our existing models) would check for violations of "common sense" heating logic.
    *   **Example Rules for Anomaly Detection:**
        *   **"Stuck Valve" Rule:** If `heating_kwh > 0` for an apartment for `X` consecutive hours while `room_temp_c > setpoint_temp_c + 2°C`, flag a high-priority alert. This suggests the valve is stuck open.
        *   **"No Response" Rule:** If `setpoint_temp_c > room_temp_c + 2°C` and `heating_kwh > 0` for `Y` consecutive hours, but `room_temp_c` does not increase, flag an alert. This suggests a blockage or failure to deliver heat.
        *   **"Sensor Mismatch" Rule:** If `room_temp_c` for one apartment is drastically different (>5°C) from all its neighbors in the same building for a prolonged period, flag a potential sensor failure.
    *   **Forecasting:** No complex forecasting is possible. The "forecast" could simply be a static profile based on the building's design specifications, adjusted by the real-time weather forecast. This is more of a "plan" than a learned forecast.

**Tier 2: The Scant-Data Phase (Weeks 4 to ~6 Months) - Bayesian & Archetype Models**

We now have some data, but not enough for robust, individualized models. The goal is to combine this limited local data with knowledge from existing, similar buildings.

*   **Solution: Bayesian Priors & Transfer Learning from Archetypes:**
    *   **How it Works (Bayesian Intuition):** A Bayesian approach starts with a "prior belief" about how a system should behave and updates this belief as it sees new data. Our "prior belief" can be derived from data from older, similar buildings.
        *   **Anomaly Detection:** Instead of a simple regression (`Energy = B * HDD`), we use a Bayesian Linear Regression. The *prior* for the coefficient `B` (the building's sensitivity to weather) is not a single value but a *distribution* (e.g., a normal distribution) whose mean and variance are calculated from all existing "Modern High-Density" buildings. As we collect the first few weeks of data for the new building, the model updates this distribution to find a "posterior" `B` that is a sensible compromise between the archetype average and the new building's own (limited) data. An anomaly is a point that is unlikely given this posterior model.
        *   **Forecasting:** Similarly, a Bayesian forecasting model (like Prophet, which has a Bayesian backend) can be initialized with stronger priors for seasonality and trend parameters, where these priors are derived from the average parameters of the building's archetype.
    *   **How it Works (Simple Transfer Learning):**
        *   **Anomaly Detection/Forecasting:** Train a model (e.g., XGBoost) on all data from a specific building archetype (e.g., "High-Efficiency"). Use this pre-trained model to generate initial predictions for the new building. As more data comes in from the new building, this model can be fine-tuned using the new data, or we can slowly blend its predictions with a new model trained only on the new building's data.
    *   **Implementation:**
        *   Requires a robust system for **classifying buildings into archetypes** based on metadata (age, size, insulation, heating system type). This becomes a critical data governance task.
        *   The MLOps pipeline needs to be able to select these "archetype models" or "Bayesian priors" for any building that is less than `N` months old.

**Tier 3: The Mature Data Phase (6+ Months)**

The building now has sufficient historical data to stand on its own.

*   **Solution: Transition to Fully Individualized Models:**
    *   **How it Works:** The system automatically transitions from using the archetype/Bayesian model to a model trained specifically on that building's (or its apartments') own data.
    *   **Implementation:**
        *   The model training pipeline (e.g., `ADTrainingWorkflow`) is triggered for this specific building once it crosses the data threshold (e.g., 6 months of data).
        *   The inference pipeline's model selection logic (`GetApprovedModelPackage` and the subsequent steps) is updated to now look for a dedicated model for this building. If one exists and is approved, it uses it; otherwise, it falls back to the Tier 2 archetype model.
        *   This creates a seamless handoff from the cold-start solution to the mature-phase solution.

**Summary of the Evolving Strategy:**

| Data Availability | Primary Approach | Models / Techniques | Key Requirement |
| :--- | :--- | :--- | :--- |
| **0-4 Weeks** | Physics-Informed Rules | Heuristics, simple thresholds, sensor cross-validation. | Well-defined engineering rules. |
| **1-6 Months** | Transfer Learning / Bayesian Priors | Archetype-based models (XGBoost, Prophet), Bayesian Regression with priors from archetypes. | Accurate building metadata and a robust archetype classification system. |
| **6+ Months** | Individualized Modeling | Fully trained models (XGBoost, Prophet, etc.) using only the building's/apartment's own historical data. | Automated trigger to graduate a building from the cold-start strategy to the mature strategy. |

This tiered approach directly addresses the high-stakes nature of new buildings by providing the best possible analysis at each stage, gracefully improving as more data becomes available, and managing stakeholder expectations by acknowledging the initial limitations of a purely data-driven approach.

___

**The Source of Confusion:**

*   In the "Multi-Building Dilemma" challenge, we concluded that a single global model was bad and the best solution was **Models per Building Archetype**.
*   In the "Cold Start Challenge," we concluded that after 6+ months, the best solution was **Individualized Models** (i.e., one model per building/apartment).

This seems like a contradiction. Here’s how they fit together in a single, coherent lifecycle.


### Clarified Hybrid Model Strategy (for both AD and EDF)

The system does not use just one strategy; it uses a **three-stage lifecycle for modeling**, where each building/apartment automatically "graduates" to the next stage as it accumulates enough data.

**Stage 1: Heuristics-Only (e.g., Age < 1 Month)**

*   **Model Used:** **No ML model.** Only the physics-informed rules engine is active.
*   **Applies to:** Brand new buildings with virtually no stable data.
*   **Rationale:** At this stage, any learned pattern is meaningless. We rely on deterministic, engineering-based rules to catch gross failures (e.g., "valve stuck open," "sensor reading is physically impossible"). This provides a basic safety net.

**Stage 2: Archetype Model (e.g., Age 1-6 Months)**

*   **Model Used:** **One model per Building Archetype.**
*   **Applies to:** Buildings with some data, but not enough to capture their unique seasonal patterns or long-term behavior reliably. This is the **primary cold-start solution**.
*   **Rationale:** The building's limited data is not used for *training*, but for *inference*. We use a robust model pre-trained on a large dataset from similar, older buildings (the "archetype"). This provides a reasonably accurate "prior belief" or "best guess" for how this new building *should* behave based on its type. It's vastly superior to a global model and the only viable ML approach when historical data is scant.

**Stage 3: Individualized Model (e.g., Age > 6 Months)**

*   **Model Used:** **One model per Building/Apartment.**
*   **Applies to:** Buildings with sufficient historical data (e.g., 6+ months, ideally including a full heating season).
*   **Rationale:** Once a building has enough of its own history, a model trained *specifically on its own data* will almost always be more accurate than a general archetype model. It can learn the unique thermal properties, specific solar gain profile, and the distinct rhythm of its residents. This is the mature, steady-state, and most accurate phase.

---

**How This Resolves the Contradiction:**

*   The conclusion of the **"Multi-Building Dilemma"** (use archetype models) is our primary strategy for **Stage 2 (the cold-start phase)**.
*   The conclusion of the **"Cold Start Challenge"** (use individualized models) is our strategy for **Stage 3 (the mature phase)**.

The overall strategy is **NOT** to choose just one of these forever, but to have an **automated system that graduates a building from Stage 1 to Stage 2 to Stage 3 over time.**

**Is this the same for both Anomaly Detection and Energy Demand Forecasting?**

**Yes, absolutely.** This three-stage lifecycle applies equally well to both use cases because both suffer from the exact same cold-start problem.

*   **AD Inference:** To detect an anomaly in a 3-month-old building, the inference pipeline would fetch the **"Archetype Model"** for that building's type from the Model Registry.
*   **EDF Inference:** To forecast demand for a 3-month-old building, the inference pipeline would fetch the **"Archetype Forecast Model"** for that building's type.
*   **AD/EDF Training:** The training pipelines are triggered differently. An **"Archetype Model"** is retrained periodically (e.g., monthly) using data from *all mature buildings within that archetype*. An **"Individualized Model"** is trained for a specific building only after it has accumulated enough data (e.g., the first training run is triggered at month 6).

**Implementation in the MLOps Pipeline:**

This strategy requires a key piece of logic in the **Inference Pipeline**, specifically in the model selection phase:

1.  **Input:** The inference workflow starts, knowing the `building_id`.
2.  **Metadata Lookup:** The first step is to query a metadata source (e.g., the Topology database) to get the `commissioning_date` and `archetype` for the given `building_id`.
3.  **Calculate Age:** `age = today - commissioning_date`.
4.  **Conditional Logic (Model Selection):**
    *   `if age < 1 month`: Route to a simple rules engine (or do nothing).
    *   `else if age >= 1 month AND age < 6 months`: The `GetApprovedModelPackage` Lambda is called with parameters to find the latest approved model for that specific **archetype**.
    *   `else (age >= 6 months)`: The `GetApprovedModelPackage` Lambda is called with parameters to find the latest approved model for that specific **`building_id`**. If no dedicated model exists yet (because its first training run hasn't completed), it falls back to the archetype model.

This provides a clear, consistent, and robust strategy that handles the entire lifecycle of a building from commissioning to maturity for both ML use cases. I hope this resolves the confusion

-->
</section>
<section id="troubleshooting-guide">
<h3>Troubleshooting Guide<a class="headerlink" href="#troubleshooting-guide" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>SFN Failures:</strong> Check execution history for failed state, input/output, error message.</p></li>
<li><p><strong>Job Failures (Processing/Training):</strong> Check CloudWatch Logs for the specific job run. Look for Python errors, resource exhaustion, S3 access issues.</p></li>
<li><p><strong>Lambda Failures:</strong> Check CloudWatch Logs. Verify IAM permissions, input payload structure, environment variables, timeouts, memory limits. Check DLQ if configured.</p></li>
<li><p><strong>Forecast Accuracy Issues:</strong></p>
<ul class="simple">
<li><p>Verify quality/availability of input weather forecasts.</p></li>
<li><p>Check feature engineering logic for errors or skew vs. training.</p></li>
<li><p>Analyze residuals from the model evaluation step.</p></li>
<li><p>Check if model drift has occurred (compare recent performance to registry metrics). Trigger retraining if needed.</p></li>
<li><p>Ensure correct model version was loaded by inference pipeline.</p></li>
</ul>
</li>
<li><p><strong>Data Loading Issues (Timestream/RDS):</strong> Check <code class="docutils literal notranslate"><span class="pre">LoadForecastsToDB</span></code> Lambda logs for database connection errors, write throttling, data type mismatches, constraint violations. Check DB metrics.</p></li>
<li><p><strong>API Serving Issues:</strong> Check API Gateway logs and metrics. Check serving Lambda logs. Verify DB connectivity and query performance.</p></li>
</ol>
</section>
<section id="security-considerations">
<h3>Security Considerations<a class="headerlink" href="#security-considerations" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Apply IAM least privilege to all roles.</p></li>
<li><p>Encrypt data at rest (S3, Timestream/RDS, EBS) and in transit (TLS).</p></li>
<li><p>Use Secrets Manager for any API keys (e.g., weather provider).</p></li>
<li><p>Secure API Gateway endpoints (Authentication - Cognito/IAM/API Keys, Authorization, Throttling).</p></li>
<li><p>Perform regular vulnerability scans on the EDF container image.</p></li>
<li><p>Consider VPC deployment and endpoints for enhanced network security.</p></li>
</ul>
</section>
<section id="roadmap-future-enhancements">
<h3>Roadmap &amp; Future Enhancements<a class="headerlink" href="#roadmap-future-enhancements" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Implement probabilistic forecasting (prediction intervals).</p></li>
<li><p>Incorporate more granular data (e.g., appliance recognition, improved occupancy detection) if available.</p></li>
<li><p>Explore more advanced forecasting models (LSTM, TFT) and benchmark rigorously.</p></li>
<li><p>Implement automated retraining triggers based on monitored forecast accuracy drift.</p></li>
<li><p>Develop more sophisticated XAI for forecasting (feature importance).</p></li>
<li><p>Add A/B testing framework for forecast models.</p></li>
<li><p>Integrate forecasts with building control systems (e.g., HVAC pre-cooling based on forecast).</p></li>
</ul>
</section>
<section id="appendices">
<h3>Appendices<a class="headerlink" href="#appendices" title="Permalink to this heading">¶</a></h3>
<section id="configuration-file-example">
<h4>Configuration File Example<a class="headerlink" href="#configuration-file-example" title="Permalink to this heading">¶</a></h4>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># config/.yaml</span>

<span class="c1"># --- General Settings ---</span>
<span class="nt">project_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ml&quot;</span>
<span class="nt">aws_region</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;eu-central-1&quot;</span>
<span class="c1"># env_suffix will likely be passed dynamically or set per deployment environment</span>

<span class="c1"># --- Data Paths (Templates - Execution specific paths often constructed) ---</span>
<span class="c1"># Base paths defined here, execution IDs/dates appended by scripts/workflows</span>
<span class="nt">s3_processed_edf_path</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;s3://{processed_bucket}/processed_edf_data/&quot;</span>
<span class="nt">s3_raw_weather_fcst_path</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;s3://{raw_bucket}/edf-inputs/weather-forecast/&quot;</span>
<span class="nt">s3_raw_calendar_path</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;s3://{raw_bucket}/edf-inputs/calendar-topology/&quot;</span>
<span class="nt">s3_feature_output_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;s3://{processed_bucket}/features/edf/{workflow_type}/{sfn_name}/{exec_id}/&quot;</span><span class="w"> </span><span class="c1"># workflow_type=training/inference</span>
<span class="nt">s3_model_artifact_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;s3://{processed_bucket}/model-artifacts/{sfn_name}/{exec_id}/&quot;</span>
<span class="nt">s3_eval_report_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;s3://{processed_bucket}/evaluation-output/{sfn_name}/{exec_id}/&quot;</span>
<span class="nt">s3_forecast_output_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;s3://{processed_bucket}/forecast-output/{sfn_name}/{exec_id}/&quot;</span>

<span class="c1"># --- AWS Resource Names (Base names - suffix added in Terraform locals) ---</span>
<span class="nt">scripts_bucket_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ml-glue-scripts&quot;</span><span class="w"> </span><span class="c1"># Base name for script bucket</span>
<span class="nt">processed_bucket_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ml-processed-data&quot;</span><span class="w"> </span><span class="c1"># Base name for processed bucket</span>
<span class="nt">raw_bucket_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ml-raw-data&quot;</span><span class="w"> </span><span class="c1"># Base name for raw bucket</span>
<span class="nt">edf_feature_group_name_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;edf-building-features&quot;</span>
<span class="nt">ecr_repo_name_edf_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;edf-training-container&quot;</span>
<span class="nt">edf_model_package_group_name_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;EDFBuildingDemandForecaster&quot;</span>
<span class="nt">lambda_register_edf_func_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;RegisterEDFModelFunction&quot;</span>
<span class="nt">lambda_load_forecasts_func_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;LoadEDFResultsLambda&quot;</span>
<span class="nt">lambda_get_model_func_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;GetApprovedModelLambda&quot;</span><span class="w"> </span><span class="c1"># Shared Lambda</span>
<span class="nt">lambda_create_sm_model_func_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;CreateSageMakerModelLambda&quot;</span><span class="w"> </span><span class="c1"># Shared Lambda</span>
<span class="nt">edf_training_sfn_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;EDFTrainingWorkflow&quot;</span>
<span class="nt">edf_inference_sfn_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;EDFInferenceWorkflow&quot;</span>
<span class="nt">edf_scheduler_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;DailyEDFInferenceTrigger&quot;</span>
<span class="nt">forecast_db_base</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;EDFDatabase&quot;</span><span class="w"> </span><span class="c1"># Timestream DB base name</span>
<span class="nt">forecast_table_name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;BuildingDemandForecasts&quot;</span><span class="w"> </span><span class="c1"># Timestream table name</span>

<span class="c1"># --- Feature Engineering (Common &amp; EDF Specific) ---</span>
<span class="nt">common_feature_eng</span><span class="p">:</span>
<span class="w">  </span><span class="nt">lookback_days_default</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">14</span><span class="w"> </span><span class="c1"># Default days history needed</span>

<span class="nt">edf_feature_eng</span><span class="p">:</span>
<span class="w">  </span><span class="nt">target_column</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;consumption_kwh&quot;</span>
<span class="w">  </span><span class="nt">timestamp_column</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;timestamp_hour&quot;</span>
<span class="w">  </span><span class="nt">building_id_column</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;building_id&quot;</span>
<span class="w">  </span><span class="nt">time_features</span><span class="p">:</span><span class="w"> </span><span class="c1"># Features derived from timestamp</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;hour_of_day&quot;</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;day_of_week&quot;</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;day_of_month&quot;</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;month_of_year&quot;</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;is_weekend&quot;</span><span class="w"> </span><span class="c1"># Example custom flag</span>
<span class="w">  </span><span class="nt">lag_features</span><span class="p">:</span><span class="w"> </span><span class="c1"># Lag values in hours</span>
<span class="w">    </span><span class="nt">consumption_kwh</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">24</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">48</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">168</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># 1d, 2d, 1wk</span>
<span class="w">    </span><span class="nt">temperature_c</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">24</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">48</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">168</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">solar_irradiance_ghi</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">24</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">rolling_window_features</span><span class="p">:</span><span class="w"> </span><span class="c1"># Window size in hours, aggregations</span>
<span class="w">    </span><span class="nt">consumption_kwh</span><span class="p">:</span>
<span class="w">      </span><span class="nt">windows</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">3</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">24</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">168</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="nt">aggs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;avg&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;stddev&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;min&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;max&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">temperature_c</span><span class="p">:</span>
<span class="w">      </span><span class="nt">windows</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">24</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="nt">aggs</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;avg&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">imputation_value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span><span class="w"> </span><span class="c1"># Value used for fillna after lags/windows</span>

<span class="c1"># --- Training Workflow ---</span>
<span class="nt">edf_training</span><span class="p">:</span>
<span class="w">  </span><span class="nt">default_strategy</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Prophet&quot;</span><span class="w"> </span><span class="c1"># Model strategy to use if not specified</span>
<span class="w">  </span><span class="nt">instance_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ml.m5.xlarge&quot;</span><span class="w"> </span><span class="c1"># Larger instance for potentially heavier training</span>
<span class="w">  </span><span class="nt">instance_count</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">max_runtime_seconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">7200</span><span class="w"> </span><span class="c1"># 2 hours</span>
<span class="w">  </span><span class="c1"># Base hyperparameters (can be overridden by execution input)</span>
<span class="w">  </span><span class="nt">hyperparameters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">Prophet</span><span class="p">:</span>
<span class="w">      </span><span class="nt">prophet_changepoint_prior_scale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.05</span>
<span class="w">      </span><span class="nt">prophet_seasonality_prior_scale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span>
<span class="w">      </span><span class="nt">prophet_holidays_prior_scale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span>
<span class="w">      </span><span class="nt">prophet_daily_seasonality</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">      </span><span class="nt">prophet_weekly_seasonality</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">      </span><span class="nt">prophet_yearly_seasonality</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;auto&#39;</span>
<span class="w">      </span><span class="c1"># prophet_regressors: [&quot;temperature_c&quot;, &quot;is_holiday_flag&quot;] # Example if using regressors</span>
<span class="w">    </span><span class="nt">XGBoost</span><span class="p">:</span>
<span class="w">      </span><span class="nt">xgb_eta</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="w">      </span><span class="nt">xgb_max_depth</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">      </span><span class="nt">xgb_num_boost_round</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">150</span>
<span class="w">      </span><span class="nt">xgb_subsample</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.7</span>
<span class="w">      </span><span class="nt">xgb_colsample_bytree</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.7</span>
<span class="w">      </span><span class="c1"># feature_columns must align with feature_engineering output for XGBoost</span>
<span class="w">      </span><span class="nt">feature_columns_string</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;temperature_c,solar_irradiance_ghi,humidity,is_holiday_flag,hour_of_day,day_of_week,day_of_month,month_of_year,consumption_lag_24h,consumption_lag_168h,consumption_roll_avg_24h&quot;</span>

<span class="c1"># --- Evaluation Workflow ---</span>
<span class="nt">edf_evaluation</span><span class="p">:</span>
<span class="w">  </span><span class="nt">instance_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ml.m5.large&quot;</span>
<span class="w">  </span><span class="nt">instance_count</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="c1"># Metrics thresholds for the &#39;CheckEvaluation&#39; choice state</span>
<span class="w">  </span><span class="nt">metrics_thresholds</span><span class="p">:</span>
<span class="w">    </span><span class="nt">max_mape</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20.0</span><span class="w"> </span><span class="c1"># Example: Fail if MAPE &gt; 20%</span>
<span class="w">    </span><span class="nt">max_rmse</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5.0</span><span class="w">  </span><span class="c1"># Example: Fail if RMSE &gt; 5 kWh (adjust based on typical consumption)</span>
<span class="w">  </span><span class="c1"># Optional: Path to historical labelled data for backtesting</span>
<span class="w">  </span><span class="c1"># historical_labels_path: &quot;s3://...&quot;</span>

<span class="c1"># --- Inference Workflow ---</span>
<span class="nt">edf_inference</span><span class="p">:</span>
<span class="w">  </span><span class="nt">scheduler_expression</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;cron(0</span><span class="nv"> </span><span class="s">5</span><span class="nv"> </span><span class="s">*</span><span class="nv"> </span><span class="s">*</span><span class="nv"> </span><span class="s">?</span><span class="nv"> </span><span class="s">*)&quot;</span><span class="w"> </span><span class="c1"># 5 AM UTC Daily</span>
<span class="w">  </span><span class="nt">scheduler_timezone</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;UTC&quot;</span>
<span class="w">  </span><span class="nt">forecast_horizon_hours</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">72</span>
<span class="w">  </span><span class="c1"># Processing job instance types (can override training defaults if needed)</span>
<span class="w">  </span><span class="nt">feature_eng_instance_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ml.m5.large&quot;</span>
<span class="w">  </span><span class="nt">feature_eng_instance_count</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">forecast_gen_instance_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ml.m5.large&quot;</span><span class="w"> </span><span class="c1"># Needs forecasting libs installed</span>
<span class="w">  </span><span class="nt">forecast_gen_instance_count</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="c1"># Target DB Config</span>
<span class="w">  </span><span class="nt">forecast_db_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;TIMESTREAM&quot;</span><span class="w"> </span><span class="c1"># TIMESTREAM | RDS | DYNAMODB</span>
<span class="w">  </span><span class="c1"># Lambda Config</span>
<span class="w">  </span><span class="nt">load_forecasts_lambda_memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span><span class="w"> </span><span class="c1"># MB</span>
<span class="w">  </span><span class="nt">load_forecasts_lambda_timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300</span><span class="w"> </span><span class="c1"># seconds</span>

<span class="c1"># --- Common Lambda Config ---</span>
<span class="c1"># Assuming shared Lambdas from AD are used</span>
<span class="nt">lambda_shared</span><span class="p">:</span>
<span class="w">   </span><span class="nt">get_model_memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="w">   </span><span class="nt">get_model_timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30</span>
<span class="w">   </span><span class="nt">create_sm_model_memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="w">   </span><span class="nt">create_sm_model_timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
</pre></div>
</div>
</section>
<section id="data-schemas">
<h4>Data Schemas<a class="headerlink" href="#data-schemas" title="Permalink to this heading">¶</a></h4>
<p>This appendix provides the formal schema definitions for the primary data entities used across the Anomaly Detection and Energy Demand Forecasting workflows.</p>
</section>
<section id="raw-meter-data">
<h4>1. Raw Meter Data<a class="headerlink" href="#raw-meter-data" title="Permalink to this heading">¶</a></h4>
<p>This represents the logical structure of data as it arrives from the central database into the S3 Raw Zone for processing. It’s often in a semi-structured format like JSON Lines.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Field Name</p></th>
<th class="head text-left"><p>Data Type</p></th>
<th class="head text-left"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">timestamp_str</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>ISO 8601 formatted timestamp (e.g., “2024-10-27T10:30:05Z”) of when the readings were recorded by the tablet.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">building_id</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>Unique identifier for the building (e.g., “bldg_A123”).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">apartment_id</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>Unique identifier for the apartment (e.g., “apt_404”).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">readings</span></code></p></td>
<td class="text-left"><p>Array[Object]</p></td>
<td class="text-left"><p>An array of sensor reading objects from the apartment.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">readings.sensor_type</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>The type of measurement (e.g., <code class="docutils literal notranslate"><span class="pre">heating_energy_kwh</span></code>, <code class="docutils literal notranslate"><span class="pre">room_temp_c</span></code>).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">readings.value</span></code></p></td>
<td class="text-left"><p>Double/Int</p></td>
<td class="text-left"><p>The numerical value of the sensor reading.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">readings.room_name</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>(Optional) The specific room for the reading, if applicable (e.g., “living_room”).</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Example (JSON Lines):</strong></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;timestamp_str&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2024-10-27T10:30:00Z&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;building_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;bldg_A123&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;apartment_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;apt_404&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;readings&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span><span class="nt">&quot;sensor_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;heating_energy_kwh&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">15432.7</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="nt">&quot;sensor_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;hot_water_litres&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">89541.2</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="nt">&quot;sensor_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;room_temp_c&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;room_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;living_room&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">21.5</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="nt">&quot;sensor_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;setpoint_temp_c&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;room_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;living_room&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">22.0</span><span class="p">}]}</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="processed-meter-data-for-anomaly-detection">
<h4>2. Processed Meter Data (for Anomaly Detection)<a class="headerlink" href="#processed-meter-data-for-anomaly-detection" title="Permalink to this heading">¶</a></h4>
<p>This is the output of the initial Ingestion Glue ETL job, stored in the S3 Processed Zone. It’s a flattened, structured table optimized for analytical queries and as the source for feature engineering.</p>
<p><strong>Format:</strong> Apache Parquet
<strong>Partitioned by:</strong> <code class="docutils literal notranslate"><span class="pre">year</span></code>, <code class="docutils literal notranslate"><span class="pre">month</span></code>, <code class="docutils literal notranslate"><span class="pre">day</span></code></p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Column Name</p></th>
<th class="head text-left"><p>Data Type</p></th>
<th class="head text-left"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">apartment_id</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>Unique identifier for the apartment.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">building_id</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>Unique identifier for the building.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">event_ts</span></code></p></td>
<td class="text-left"><p>Timestamp</p></td>
<td class="text-left"><p>The timestamp of the reading, cast to a proper timestamp type.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">heating_energy_kwh</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>The cumulative heating energy consumption in kilowatt-hours.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">hot_water_litres</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>The cumulative hot water consumption in litres.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">room_temp_c</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>The measured temperature in Celsius for a specific room (or average).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">setpoint_temp_c</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>The user-defined target temperature in Celsius for a specific room.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">outdoor_temp_c</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>The outdoor temperature at the time of the reading, joined from weather data.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>year</strong></p></td>
<td class="text-left"><p>Integer</p></td>
<td class="text-left"><p><strong>Partition Key:</strong> Year derived from <code class="docutils literal notranslate"><span class="pre">event_ts</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>month</strong></p></td>
<td class="text-left"><p>Integer</p></td>
<td class="text-left"><p><strong>Partition Key:</strong> Month derived from <code class="docutils literal notranslate"><span class="pre">event_ts</span></code>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>day</strong></p></td>
<td class="text-left"><p>Integer</p></td>
<td class="text-left"><p><strong>Partition Key:</strong> Day derived from <code class="docutils literal notranslate"><span class="pre">event_ts</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="weather-data">
<h4>3. Weather Data<a class="headerlink" href="#weather-data" title="Permalink to this heading">¶</a></h4>
<p><strong>3.1 Raw Weather Forecast Data (from API)</strong></p>
<p>This is the raw JSON structure ingested from the external weather forecast API into the S3 Raw Zone.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Field Name</p></th>
<th class="head text-left"><p>Data Type</p></th>
<th class="head text-left"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">latitude</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>Latitude of the forecast location.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">longitude</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>Longitude of the forecast location.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">generationtime_ms</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>Time taken by the API to generate the forecast.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">utc_offset_seconds</span></code></p></td>
<td class="text-left"><p>Integer</p></td>
<td class="text-left"><p>UTC offset for the location.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">hourly</span></code></p></td>
<td class="text-left"><p>Object</p></td>
<td class="text-left"><p>An object containing arrays of hourly forecast values.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">hourly.time</span></code></p></td>
<td class="text-left"><p>Array[String]</p></td>
<td class="text-left"><p>Array of ISO 8601 timestamps for the forecast horizon.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">hourly.temperature_2m</span></code></p></td>
<td class="text-left"><p>Array[Double]</p></td>
<td class="text-left"><p>Array of corresponding forecasted temperatures (°C).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">hourly.cloudcover</span></code></p></td>
<td class="text-left"><p>Array[Integer]</p></td>
<td class="text-left"><p>Array of corresponding forecasted cloud cover (%).</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">hourly.shortwave_radiation</span></code></p></td>
<td class="text-left"><p>Array[Double]</p></td>
<td class="text-left"><p>Array of corresponding forecasted solar irradiance (W/m²).</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>3.2 Processed Weather Data (Joined in <code class="docutils literal notranslate"><span class="pre">processed_edf_data</span></code>)</strong></p>
<p>This represents the clean, hourly weather data after being processed and joined to the consumption data.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Column Name</p></th>
<th class="head text-left"><p>Data Type</p></th>
<th class="head text-left"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">building_id</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>Unique identifier for the building the weather corresponds to.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">timestamp_hour</span></code></p></td>
<td class="text-left"><p>Timestamp</p></td>
<td class="text-left"><p>The specific hour for which the weather data is valid, truncated.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">temperature_c</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>The average temperature in Celsius for that hour.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">humidity</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>The average relative humidity (%) for that hour.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">solar_irradiance_ghi</span></code></p></td>
<td class="text-left"><p>Double</p></td>
<td class="text-left"><p>The average Global Horizontal Irradiance (W/m²) for that hour.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">is_holiday_flag</span></code></p></td>
<td class="text-left"><p>Integer</p></td>
<td class="text-left"><p>A binary flag (1 or 0) indicating if the date is a public holiday.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="feature-store-features-anomaly-detection">
<h4>4. Feature Store Features (Anomaly Detection)<a class="headerlink" href="#feature-store-features-anomaly-detection" title="Permalink to this heading">¶</a></h4>
<p>This defines the schema of the <code class="docutils literal notranslate"><span class="pre">ad-apartment-features</span></code> Feature Group in SageMaker Feature Store. These are the inputs to the AD model.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Feature Name</p></th>
<th class="head text-left"><p>Data Type</p></th>
<th class="head text-left"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>apartment_record_id</strong></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p><strong>Record Identifier:</strong> Unique ID for the record (e.g., <code class="docutils literal notranslate"><span class="pre">[apartment_id]_[date]</span></code>).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>event_time</strong></p></td>
<td class="text-left"><p>Fractional</p></td>
<td class="text-left"><p><strong>Event Time:</strong> Timestamp when the features were computed/ingested.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">event_date</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>The specific date (YYYY-MM-DD) these daily features correspond to.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">building_id</span></code></p></td>
<td class="text-left"><p>String</p></td>
<td class="text-left"><p>Identifier for the building.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">avg_temp_diff</span></code></p></td>
<td class="text-left"><p>Fractional</p></td>
<td class="text-left"><p>The average difference between the setpoint and actual room temperature for the day.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">daily_energy_kwh</span></code></p></td>
<td class="text-left"><p>Fractional</p></td>
<td class="text-left"><p>The total heating energy consumed on that day.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">hdd</span></code></p></td>
<td class="text-left"><p>Fractional</p></td>
<td class="text-left"><p>Heating Degree Days, a measure of how cold the day was.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">energy_lag_1d</span></code></p></td>
<td class="text-left"><p>Fractional</p></td>
<td class="text-left"><p>The value of <code class="docutils literal notranslate"><span class="pre">daily_energy_kwh</span></code> from the previous day.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">energy_roll_avg_7d</span></code></p></td>
<td class="text-left"><p>Fractional</p></td>
<td class="text-left"><p>The 7-day rolling average of <code class="docutils literal notranslate"><span class="pre">daily_energy_kwh</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">temp_diff_roll_std_3d</span></code></p></td>
<td class="text-left"><p>Fractional</p></td>
<td class="text-left"><p>The 3-day rolling standard deviation of <code class="docutils literal notranslate"><span class="pre">avg_temp_diff</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="alert-table-schema-dynamodb">
<h4>5. Alert Table Schema (DynamoDB)<a class="headerlink" href="#alert-table-schema-dynamodb" title="Permalink to this heading">¶</a></h4>
<p>This defines the structure of the <code class="docutils literal notranslate"><span class="pre">ad-alerts</span></code> table in Amazon DynamoDB, where the inference pipeline stores actionable alerts.</p>
<p><strong>Table Name:</strong> <code class="docutils literal notranslate"><span class="pre">hometech-ml-ad-alerts-[env]</span></code></p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Attribute Name</p></th>
<th class="head text-left"><p>Data Type</p></th>
<th class="head text-left"><p>Key Type / Index</p></th>
<th class="head text-left"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>AlertID</strong></p></td>
<td class="text-left"><p>String (S)</p></td>
<td class="text-left"><p><strong>Partition Key (PK)</strong></p></td>
<td class="text-left"><p>Unique identifier for the alert. <strong>Format:</strong> <code class="docutils literal notranslate"><span class="pre">[ApartmentID]#[EventDate]</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">ApartmentID</span></code></p></td>
<td class="text-left"><p>String (S)</p></td>
<td class="text-left"><p>GSI-1 Partition Key</p></td>
<td class="text-left"><p>The unique identifier of the apartment that triggered the alert.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">BuildingID</span></code></p></td>
<td class="text-left"><p>String (S)</p></td>
<td class="text-left"><p>GSI-2 Partition Key</p></td>
<td class="text-left"><p>The unique identifier of the building.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">EventDate</span></code></p></td>
<td class="text-left"><p>String (S)</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p>The date (YYYY-MM-DD) for which the anomaly was detected.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">AlertTimestamp</span></code></p></td>
<td class="text-left"><p>String (S)</p></td>
<td class="text-left"><p>GSI-2 Sort Key</p></td>
<td class="text-left"><p>ISO 8601 timestamp of when the alert was created by the pipeline.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">AnomalyScore</span></code></p></td>
<td class="text-left"><p>Number (N)</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p>The raw numerical score from the ML model. Higher means more anomalous.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">Threshold</span></code></p></td>
<td class="text-left"><p>Number (N)</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p>The score threshold that was breached to create this alert.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Status</strong></p></td>
<td class="text-left"><p>String (S)</p></td>
<td class="text-left"><p>GSI-1 Sort Key</p></td>
<td class="text-left"><p>The current state of the alert. <strong>Values:</strong> <code class="docutils literal notranslate"><span class="pre">Unseen</span></code>, <code class="docutils literal notranslate"><span class="pre">Investigating</span></code>, <code class="docutils literal notranslate"><span class="pre">Resolved-True</span> <span class="pre">Positive</span></code>, <code class="docutils literal notranslate"><span class="pre">Resolved-False</span> <span class="pre">Positive</span></code>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">ModelVersion</span></code></p></td>
<td class="text-left"><p>String (S)</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p>Version of the model package that generated the score (for lineage).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">FeedbackNotes</span></code></p></td>
<td class="text-left"><p>String (S)</p></td>
<td class="text-left"><p>-</p></td>
<td class="text-left"><p>(Optional) Notes entered by the maintenance technician during review.</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Global Secondary Indexes (GSIs):</strong></p>
<ul class="simple">
<li><p><strong>GSI-1 (<code class="docutils literal notranslate"><span class="pre">ApartmentStatusIndex</span></code>):</strong> Allows efficiently querying for alerts of a specific <code class="docutils literal notranslate"><span class="pre">Status</span></code> within a given <code class="docutils literal notranslate"><span class="pre">ApartmentID</span></code>.</p>
<ul>
<li><p><strong>Partition Key:</strong> <code class="docutils literal notranslate"><span class="pre">ApartmentID</span></code></p></li>
<li><p><strong>Sort Key:</strong> <code class="docutils literal notranslate"><span class="pre">Status</span></code></p></li>
</ul>
</li>
<li><p><strong>GSI-2 (<code class="docutils literal notranslate"><span class="pre">BuildingAlertsIndex</span></code>):</strong> Allows efficiently querying for all alerts in a <code class="docutils literal notranslate"><span class="pre">BuildingID</span></code>, sorted by time.</p>
<ul>
<li><p><strong>Partition Key:</strong> <code class="docutils literal notranslate"><span class="pre">BuildingID</span></code></p></li>
<li><p><strong>Sort Key:</strong> <code class="docutils literal notranslate"><span class="pre">AlertTimestamp</span></code></p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="adas_engine/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">ADAS: Data Engine</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="iot_anomaly.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Anomaly Detection in Time Series IoT Data</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, Deepak Karkala
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Energy Demand Forecasting in Time Series IoT Data</a><ul>
<li><a class="reference internal" href="#id1"></a><ul>
<li><a class="reference internal" href="#tl-dr-ml-powered-energy-demand-forecasting-for-smart-buildings">TL;DR: ML-Powered Energy Demand Forecasting for Smart Buildings</a></li>
<li><a class="reference internal" href="#introduction">Introduction</a><ul>
<li><a class="reference internal" href="#purpose">Purpose</a></li>
<li><a class="reference internal" href="#business-goal">Business Goal</a></li>
<li><a class="reference internal" href="#scope">Scope</a></li>
<li><a class="reference internal" href="#key-technologies">Key Technologies</a></li>
</ul>
</li>
<li><a class="reference internal" href="#discovery-and-scoping">Discovery and Scoping</a><ul>
<li><a class="reference internal" href="#use-case-evaluation">Use Case Evaluation</a></li>
<li><a class="reference internal" href="#product-strategies">Product Strategies</a></li>
<li><a class="reference internal" href="#features">Features</a></li>
<li><a class="reference internal" href="#product-requirements-document">Product Requirements Document</a></li>
<li><a class="reference internal" href="#development-stages">Development Stages</a></li>
</ul>
</li>
<li><a class="reference internal" href="#system-architecture">System Architecture</a><ul>
<li><a class="reference internal" href="#overall-data-flow">Overall Data Flow</a></li>
<li><a class="reference internal" href="#training-workflow">Training Workflow</a></li>
<li><a class="reference internal" href="#inference-workflow">Inference Workflow</a></li>
<li><a class="reference internal" href="#forecast-serving">Forecast Serving</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-development-iteration">Model Development &amp; Iteration</a></li>
<li><a class="reference internal" href="#configuration-management">Configuration Management</a></li>
<li><a class="reference internal" href="#infrastructure-as-code-terraform">Infrastructure as Code (Terraform)</a></li>
<li><a class="reference internal" href="#ci-cd-pipeline-bitbucket">CI/CD Pipeline (Bitbucket)</a></li>
<li><a class="reference internal" href="#deployment-execution">Deployment &amp; Execution</a></li>
<li><a class="reference internal" href="#monitoring-alerting">Monitoring &amp; Alerting</a></li>
<li><a class="reference internal" href="#estimated-monthly-costs">Estimated Monthly Costs</a></li>
<li><a class="reference internal" href="#challenges-and-learnings">Challenges and learnings</a></li>
<li><a class="reference internal" href="#troubleshooting-guide">Troubleshooting Guide</a></li>
<li><a class="reference internal" href="#security-considerations">Security Considerations</a></li>
<li><a class="reference internal" href="#roadmap-future-enhancements">Roadmap &amp; Future Enhancements</a></li>
<li><a class="reference internal" href="#appendices">Appendices</a><ul>
<li><a class="reference internal" href="#configuration-file-example">Configuration File Example</a></li>
<li><a class="reference internal" href="#data-schemas">Data Schemas</a></li>
<li><a class="reference internal" href="#raw-meter-data">1. Raw Meter Data</a></li>
<li><a class="reference internal" href="#processed-meter-data-for-anomaly-detection">2. Processed Meter Data (for Anomaly Detection)</a></li>
<li><a class="reference internal" href="#weather-data">3. Weather Data</a></li>
<li><a class="reference internal" href="#feature-store-features-anomaly-detection">4. Feature Store Features (Anomaly Detection)</a></li>
<li><a class="reference internal" href="#alert-table-schema-dynamodb">5. Alert Table Schema (DynamoDB)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/scripts/furo.js?v=4e2eecee"></script>
    </body>
</html>